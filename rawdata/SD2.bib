@article{IBRAHIM2021100016,
title = {Causality-based accountability mechanisms for socio-technical systems},
journal = {Journal of Responsible Technology},
volume = {7-8},
pages = {100016},
year = {2021},
issn = {2666-6596},
doi = {https://doi.org/10.1016/j.jrt.2021.100016},
url = {https://www.sciencedirect.com/science/article/pii/S2666659621000093},
author1 = {Amjad Ibrahim and Stavros Kyriakopoulos and Alexander Pretschner},
keywords = {Accountability, Actual causality, Socio-technical systems},
abstract = {With the rapid deployment of socio-technical systems into all aspects of daily life, we need to be prepared for their failures. It is inherently impractical to specify all the lawful interactions of these systems, in turn, the possibility of invalid interactions cannot be excluded at design time. As modern systems might harm people, or compromise assets if they fail, they ought to be accountable. Accountability is an interdisciplinary concept that cannot be easily described as a holistic technical property of a system. Thus, in this paper, we propose a bottom-up approach to enable accountability using goal-specific accountability mechanisms. Each mechanism provides forensic capabilities that help us to identify the root cause for a specific type of events, both to eliminate the underlying (technical) problem and to assign blame. This paper presents the different ingredients that are required to design and build an accountability mechanism and focuses on the technical and practical utilization of causality theories as a cornerstone to achieve our goal. To the best of our knowledge, the literature lacks a systematic methodology to envision, design, and implement abilities that promote accountability in systems. With a case study from the area of microservice-based systems, which we deem representative of modern complex systems, we demonstrate the effectiveness of the approach as a whole. We show that it is generic enough to accommodate different accountability goals and mechanisms.}
}
@article{MAGABALEH2024111859,
title = {Systematic review of software engineering uses of multi-criteria decision-making methods: Trends, bibliographic analysis, challenges, recommendations, and future directions},
journal = {Applied Soft Computing},
volume = {163},
pages = {111859},
year = {2024},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2024.111859},
url = {https://www.sciencedirect.com/science/article/pii/S1568494624006331},
author1 = {Aws A. Magabaleh and Lana L. Ghraibeh and Afnan Y. Audeh and A.S. Albahri and Muhammet Deveci and Jurgita Antucheviciene},
keywords = {MCDM, Software engineering, SDLC, Software development life cycle, Systematic literature review (SLR)},
abstract = {Correctly adhering to the processes within the software development life cycle (SDLC), from analysis and design to coding and testing, is vital for ensuring the successful and efficient creation of high-quality software applications. These structured phases provide a systematic approach to software development, facilitating clear communication, reducing errors, and improving collaboration among development teams. For the proper and correct use of SDLC processes, it is essential for both software engineers and software decision makers to perform the correct and needed actions while performing each software process, and one method of facilitating that is multicriteria decision making (MCDM). This study aims to provide a systematic review of the use of MCDM within the field of software engineering (SE), encompassing methodologies such as fuzzy MCDM, AHP, TOPSIS, DEMATEL, and other methods, with a deliberate focus on software engineering development processes. To ensure the high quality of this review, a methodical and structured literature search process was performed with strict selection criteria, resulting in the identification of 32 contributions on the applications of MCDM in SE from various databases, including Scopus, ScienceDirect, IEEE Xplore digital library (IEEE), and Web of Science (WOS). The selected papers were taxonomized into seven main categories, with some divided into subcategories. This paper presents a systematic and comprehensive analysis of the aforementioned studies, investigating the challenges, motivations, and recommendations found within each, thereby paving the way for potential future research. Bibliometric analysis is also provided to show concise quantitative analysis of related bibliographic information, which draws several key insights into publication trends. Finally, a critical analysis of the current literature and existing research is presented, while also addressing relevant research gaps.}
}
@article{OUKES2021105379,
title = {Domain-Driven Design applied to land administration system development: Lessons from the Netherlands},
journal = {Land Use Policy},
volume = {104},
pages = {105379},
year = {2021},
issn = {0264-8377},
doi = {https://doi.org/10.1016/j.landusepol.2021.105379},
url = {https://www.sciencedirect.com/science/article/pii/S0264837721001022},
author1 = {Peter Oukes and Marc van Andel and Erwin Folmer and Rohan Bennett and Christiaan Lemmen},
keywords = {Domain-Driven Design, DDD, Event-based modelling, Implementation, Interoperability, Land registration, Land administration, Land administration domain model, LADM},
abstract = {The introduction or renewal of information systems conventionally begins with data modelling. In the domain of land administration, like others, the process is challenging: complex laws and regulations, lengthy process descriptions, shared organisational responsibilities, differing information encodings and formats, and seeking compliance with the LADM ISO 19152 standard, must be considered. Between 2016 and 2018, The Netherlands’ Cadastre, Land Registry and Mapping Agency – in short, Kadaster – successfully undertook the renewal of the information system supporting its deeds registration. The previous system dated back to the 1980s. In-house data modelling specialists led the program, the most extensive undertaken in decades. Inspired by action-research principles, the process and resultant lessons are documented using a case study approach. It is shown that beyond Model Driven Architectures, other model-driven methodologies, such as Domain-Driven Design, are entirely useful in the land administration domain. A domain is usually more extensive than a few objects, and to make it more manageable, DDD divides a domain into subdomains. The DDD term ‘problem domain’ is used to define a functional area within a context such as an organisation or department. The terms domain in DDD and LADM have common characteristics as considering contexts such as a land registry and a cadastre. Evans (2003) and Vernon (2013) articulate how DDD is a set of design practices, techniques and fundamental principles, terms, and implications to facilitate the development of software projects within complex domains, used to guide software developers and domain experts to share and represent models of knowledge from the domain. In DDD, the ubiquitous language is also essential in intersecting the jargons between domain experts and IT experts. The LADM provides a formal language for describing similarities and differences for describing the many aspects of the land administration domain. The approach demands greater participation from domain experts: they lead modelling of the current state and its evolution: the events. The Annex N of ISO 19152 describes that LADM covers both event-based and state-based modelling via LA_Source and VersionedObject. The application of Event-Based Modelling and Event Sourcing is still relatively novel to the LA domain. Event Sourcing ensures that all current state changes are stored as a sequence of events, enabling querying and state reconstruction. The new information system is considered futureproof, delivering improvements for deed registration times, monitoring, traceability/auditing, history management and interoperability. Further research suggestions include undertaken Domain-Driven Design in other contexts, particularly those implementing LADM.}
}
@article{SEDKI2024112154,
title = {AML: An accuracy metric model for effective evaluation of log parsing techniques},
journal = {Journal of Systems and Software},
volume = {216},
pages = {112154},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112154},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001997},
author1 = {Issam Sedki and Abdelwahab Hamou-Lhadj and Otmane Ait Mohamed},
keywords = {Log parsing, Log analytics, Software logging, AIOps, Software maintenance and evolution},
abstract = {Logs are essential for the maintenance of large software systems. Software engineers often analyze logs for debugging, root cause analysis, and anomaly detection tasks. Logs, however, are partly structured, making the extraction of useful information from massive log files a challenging task. Recently, many log parsing techniques have been proposed to automatically extract log templates from unstructured log files. These parsers, however, are evaluated using different accuracy metrics. In this paper, we show that these metrics have several drawbacks, making it challenging to understand the strengths and limitations of existing parsers. To address this, we propose a novel accuracy metric, called AML (Accuracy Metric for Log Parsing). AML is a robust accuracy metric that is inspired by research in the field of remote sensing. It is based on measuring omission and commission errors. We use AML to assess the accuracy of 14 log parsing tools applied to the parsing of 16 log datasets. We also show how AML compares to existing accuracy metrics. Our findings demonstrate that AML is a promising accuracy metric for log parsing compared to alternative solutions, which enables a comprehensive evaluation of log parsing tools to help better decision-making in selecting and improving log parsing techniques.}
}
@article{GONZALEZPRIETO2023111707,
title = {Reliability in software engineering qualitative research through Inter-Coder Agreement},
journal = {Journal of Systems and Software},
volume = {202},
pages = {111707},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111707},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001024},
author1 = {Ángel González-Prieto and Jorge Perez and Jessica Diaz and Daniel López-Fernández},
keywords = {Inter-Coder Agreement, Krippendorff’s αcoefficient, Coding, Qualitative research, Software engineering},
abstract = {The research on empirical software engineering that uses qualitative data analysis is increasing. However, most of them do not deepen into the validity of the findings, specifically in the reliability of coding in which these methodologies rely on. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis through Inter-Coder Agreement (ICA), based on the use of coefficients to measure the degree of agreement in collaborative coding. We systematically review several existing variants of Krippendorff’s α coefficients and provide a novel common mathematical framework to unify them. Finally, this paper illustrates the use of this theoretical framework in a large case study on DevOps culture. We expect that this work will help researchers who are committed to measuring consensus with quantitative techniques in collaborative coding, conducted as part of a qualitative research, to improve the rigor of their findings.}
}
@article{KRITIKOS2015103,
title = {Security Enforcement for Multi-Cloud Platforms – The Case of PaaSage},
journal = {Procedia Computer Science},
volume = {68},
pages = {103-115},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.227},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030720},
author1 = {Kyriakos Kritikos and Tom Kirkham and Bartosz Kryza and Philippe Massonet},
keywords = {cloud, security, authentication, author1isation, model-driven, identity, policies, permissions, roles, users, meta-model, SAML},
abstract = {Multi-cloud adaptive application provisioning promises to solve the vendor lock-in problem and lead to optimizing the user re- quirements through the selection of the best from the great variety of services offered by cloud providers. As such, various research prototypes and platforms attempt to support this provisioning type. One major concern in using such platforms comes with respect to security in terms of improper access to user personal data and VMs as well as to platform services. To successfully address this concern, this paper proposes a novel model-driven approach and architecture able to secure multi-cloud platforms as well as enable users to have their own private space. Such a solution exploits state-of-the-art security standards and secure model manage- ment technology. This solution is able to cover different security scenarios involving external, web-based and programmatic user authentication.}
}
@article{ALMUAIRFI202013,
title = {Security controls in infrastructure as code},
journal = {Computer Fraud & Security},
volume = {2020},
number = {10},
pages = {13-19},
year = {2020},
issn = {1361-3723},
doi = {https://doi.org/10.1016/S1361-3723(20)30109-3},
url = {https://www.sciencedirect.com/science/article/pii/S1361372320301093},
author1 = {Sadiq Almuairfi and Mamdouh Alenezi},
abstract = {The development, deployment and management of software applications have shifted dramatically in the past 10 years. This fundamental shift is what we now know as development operations (DevOps). Infrastructure as Code (IaC) is one of the main tenets of DevOps. Previously, manual configuration via cloud providers’ UI consoles and physical hardware used to take place. But now, with the concept of IaC, the IT infrastructure can be automated by using blueprints that are easily readable by machines.}
}
@article{WARNETT2025112257,
title = {A model-driven, metrics-based approach to assessing support for quality aspects in MLOps system architectures},
journal = {Journal of Systems and Software},
volume = {220},
pages = {112257},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112257},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003017},
author1 = {Stephen John Warnett and Evangelos Ntentos and Uwe Zdun},
keywords = {Software architecture quality, Metrics, Distributed system modelling, Distributed software architecture, MLOps, Machine learning},
abstract = {In machine learning (ML) and machine learning operations (MLOps), automation serves as a fundamental pillar, streamlining the deployment of ML models and representing an architectural quality aspect. Support for automation is especially relevant when dealing with ML deployments characterised by the continuous delivery of ML models. Taking automation in MLOps systems as an example, we present novel metrics that offer reliable insights into support for this vital quality attribute, validated by ordinal regression analysis. Our method introduces novel, technology-agnostic metrics aligned with typical Architectural Design Decisions (ADDs) for automation in MLOps. Through systematic processes, we demonstrate the feasibility of our approach in evaluating automation-related ADDs and decision options. Our approach can itself be automated within continuous integration/continuous delivery pipelines. It can also be modified and extended to evaluate any relevant architectural quality aspects, thereby assisting in enhancing compliance with non-functional requirements and streamlining development, quality assurance and release cycles.}
}
@article{SUAREZOTERO2023111743,
title = {CoDEvo: Column family database evolution using model transformations},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111743},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111743},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001383},
author1 = {Pablo Suárez-Otero and Michael J. Mior and María José Suárez-Cabal and Javier Tuya},
keywords = {Software requirements, Consistency, MDE, Model transformation, NoSQL, Evolution},
abstract = {In recent years, software applications have been working with NoSQL databases as they have emerged to handle big data more efficiently than traditional databases. The data models of these databases are designed to satisfy the requirements of the software application, which means that the models must evolve when the requirements of the software application change. To avoid mistakes during the design and evolution of these NoSQL models, there are several methodologies that recommend using a conceptual model. This implies that consistency between the conceptual model and the schema must be maintained when either evolving the database or the software application. In this work, we propose CoDEvo, a model-driven engineering approach that uses model transformations to address the evolution of a NoSQL column family DBMS schema when the underlying conceptual model evolves due to software requirement changes, aiming to maintain consistency between the schema and conceptual model. We have addressed this problem by defining transformation rules that determine how to evolve the schema for a specific conceptual model change. To validate these transformations, we applied them to conceptual model changes from 9 open-source software applications, comparing the output schemas from CoDEvo with the schemas that were defined in these applications.}
}
@article{BARTUSEVICS2017112,
title = {An Approach for Development of Reusable Function Library for Automation of Continuous Processes},
journal = {Procedia Computer Science},
volume = {104},
pages = {112-119},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.082},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917300832},
author1 = {Arturs Bartusevics and Leonids Novickis and Andrejs Lesovskis},
keywords = {Automation, DevOps, IT operations, Automation Scripts},
abstract = {The paper introduce to challenges in automation of continuous processes in companies with many active software development projects. Usually companies have automation solutions for particular projects. When new software development projects are coming, companies would like to save up resources and efforts needed for automation solutions in these projects. In case when automation solutions, implemented in existing projects, are reusable, they could be used in new project without additional efforts for refactoring, development automation solution from scratch etc. Current paper provides an approach for development of library of reusable functions. This library allows reusing implemented automation functions in the different projects and in the different workflows. Finally, a prototype is developed for practical experiments. Based on its results, benefits, disadvantages and improvement directions of provided approach are detected.}
}
@article{BASSILIADES201881,
title = {PaaSport semantic model: An ontology for a platform-as-a-service semantically interoperable marketplace},
journal = {Data & Knowledge Engineering},
volume = {113},
pages = {81-115},
year = {2018},
issn = {0169-023X},
doi = {https://doi.org/10.1016/j.datak.2017.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S0169023X17300551},
author1 = {Nick Bassiliades and Moisis Symeonidis and Panagiotis Gouvas and Efstratios Kontopoulos and Georgios Meditskos and Ioannis Vlahavas},
keywords = {Cloud computing, Platform-as-a-Service, Cloud Marketplace, Semantic interoperability, Ontologies, Quality and metrics},
abstract = {PaaS is a Cloud computing service that provides a computing platform to develop, run, and manage applications without the complexity of infrastructure maintenance. SMEs are reluctant to enter the growing PaaS market due to the possibility of being locked in to a certain platform, mostly provided by the market's giants. The PaaSport Marketplace aims to avoid the provider lock-in problem by allowing Platform provider SMEs to roll out semantically interoperable PaaS offerings and Software SMEs to deploy or migrate their applications on the best-matching offering, through a thin, non-intrusive Cloud broker. In this paper, we present the PaaSport semantic model, namely an OWL ontology, extension of the DUL ontology. The ontology is used for semantically representing (a) PaaS offering capabilities and (b) requirements of applications to be deployed. The ontology has been designed to optimally support a semantic matchmaking and ranking algorithm that recommends the best-matching PaaS offering to the application developer. The DUL ontology offers seamless extensibility, since both PaaS Characteristics and parameters are defined as classes; therefore, extending the ontology with new characteristics and parameters requires the addition of new specialized subclasses of the already existing classes, which is less complicated than adding ontology properties. The PaaSport ontology is evaluated through verification tools, competency questions, human experts, application tasks and query performance tests.}
}
@incollection{DAVIS2024709,
title = {Smart Manufacturing},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies (Second Edition)},
publisher = {Elsevier},
edition = {Second Edition},
address = {Oxford},
pages = {709-720},
year = {2024},
isbn = {978-0-443-22287-0},
doi = {https://doi.org/10.1016/B978-0-323-90386-8.00148-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780323903868001480},
author1 = {Jim Davis},
keywords = {Advance manufacturing, Digital manufacturing, Industry 4.0, Intelligent manufacturing, Manufacturing 4.0, Smart manufacturing},
abstract = {Smart Manufacturing (SM) uses the integration of next generation Operations Technology (OT) and Information Technology (IT) to realize significant untapped market opportunities. Opportunities accrue from a broader, more customized product space and accelerated dynamic, and precision manufacturing, drawing upon the cumulative impacts of every step across enterprise value and supply chains. Significantly increased energy, material and workforce productivity and improved environmental sustainability are pathways to opportunities as well as outcomes of enterprise optimization. This paper returns smart manufacturing (SM) as a term of “practice” to its roots, revisiting it as a forward looking term about next-generation operations technology (OT) and information technology (IT) integration based on the manufacturing objectives that originally defined it. Building meaning through a lens of practices that orchestrate capabilities to achieve business and operational objectives makes it possible to describe SM itself as a set of practices that distinguish it from today’s 40-year history of applying IT to manufacturing. Key practices at SM’s core are centered on: business and technology; seams and an OT/IT journey; OT/IT enterprise workflow modeling; cyberinfrastructure and reference architecture; and workforce, innovation, and marketplace.}
}
@article{DAVID2023111626,
title = {Collaborative Model-Driven Software Engineering — A systematic survey of practices and needs in industry},
journal = {Journal of Systems and Software},
volume = {199},
pages = {111626},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111626},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000213},
author1 = {Istvan David and Kousar Aslam and Ivano Malavolta and Patricia Lago},
keywords = {Model-driven engineering, Collaborative software engineering, Industry survey},
abstract = {The engineering of modern software-intensive systems is carried out in collaboration among stakeholders with specialized expertise. The complexity of such systems often also necessitates employing more rigorous approaches, such as Model-Driven Software Engineering (MDSE). Collaborative MDSE is the combination of the two disciplines, with its specific opportunities and challenges. The rapid expansion and maturation of the field started attracting tool builders from outside of academia. However, available systematic studies on collaborative MDSE focus exclusively on mapping academic research and fail to identify how academic research aligns with industry practices and needs. To address this shortcoming, we have carried out a mixed-method survey on the practices and needs concerning collaborative MDSE. First, we carried out a qualitative survey in two focus group sessions, interviewing seven industry experts. Second, based on the results of the interviews, we constructed a questionnaire and carried out a questionnaire survey with 41 industry expert participants. In this paper, we report the results of our study, investigate the alignment of academic research with the needs of practitioners, and suggest directions on research and development of the supporting techniques of collaborative MDSE.}
}
@article{HASAN2023111702,
title = {Legacy systems to cloud migration: A review from the architectural perspective},
journal = {Journal of Systems and Software},
volume = {202},
pages = {111702},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111702},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000973},
author1 = {Muhammad Hafiz Hasan and Mohd Hafeez Osman and Novia Indriaty Admodisastro and Muhamad Sufri Muhammad},
keywords = {Monolith, Cloud migration, Software architecture, Systematic review},
abstract = {Legacy systems are business-critical systems that hold the organization’s core business functions developed in a traditional way using monolith architecture and usually deployed on-premises. Through time, this system is exposed to improvement changes, increasing its size and number of functionalities, thus increasing its complexity, and maintaining it becomes a disadvantage to the organization. Migration to the cloud environment becomes the primary option to improve legacy application agility, maintainability, and flexibility. However, to take advantage of the cloud environment, monolith legacy application needs to be rearchitected as microservice architecture to fully benefit from cloud advantages. This paper aims to understand the motivation for cloud migration, investigate existing cloud migration frameworks, identify the target architecture for the cloud, and establish any empirical quality issues in cloud migration from the implementation point of view. To achieve those objectives, we conducted a systematic literature review (SLR) of 47 selected studies from the most relevant scientific digital libraries covering pre-migration, migration, and post-migration stages. The SLR outcome provided us with the primary motivation for the cloud migration, existing cloud migration frameworks, targeted migration architecture patterns, and migration challenges. The results also highlight areas where more research is needed and suggest future research in this field. Furthermore, our analysis shows that current migration approaches lack quality consideration, thus contributing to post-migration quality concerns.}
}
@article{NGUYEN2024281,
title = {QFaaS: A Serverless Function-as-a-Service framework for Quantum computing},
journal = {Future Generation Computer Systems},
volume = {154},
pages = {281-300},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24000189},
author1 = {Hoa T. Nguyen and Muhammad Usman and Rajkumar Buyya},
keywords = {Quantum serverless, Quantum function-as-a-service, Quantum software engineering, Hybrid quantum-classical computing, Quantum DevOps, Quantum cloud computing},
abstract = {Quantum computing is rapidly reaching a point in which its application design and engineering aspects must be seriously considered. However, quantum software engineering is still in its infancy, with numerous challenges, especially in dealing with the diversity of quantum programming languages and noisy intermediate-scale quantum (NISQ) systems. To alleviate these challenges, we propose QFaaS, a holistic Quantum Function-as-a-Service framework, which leverages the advantages of the serverless model, DevOps lifecycle, and the state-of-the-art software techniques to advance practical quantum computing for next-generation application development in the NISQ era. Our framework provides essential elements of a serverless quantum system to streamline service-oriented quantum application development in cloud environments, such as combining hybrid quantum–classical computation, automating the backend selection, cold start mitigation, and adapting DevOps techniques. QFaaS offers a full-stack and unified quantum serverless platform by integrating multiple well-known quantum software development kits (Qiskit, Q#, Cirq, and Braket), quantum simulators, and cloud providers (IBM Quantum and Amazon Braket). This paper proposes the concept of quantum function-as-a-service, system design, operation workflows, implementation of QFaaS, and lessons learned on the benefits and limitations of quantum serverless computing. We also present practical use cases with various quantum applications on today’s quantum computers and simulators to demonstrate our framework capability to facilitate the ongoing quantum software transition.}
}
@article{KRITIKOS2017206,
title = {Towards a security-enhanced PaaS platform for multi-cloud applications},
journal = {Future Generation Computer Systems},
volume = {67},
pages = {206-226},
year = {2017},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.10.008},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16303880},
author1 = {Kyriakos Kritikos and Tom Kirkham and Bartosz Kryza and Philippe Massonet},
keywords = {Cloud, Security, Meta-model, Policies, Roles, SAML},
abstract = {Multi-cloud adaptive application provisioning can solve the vendor lock-in problem and allows optimising user requirements by selecting the best from the multitude of services offered by different cloud providers. To this end, such provisioning type is increasingly supported by new or existing research prototypes and platforms. One major concern, actually preventing users from moving to the cloud, comes with respect to security, which becomes more complex in multi-cloud settings. Such a concern spans two main aspects: (a) suitable access control on user personal data, VMs and platform services and (b) planning and adapting application deployments based on security requirements. As such, this paper addresses both security aspects by proposing a novel model-driven approach and architecture which secures multi-cloud platforms, enables users to have their own private space and guarantees that application deployments are not only constructed based on but can also maintain a certain user-required security level. Such a solution exploits state-of-the-art security standards, security software and secure model management technology. Moreover, it covers different access control scenarios involving external, web-based and programmatic user authentication.}
}
@article{SANTOSJUNIOR2021106570,
title = {From a Scrum Reference Ontology to the Integration of Applications for Data-Driven Software Development},
journal = {Information and Software Technology},
volume = {136},
pages = {106570},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106570},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921000537},
author1 = {Paulo Sérgio {Santos Júnior} and Monalessa Perini Barcellos and Ricardo de Almeida Falbo and João Paulo A. Almeida},
keywords = {Ontology, Scrum, Semantic Interoperability, Application Integration},
abstract = {Context
Organizations often use different applications to support the Scrum process, including project management tools, source repository and quality assessment tools. These applications store useful data for decision-making. However, data items often remain spread in different applications, each of which adopt different data and behavioral models, posing a barrier for integrated data usage. As a consequence, data-driven decisions in agile development are uncommon, missing valuable opportunities for informed decision making.
Objective
Considering the need to address semantic issues to properly integrate applications that support the agile development process, we aim to provide a common and comprehensive conceptualization about Scrum in the software development context and apply this conceptualization to support application integration.
Method
We have developed the Scrum Reference Ontology (SRO) and used it to semantically integrate Azure DevOps and Clockify.
Results
SRO served as a reference model to build software artifacts in a semantic integration architecture that enables applications to automatically share, exchange and combine data and services. The integrated solution was used in the software development unit of a Brazilian government agency. Results demonstrate that the integrated solution contributed to improving estimates, provided data that helped allocate teams, manage team productivity and project performance, and enabled to identify and fix problems in the Scrum process execution.
Conclusions
SRO can serve as an interlingua for application integration in the context of Scrum-process support. By capturing the conceptualization underlying Scrum, the reference ontology can address semantic conflicts and thereby support the development of integrated data-driven solutions for decision making.}
}
@article{WANG2023111754,
title = {Microservice architecture recovery based on intra-service and inter-service features},
journal = {Journal of Systems and Software},
volume = {204},
pages = {111754},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111754},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001498},
author1 = {Lulu Wang and Peng Hu and Xianglong Kong and Wenjie Ouyang and Bixin Li and Haixin Xu and Tao Shao},
keywords = {Microservice architecture, Architecture recovery, System Dependency Graph, Reverse engineering, Software understanding},
abstract = {Microservice architecture supports independent development and deployment; it facilitates software system design and co-development. However, it also brings new challenges to a variety of software engineering tasks, especially in reverse engineering. An improper design or maintenance routine may cause complex invocation, obscure code logic, and complicate service layers, which may lead to difficulties in understanding, even further testing, or maintenance. To reduce the severity of this problem, we present a novel microservice architecture recovery technique that parses the source code to build a fine-grained dependency graph. This process recovers six key information components of the microservice architecture, which helps developers understand the system. Experimental results based on 12 projects show that the recovered accuracy is 94% on average. The results benefit any engineer unfamiliar with the project, increases their answering accuracy by 23.81% on average, and reduces their training time by 65.43% on average.}
}
@article{ULUSOY2019106,
title = {Omni-script: Device independent user interface development for omni-channel fintech applications},
journal = {Computer Standards & Interfaces},
volume = {64},
pages = {106-116},
year = {2019},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2019.01.003},
url = {https://www.sciencedirect.com/science/article/pii/S0920548918302332},
author1 = {Simge Ulusoy and Alper Batıoğlu and Tolga Ovatman},
keywords = {Multimodal user interface definition, Omni-channel user experience, Omni-channel banking services},
abstract = {As the number of devices and platforms gradually increased that serve as a medium for online banking services, the number of replicated services for different platforms also increase. Each service has a different representation in a different platform even though targeting the same business objectives. This situation results in the problem of re-developing similar set of requirements for different devices and services which in turn results in multiplied effort in software development and degradation in the software quality because of the replicated code. The objective of this paper is to present a device independent user interface development approach that eliminates the replicated user interface development effort of the same set of services for different platforms/devices while increasing the presentation performance. In this context, we provide a basic technique that uses a json based user interface definition format, called omni-script, to separate the representation of banking services in different platforms/devices, so called channels. Omni-script is a pioneering solution being used in a real development environment in banking software domain that is experiencing a shift towards omni-channel user experience, where users are able to seamlessly continue using banking services in different platforms and devices. As the evaluation method, we have measured the actual development effort on a representative sample development team and a set of services. Additionally, we later deployed and measured the rendering time and the amount of data communication for the proposed approach in a real production environment. Production deployment resulted in a remarkable decrease in data communication and in loading times at the client side. As importantly, the proposed approach also resulted in a nearly 75% decrease in service development time by enabling analysts to produce user interface prototypes that can be integrated to service development with minimal effort.}
}
@article{DIAZ2024111908,
title = {Harmonizing DevOps taxonomies — A grounded theory study},
journal = {Journal of Systems and Software},
volume = {208},
pages = {111908},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111908},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003035},
author1 = {Jessica Díaz and Jorge Pérez and Isaque Alves and Fabio Kon and Leonardo Leite and Paulo Meirelles and Carla Rocha},
keywords = {DevOps team structures, DevOps taxonomies, Grounded theory, Inter-coder agreement},
abstract = {Context:
DevOps responds to the growing need of companies to streamline the software development process and thus has experienced widespread adoption in the past few years. However, the successful adoption of DevOps requires companies to address important cultural and organizational changes. Nevertheless, it is crucial to recognize that various DevOps taxonomies exist, both from academic and practitioner perspectives, which may lead to misleading or failed adoption of DevOps.
Objective:
This paper presents empirical research on the structure of DevOps teams in software-producing organizations. The goal is to better understand the organizational structure and characteristics of teams adopting DevOps by harmonizing the existing knowledge.
Methods:
To achieve this, we employed a grounded theory approach with collaborative coding, involving two research groups. Inter-Coder Agreement (ICA) was utilized to guide the discussion rounds. We conducted a comprehensive analysis of existing studies on DevOps teams and taxonomies to gain a deeper understanding of the subject.
Results:
From the analysis, we built a substantive and analytic theory of DevOps taxonomies. The theory is substantive in that the scope of validity refers to the ten secondary studies processed and analytic in that it analyzes “what is” rather than explaining causality or attempting predictive generalizations. A public repository with all the data related to the products resulting from the analysis and generation of the theory is available.
Conclusions:
We built a theory on DevOps taxonomies and tested whether it harmonizes the existing taxonomies, i.e., whether our theory can instantiate the others. This is the first step to define which taxonomies are best suited to approach DevOps culture and practices according to the companies’ objectives and capabilities. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{KHALAJZADEH2020100964,
title = {An end-to-end model-based approach to support big data analytics development},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100964},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100964},
url = {https://www.sciencedirect.com/science/article/pii/S2590118420300241},
author1 = {Hourieh Khalajzadeh and Andrew J. Simmons and Mohamed Abdelrazek and John Grundy and John Hosking and Qiang He},
keywords = {Big data analytics, Big data modeling, Big data toolkits, Domain-specific visual languages, Multidisciplinary teams, End-user tools},
abstract = {We present BiDaML 2.0, an integrated suite of visual languages and supporting tool to help multidisciplinary teams with the design of big data analytics solutions. BiDaML tool support provides a platform for efficiently producing BiDaML diagrams and facilitating their design, creation, report and code generation. We evaluated BiDaML using two types of evaluations, a theoretical analysis using the “physics of notations”, and an empirical study with 1) a group of 12 target end-users and 2) five individual end-users. Participants mostly agreed that BiDaML was straightforward to understand/learn, and prefer BiDaML for supporting complex data analytics solution modeling than other modeling languages.}
}
@article{MUSTAFA20211819,
title = {Automated Test Case Generation from Requirements: A Systematic Literature Review},
journal = {Computers, Materials and Continua},
volume = {67},
number = {2},
pages = {1819-1833},
year = {2021},
issn = {1546-2218},
doi = {https://doi.org/10.32604/cmc.2021.014391},
url = {https://www.sciencedirect.com/science/article/pii/S1546221821010778},
author1 = {Ahmad Mustafa and Wan M. N. Wan-Kadir and Noraini Ibrahim and Muhammad Arif Shah and Muhammad Younas and Atif Khan and Mahdi Zareei and Faisal Alanazi},
keywords = {Test case generation, functional testing techniques, requirements-based test case generation, system testing, natural language requirement, requirements tractability, coverage criteria},
abstract = {Software testing is an important and cost intensive activity in software development. The major contribution in cost is due to test case generations. Requirement-based testing is an approach in which test cases are derivative from requirements without considering the implementation’s internal structure. Requirement-based testing includes functional and nonfunctional requirements. The objective of this study is to explore the approaches that generate test cases from requirements. A systematic literature review based on two research questions and extensive quality assessment criteria includes studies. The study identifies 30 primary studies from 410 studies spanned from 2000 to 2018. The review’s finding shows that 53% of journal papers, 42% of conference papers, and 5% of book chapters’ address requirements-based testing. Most of the studies use UML, activity, and use case diagrams for test case generation from requirements. One of the significant lessons learned is that most software testing errors are traced back to errors in natural language requirements. A substantial amount of work focuses on UML diagrams for test case generations, which cannot capture all the system’s developed attributes. Furthermore, there is a lack of UML-based models that can generate test cases from natural language requirements by refining them in context. Coverage criteria indicate how efficiently the testing has been performed 12.37% of studies use requirements coverage, 20% of studies cover path coverage, and 17% study basic coverage.}
}
@article{HAKIRI2024110350,
title = {A comprehensive survey on digital twin for future networks and emerging Internet of Things industry},
journal = {Computer Networks},
volume = {244},
pages = {110350},
year = {2024},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2024.110350},
url = {https://www.sciencedirect.com/science/article/pii/S1389128624001828},
author1 = {Akram Hakiri and Aniruddha Gokhale and Sadok Ben Yahia and Nedra Mellouli},
keywords = {Digital twin, Internet of Things, Interoperability, Standardization, Frameworks and prototypes, Security},
abstract = {The rapid growth of industrial digitalization in the Industry 4.0 era is fundamentally transforming the industrial sector by connecting products, machines, and people, offering real-time digital models to allow self-diagnosis, self-optimization and self-configuration. However, this uptake in such a digital transformation faces numerous obstacles. For example, the lack of real-time data feeds to perform custom closed-loop control and realize common, powerful industrial systems, the complexity of traditional tools and their inability in finding effective solutions to industry problems, lack of capabilities to experiment rapidly on innovative ideas, and the absence of continuous real-time interactions between physical objects and their simulation representations along with reliable two-way communications, are key barriers towards the adoption of such a digital transformation. Digital twins hold the promise of improving maintainability and deployability, enabling flexibility, auditability, and responsiveness to changing conditions, allowing continuous learning, monitoring and actuation, and allowing easy integration of new technologies in order to deploy open, scalable and reliable Industrial Internet of Things (IIoT). A critical understanding of this emerging paradigm is necessary to address the multiple dimensions of challenges in realizing digital twins at scale and create new means to generate knowledge in the industrial IoT. To address these requirements, this paper surveys existing digital twin along software technologies, standardization efforts and the wide range of recent and state-of-the-art digital twin-based projects; presents diverse use cases that can benefit from this emerging technology; followed by an in-depth discussion of the major challenges in this area drawing upon the research status and key trends in Digital Twins.}
}
@article{CIAVOTTA2017931,
title = {A Microservice-based Middleware for the Digital Factory},
journal = {Procedia Manufacturing},
volume = {11},
pages = {931-938},
year = {2017},
note = {27th International Conference on Flexible Automation and Intelligent Manufacturing, FAIM2017, 27-30 June 2017, Modena, Italy},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2017.07.197},
url = {https://www.sciencedirect.com/science/article/pii/S2351978917304055},
author1 = {Michele Ciavotta and Marino Alge and Silvia Menato and Diego Rovere and Paolo Pedrazzoli},
keywords = {Microservices, Real-digital synchronization, Cyber Physical Systems, Data Stream Analysis, Smart Factory, Industrie 4.0},
abstract = {In recent years a considerable effort has been spent by research and industrial communities in the digitalization of production environments with the main objective of achieving a new automation paradigm, more flexible, responsive to changes, and safe. This paper presents the architecture, and discusses the benefits, of a distributed middleware prototype supporting a new generation of smart-factory-enabled applications with special attention paid to simulation tools. Devised within the scope of MAYA EU project, the proposed platform aims at being the first solution capable of empowering shop-floor Cyber-Physical-Systems (CPSs), providing an environment for their Digital Twin along the whole plant life-cycle. The platform implements a microservice IoT-Big Data architecture supporting the distributed publication of multidisciplinary simulation models, managing in an optimized way streams of data coming from the shop-floor for real-digital synchronization, ensuring security and confidentiality of sensible data.}
}
@article{BRUNELIERE2022104672,
title = {AIDOaRt: AI-augmented Automation for DevOps, a model-based framework for continuous development in Cyber–Physical Systems},
journal = {Microprocessors and Microsystems},
volume = {94},
pages = {104672},
year = {2022},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2022.104672},
url = {https://www.sciencedirect.com/science/article/pii/S0141933122002022},
author1 = {Hugo Bruneliere and Vittoriano Muttillo and Romina Eramo and Luca Berardinelli and Abel Gómez and Alessandra Bagnato and Andrey Sadovykh and Antonio Cicchetti},
keywords = {Cyber–Physical Systems, Continuous development, System engineering, Software engineering, Model Driven Engineering, Artificial Intelligence, DevOps, AIOps},
abstract = {The advent of complex Cyber–Physical Systems (CPSs) creates the need for more efficient engineering processes. Recently, DevOps promoted the idea of considering a closer continuous integration between system development (including its design) and operational deployment. Despite their use being still currently limited, Artificial Intelligence (AI) techniques are suitable candidates for improving such system engineering activities (cf. AIOps). In this context, AIDOaRT is a large European collaborative project that aims at providing AI-augmented automation capabilities to better support the modeling, coding, testing, monitoring, and continuous development of CPSs. The project proposes to combine Model Driven Engineering principles and techniques with AI-enhanced methods and tools for engineering more trustable CPSs. The resulting framework will (1) enable the dynamic observation and analysis of system data collected at both runtime and design time and (2) provide dedicated AI-augmented solutions that will then be validated in concrete industrial cases. This paper describes the main research objectives and underlying paradigms of the AIDOaRt project. It also introduces the conceptual architecture and proposed approach of the AIDOaRt overall solution. Finally, it reports on the actual project practices and discusses the current results and future plans.}
}
@article{OZKAYA2020100963,
title = {Understanding Practitioners’ Challenges on Software Modeling: A Survey},
journal = {Journal of Computer Languages},
volume = {58},
pages = {100963},
year = {2020},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2020.100963},
url = {https://www.sciencedirect.com/science/article/pii/S259011842030023X},
author1 = {Mert Ozkaya and Ferhat Erata},
keywords = {model-driven software development, software modeling challenges, practitioners, survey},
abstract = {Software modeling is considered as the high-level design technique for describing abstract statements about software systems. While some practitioners create models for the early analysis of their design decisions and generating code from their models, some practitioners create models for the eased communication among stakeholders. There also exist practitioners who ignore modeling and directly proceed with coding. We aim in this paper to understand the challenges that practitioners face with in their software modeling activities. We surveyed 80 practitioners from 18 countries who work in 18 different industries. We focussed on 8 categories of software modeling challenges: (i) managing the language complexity, (ii) extending modeling languages, (iii) domain-specific modeling environments, (iv) developing formal modeling languages, (v) analysing models, (vi) separation of concerns, (vii) transforming models, and (viii) managing models. As the results show, the separation of concerns is the least challenging category for practitioners, while analysing models is the top challenging category. Various concrete challenges in different categories have been observed, including (i) using the modeling languages with steep learning-curve, (ii) extending the language semantics without inconsistencies and updating the language tools accordingly, (iii) evolving the DSL tools with new requirements, (iv) defining the languages’ formal semantics in terms of the translations in any formal languages, (v) decomposing models into separate viewpoints and analysing the consistencies between different viewpoint models, (vi) the consistent model transformation and the model synchronisations, (vii) using model checkers for formal analysis, and (viii) versioning models.}
}
@article{GINERMIGUELEZ2023101209,
title = {A domain-specific language for describing machine learning datasets},
journal = {Journal of Computer Languages},
volume = {76},
pages = {101209},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101209},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000199},
author1 = {Joan Giner-Miguelez and Abel Gómez and Jordi Cabot},
keywords = {Datasets, Machine learning, MDE, Domain-specific languages, Fairness},
abstract = {Datasets are essential for training and evaluating machine learning (ML) models. However, they are also at the root of many undesirable model behaviors, such as biased predictions. To address this issue, the machine learning community is proposing a data-centric cultural shift, where data issues are given the attention they deserve and more standard practices for gathering and describing datasets are discussed and established. So far, these proposals are mostly high-level guidelines described in natural language and, as such, they are difficult to formalize and apply to particular datasets. In this sense, and inspired by these proposals, we define a new domain-specific language (DSL) to precisely describe machine learning datasets in terms of their structure, provenance, and social concerns. We believe this DSL will facilitate any ML initiative to leverage and benefit from this data-centric shift in ML (e.g., selecting the most appropriate dataset for a new project or better replicating other ML results). The DSL is implemented as a Visual Studio Code plugin, and it has been published under an open-source license.}
}
@incollection{RISCOMARTIN2019291,
title = {Chapter 14 - Model Management and Execution in DEVS Unified Process},
editor = {Lin Zhang and Bernard P. Zeigler and Yuanjun laili},
booktitle = {Model Engineering for Simulation},
publisher = {Academic Press},
pages = {291-313},
year = {2019},
isbn = {978-0-12-813543-3},
doi = {https://doi.org/10.1016/B978-0-12-813543-3.00014-7},
url = {https://www.sciencedirect.com/science/article/pii/B9780128135433000147},
author1 = {José L. Risco-Martín and Saurabh Mittal},
keywords = {Modeling and simulation, Discrete-event systems, DUNIP, Parallel DEVS, System entity structure, Platform-specific modeling},
abstract = {Any modeling and simulation (M&S) endeavor comprise of two activities: modeling and simulation. Though each is an independent discipline in itself, often M&S is taken together as an integrated approach executed through software engineering practices. If not developed through a component-based methodology, it may result in various problems at later stages of any large scale M&S effort, for example, maintenance, integration, augmentation, verification, and validation. In this chapter, we describe the model engineering process for projects that subscribe to discrete-event world-view. We present model management from various perspectives as described in DEVS Unified Process in Mittal and Risco-Martín's earlier work. After specification of model, we then focus on the transparent execution of models in a net-centric system of systems environment and present state of the art in model and simulation interoperability within DUNIP.}
}
@article{TUMA2021111003,
title = {Finding security threats that matter: Two industrial case studies},
journal = {Journal of Systems and Software},
volume = {179},
pages = {111003},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111003},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100100X},
author1 = {Katja Tuma and Christian Sandberg and Urban Thorsson and Mathias Widman and Thomas Herpel and Riccardo Scandariato},
keywords = {Threat analysis, Risk, STRIDE, Case study, Empirical software engineering, Security deskilling},
abstract = {In the past decade, speed has become an essential trait of software development (e.g., agile, continuous integration, DevOps) and any inefficiency is considered unaffordable time waster. Such a fast pace causes challenges for architectural threat analysis. Leading techniques for threat analysis, like STRIDE, have the advantage of being systematic. However, they are not equipped to discern between important and less critical threats, while the threats are being discovered. Consequently, many threats are discarded at a later time, when their risk value is assessed. An alternative technique, called eSTRIDE, promises to remove these inefficiencies by focusing the analysis on the critical parts of the architecture. Yet, no empirical evidence exists about the actual effect of trading off systematicity, for a more focused attention on high-priority threats. This paper contributes with an empirical study comparing these two approaches in the context of two industrial case studies. We found that the two approaches yield the same number of security threats during a given time frame. However, participants using eSTRIDE found twice as many high-priority threats. The underlying analysis procedures cause similarities and differences in the execution. In addition, security expertise has an effect (albeit small) on the quality of analysis outcomes and execution.}
}
@article{NAVEED2024107423,
title = {Model driven engineering for machine learning components: A systematic literature review},
journal = {Information and Software Technology},
volume = {169},
pages = {107423},
year = {2024},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107423},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924000284},
author1 = {Hira Naveed and Chetan Arora and Hourieh Khalajzadeh and John Grundy and Omar Haggag},
keywords = {Model driven engineering, Software engineering, Artificial intelligence, Machine learning, Systematic literature review},
abstract = {Context:
Machine Learning (ML) has become widely adopted as a component in many modern software applications. Due to the large volumes of data available, organizations want to increasingly leverage their data to extract meaningful insights and enhance business profitability. ML components enable predictive capabilities, anomaly detection, recommendation, accurate image and text processing, and informed decision-making. However, developing systems with ML components is not trivial; it requires time, effort, knowledge, and expertise in ML, data processing, and software engineering. There have been several studies on the use of model-driven engineering (MDE) techniques to address these challenges when developing traditional software and cyber–physical systems. Recently, there has been a growing interest in applying MDE for systems with ML components.
Objective:
The goal of this study is to further explore the promising intersection of MDE with ML (MDE4ML) through a systematic literature review (SLR). Through this SLR, we wanted to analyze existing studies, including their motivations, MDE solutions, evaluation techniques, key benefits and limitations.
Method:
Our SLR is conducted following the well-established guidelines by Kitchenham. We started by devising a protocol and systematically searching seven databases, which resulted in 3934 papers. After iterative filtering, we selected 46 highly relevant primary studies for data extraction, synthesis, and reporting.
Results:
We analyzed selected studies with respect to several areas of interest and identified the following: (1) the key motivations behind using MDE4ML; (2) a variety of MDE solutions applied, such as modeling languages, model transformations, tool support, targeted ML aspects, contributions and more; (3) the evaluation techniques and metrics used; and (4) the limitations and directions for future work. We also discuss the gaps in existing literature and provide recommendations for future research.
Conclusion:
This SLR highlights current trends, gaps and future research directions in the field of MDE4ML, benefiting both researchers and practitioners.}
}
@article{KRITIKOS2018155,
title = {Reprint of “Towards a security-enhanced PaaS platform for multi-cloud applications”},
journal = {Future Generation Computer Systems},
volume = {78},
pages = {155-175},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2016.11.014},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X16306343},
author1 = {Kyriakos Kritikos and Tom Kirkham and Bartosz Kryza and Philippe Massonet},
keywords = {Cloud, Security, Meta-model, Policies, Roles, SAML},
abstract = {Multi-cloud adaptive application provisioning can solve the vendor lock-in problem and allows optimising user requirements by selecting the best from the multitude of services offered by different cloud providers. To this end, such provisioning type is increasingly supported by new or existing research prototypes and platforms. One major concern, actually preventing users from moving to the cloud, comes with respect to security, which becomes more complex in multi-cloud settings. Such a concern spans two main aspects: (a) suitable access control on user personal data, VMs and platform services and (b) planning and adapting application deployments based on security requirements. As such, this paper addresses both security aspects by proposing a novel model-driven approach and architecture which secures multi-cloud platforms, enables users to have their own private space and guarantees that application deployments are not only constructed based on but can also maintain a certain user-required security level. Such a solution exploits state-of-the-art security standards, security software and secure model management technology. Moreover, it covers different access control scenarios involving external, web-based and programmatic user authentication.}
}
@article{LASCU2015261,
title = {Automatic deployment of component-based applications},
journal = {Science of Computer Programming},
volume = {113},
pages = {261-284},
year = {2015},
note = {Formal Aspects of Component Software (FACS 2013)},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2015.07.006},
url = {https://www.sciencedirect.com/science/article/pii/S0167642315001409},
author1 = {Tudor A. Lascu and Jacopo Mauro and Gianluigi Zavattaro},
keywords = {Cloud applications management, Component configuration, Deployment planning},
abstract = {In distributed systems like those based on cloud or service-oriented frameworks, applications are typically assembled by deploying and connecting a large number of heterogeneous software components, spanning from fine-grained packages to coarse-grained complex services. Automation techniques and tools have been proposed to ease the deployment process of these complex system. By relying on a formal model of components, we describe a sound and complete algorithm for computing the sequence of actions that permits the deployment of a desired configuration even in the presence of circular dependencies among components. We give a proof for the polynomiality of the devised algorithm and exploit it to develop METIS, a tool for computing deployment plans. The validation of METIS has been performed in two ways: on the one hand, by considering artificial scenarios consisting of a huge number of different components synthesized by following typical configuration patterns and, on the other hand, by exploiting it to deploy real-life installations of a WordPress blogging service.}
}
@article{SAADATMAND2023104967,
title = {SmartDelta project: Automated quality assurance and optimization across product versions and variants},
journal = {Microprocessors and Microsystems},
volume = {103},
pages = {104967},
year = {2023},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2023.104967},
url = {https://www.sciencedirect.com/science/article/pii/S0141933123002119},
author1 = {Mehrdad Saadatmand and Muhammad Abbas and Eduard Paul Enoiu and Bernd-Holger Schlingloff and Wasif Afzal and Benedikt Dornauer and Michael Felderer},
keywords = {Research project, Software product lines, Software variants, Continuous system engineering, Software quality, ITEA, EUREKA},
abstract = {Software systems are often built in increments with additional features or enhancements on top of existing products. This incremental development may result in the deterioration of certain quality aspects. In other words, the software can be considered an evolving entity emanating different quality characteristics as it gets updated over time with new features or deployed in different operational environments. Approaching software development with this mindset and awareness regarding quality evolution over time can be a key factor for the long-term success of a company in today’s highly competitive market of industrial software-intensive products. Therefore, it is important to be able to accurately analyze and determine the quality implications of each change and increment to a software system. To address this challenge, the multinational SmartDelta project develops automated solutions for the quality assessment of product deltas in a continuous engineering environment. The project provides smart analytics from development artifacts and system executions, offering insights into quality degradation or improvements across different product versions, and providing recommendations for the next builds. This paper presents the challenges in incremental software development tackled in the scope of the SmartDelta project, and the solutions that are produced and planned in the project, along with the industrial impact of the project for software-intensive industrial systems.}
}
@incollection{RODRIGUEZ2019135,
title = {Chapter Four - Advances in Using Agile and Lean Processes for Software Development},
editor = {Atif M. Memon},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {113},
pages = {135-224},
year = {2019},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2018.03.014},
url = {https://www.sciencedirect.com/science/article/pii/S0065245818300299},
author1 = {Pilar Rodríguez and Mika Mäntylä and Markku Oivo and Lucy Ellen Lwakatare and Pertti Seppänen and Pasi Kuvaja},
keywords = {Software processes, Agile software development, Lean software development, Lean thinking, Leagility, Rapid releases, Continuous delivery, Continuous deployment, DevOps, Lean startup, Metrics, Technical debt},
abstract = {Software development processes have evolved according to market needs. Fast changing conditions that characterize current software markets have favored methods advocating speed and flexibility. Agile and Lean software development are in the forefront of these methods. This chapter presents a unified view of Agile software development, Lean software development, and most recent advances toward rapid releases. First, we introduce the area and explain the reasons why the software development industry begun to move into this direction in the late 1990s. Section 2 characterizes the research trends on Agile software development. This section helps understand the relevance of Agile software development in the research literature. Section 3 provides a walk through the roots of Agile and Lean thinking, as they originally emerged in manufacturing. Section 4 develops into Agile and Lean for software development. Main characteristics and most popular methods and practices of Agile and Lean software development are developed in this section. Section 5 centers on rapid releases, continuous delivery, and continuous deployment, the latest advances in the area to get speed. The concepts of DevOps, as a means to take full (end-to-end) advantage of Agile and Lean, and Lean start-up, as an approach to foster innovation, are the focus of the two following 6 DevOps, 7 The Lean Startup Movement. Finally, Section 8 focuses on two important aspects of Agile and Lean software development: (1) metrics to guide decision making and (2) technical debt as a mechanism to gain business advantage. To wrap up the chapter, we peer into future directions in the area.}
}
@article{KANIJ2024112169,
title = {Enhancing understanding and addressing gender bias in IT/SE job advertisements},
journal = {Journal of Systems and Software},
volume = {217},
pages = {112169},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112169},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002140},
author1 = {Tanjila Kanij and John Grundy and Jennifer McIntosh},
keywords = {Job advertisement, Gender bias},
abstract = {The majority of Information Technology (IT)/Software Engineering (SE) professionals are male. A potential reason for the low number of female IT/SE professionals might be that the roles and the way they are advertised are biased towards male candidates. The aim of this research is to collect information about the present state of practice of gender inclusiveness within IT/SE job advertisements and how, if needed, we might improve this. We conducted a survey of hiring managers and IT/SE professionals (who are employed in IT/SE roles). The survey collected their general views on gender bias within job advertisements. According to their opinions, job advertisements are often biased towards male candidates. Based on the review and suggestions from our participants we developed a set of recommendations to help hiring managers design more gender inclusive SE job advertisements. This will be a first step toward developing a gender balanced SE workforce.}
}
@article{CASTELLANOS2021110869,
title = {ACCORDANT: A domain specific-model and DevOps approach for big data analytics architectures},
journal = {Journal of Systems and Software},
volume = {172},
pages = {110869},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110869},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302594},
author1 = {Camilo Castellanos and Carlos A. Varela and Dario Correal},
keywords = {Software architecture, Big data analytics deployment, DevOps, Domain-specific model, Quality scenarios, Performance monitoring},
abstract = {Big data analytics (BDA) applications use machine learning algorithms to extract valuable insights from large, fast, and heterogeneous data sources. New software engineering challenges for BDA applications include ensuring performance levels of data-driven algorithms even in the presence of large data volume, velocity, and variety (3Vs). BDA software complexity frequently leads to delayed deployments, longer development cycles, and challenging performance assessment. This paper proposes a Domain-Specific Model (DSM), and DevOps practices to design, deploy, and monitor performance metrics in BDA applications. Our proposal includes a design process, and a framework to define architectural inputs, software components, and deployment strategies through integrated high-level abstractions to enable QS monitoring. We evaluate our approach with four use cases from different domains to demonstrate a high level of generalization. Our results show a shorter deployment and monitoring times, and a higher gain factor per iteration compared to similar approaches.}
}
@article{ALSHARA2018339,
title = {CoMe4ACloud: An end-to-end framework for autonomic Cloud systems},
journal = {Future Generation Computer Systems},
volume = {86},
pages = {339-354},
year = {2018},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.03.039},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X17320605},
author1 = {Zakarea Al-Shara and Frederico Alvares and Hugo Bruneliere and Jonathan Lejeune and Charles Prud’Homme and Thomas Ledoux},
keywords = {Cloud Computing, Autonomic Computing, Model Driven Engineering, Constraint Programming},
abstract = {Autonomic Computing has largely contributed to the development of self-manageable Cloud services. It notably allows freeing Cloud administrators of the burden of manually managing varying-demand services, while still enforcing Service-Level Agreements (SLAs). All Cloud artifacts, regardless of the layer carrying them, share many common characteristics. Thus, it should be possible to specify, (re)configure and monitor any XaaS (Anything-as-a-Service) layer in an homogeneous way. To this end, the CoMe4ACloud approach proposes a generic model-based architecture for autonomic management of Cloud systems. We derive a generic unique Autonomic Manager (AM) capable of managing any Cloud service, regardless of the layer. This AM is based on a constraint solver which aims at finding the optimal configuration for the modeled XaaS, i.e. the best balance between costs and revenues while meeting the constraints established by the SLA. We evaluate our approach in two different ways. Firstly, we analyze qualitatively the impact of the AM behavior on the system configuration when a given series of events occurs. We show that the AM takes decisions in less than 10 s for several hundred nodes simulating virtual/physical machines. Secondly, we demonstrate the feasibility of the integration with real Cloud systems, such as Openstack, while still remaining generic. Finally, we discuss our approach according to the current state-of-the-art.}
}
@article{DINTEN2024100668,
title = {Model-based tool for the design, configuration and deployment of data-intensive applications in hybrid environments: An Industry 4.0 case study},
journal = {Journal of Industrial Information Integration},
volume = {41},
pages = {100668},
year = {2024},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2024.100668},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X24001122},
author1 = {Ricardo Dintén and Patricia {López Martínez} and Marta Zorrilla},
keywords = {Model-based, Digital platform, Containerisation, Service-oriented, Cloud/edge computing, Internet of things},
abstract = {The fourth industrial revolution advocates the reformulation of industrial processes to achieve the end-to-end (provider-customer) digitalisation of the industrial sector. As is well known, the industrial environment is very complex, where legacy systems must interoperate and integrate with modern devices and sensors. Communication among them requires specific and costly developments, so architectures based on data sharing and services implementation are considered one of the most flexible and appropriate technological solutions to gradually achieve the desired horizontal and vertical integration of the value chain. The design and deployment of data-intensive applications is not straightforward, therefore this paper proposes a model-based tool to characterise the different elements to be configured in an application and to make its deployment easier by generating configuration, orchestration and deployment files and sending them to the corresponding nodes for their execution. In few words, this article highlights the advantages of distributed and data-centric architectures to face the challenge of integration and interoperability in data-intensive complex systems and presents the extension of the RAI4 metamodel proposed in Martínez et al. (2021) that now allows specifying how, containerised or not, and where, on the cloud, fog, edge or on-premise, each service can be hosted according to its functional and non-functional requirements, mainly issues related with real-time, security and cyber physical hardware dependencies. For the sake of comprehension, a pseudo-real use case addressed to pre-process and store pollution data from environmental sensors installed in a smart city is described in detail, including different deployment settings.}
}
@article{SHEHAB2024111914,
title = {Commit-time defect prediction using one-class classification},
journal = {Journal of Systems and Software},
volume = {208},
pages = {111914},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111914},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003096},
author1 = {Mohammed A. Shehab and Wael Khreich and Abdelwahab Hamou-Lhadj and Issam Sedki},
keywords = {Just-In-Time Software Defect Prediction (JIT-SDP), One-class classification, Machine learning, Software maintenance and evolution, Software reliability},
abstract = {Existing Just-In-Time Software Defect Prediction methods suffer from the data imbalance problem, where the majority class (normal commits) significantly outnumbers the minority class (buggy commits). This results in a higher probability of misclassification. Various data balancing techniques have been proposed to address this challenge with varying degrees of success. In this study, we propose an approach that rely on One-Class Classification (OCC) to train models using data from the majority class only. This eliminates the need for data balancing. We compare the accuracy of three OCC algorithms - One-class SVM, Isolation Forest, and One-class k-NN - to their binary counterparts - SVM, Random Forest, and k-NN - on 34 software projects. Our results show that the data imbalance ratio (the proportion of normal to buggy commits) plays a crucial role in determining the optimal classification approach. We found that for projects with medium to high imbalance ratio, OCC algorithms outperform binary classifiers with and without data balancing, using cross and time-sensitive validation approaches. Furthermore, we found that OCC methods require fewer features for projects with medium to high IR, reducing the computational overhead of training and response time while providing a better understanding of the data and algorithm behavior.}
}
@article{DALIBOR2022101117,
title = {Generating customized low-code development platforms for digital twins},
journal = {Journal of Computer Languages},
volume = {70},
pages = {101117},
year = {2022},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2022.101117},
url = {https://www.sciencedirect.com/science/article/pii/S2590118422000235},
author1 = {Manuela Dalibor and Malte Heithoff and Judith Michael and Lukas Netz and Jérôme Pfeiffer and Bernhard Rumpe and Simon Varga and Andreas Wortmann},
keywords = {Digital twin, Low-code development platform, Domain-specific languages, Model-driven engineering, Code generation},
abstract = {A digital twin improves our use of a cyber–physical system and understanding of its emerging behavior. To this effect, a digital twin is to be developed and configured and potentially also operated by domain experts, who rarely have a professional software engineering background and for whom easy access and support, e.g., in form of low-code platforms are missing. In this paper, we report on an integrated method for the model-driven engineering of low-code development platforms for digital twins that enables domain experts to create and operate digital twins for cyber–physical systems using the most appropriate modeling languages. The foundation of this method is (1) a code generation infrastructure for information systems combined with (2) an extensible base architecture for self-adaptive digital twins and (3) reusable language components for their configuration. Using this method, software engineers first configure the information system with the required modeling languages to generate the low-code development platform for digital twins before domain experts leverage the generated platform to create digital twins. This two-step method facilitates creating tailored low-code development platforms as well as creating and operating customized digital twins for a variety of applications.}
}
@article{BARTUSEVICS20153,
title = {Models for Implementation of Software Configuration Management},
journal = {Procedia Computer Science},
volume = {43},
pages = {3-10},
year = {2015},
note = {ICTE in Regional Development, December 2014, Valmiera, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2014.12.002},
url = {https://www.sciencedirect.com/science/article/pii/S1877050914015701},
author1 = {Arturs Bartusevics and Leonids Novickis},
keywords = {Software Configuration Management, Model-Driven Approach.},
abstract = {The main scope of software configuration management is control of software evolution process to include at final version only valid and tested items. To achieve this, software configuration management have to prepare solutions for tasks such as identification of software configuration items, version control, build and deploy management etc. The paper provides new model-driven approach for implementation of software configuration management. New approach is supported by set of models to describe software configuration management process from different sides. New approach helps to organize existing solutions in parameterized way that increase ability of its reuse. Current paper introduces to problems in software configuration management area and main trends of new solutions. After introduction, new model-driven approach described. The second part of paper provides models for new approach. Presentation of models combined with simplified use case, which illustrates practical application of models. Finally, directions of further researches are provided.}
}
@article{GILL2019507,
title = {Configuration information system architecture: Insights from applied action design research},
journal = {Information & Management},
volume = {56},
number = {4},
pages = {507-525},
year = {2019},
issn = {0378-7206},
doi = {https://doi.org/10.1016/j.im.2018.09.011},
url = {https://www.sciencedirect.com/science/article/pii/S0378720617310091},
author1 = {Asif Qumer Gill and Eng Chew},
keywords = {Action design research, Architecture design, Configuration information system, Resilience, Service},
abstract = {One of the critical information systems that enables service resilience is the service configuration information system (CiS). The fundamental challenge for organisations is the effective designing and implementation of the CiS architecture. This paper addresses this important research problem and reports insights from a completed applied action design research (ADR) project in an Australian financial services organisation. This paper aims to provide guidance to researchers and practitioners contemplating ADR, rooted in the organisational context, for practice-oriented academia-industry collaborative research. This research also contributes in terms of the CiS reference architecture design knowledge and demonstrates the applicability of the ADR method.}
}
@article{TSILIONIS2022106734,
title = {A model-driven framework to support strategic agility: Value-added perspective},
journal = {Information and Software Technology},
volume = {141},
pages = {106734},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106734},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001841},
author1 = {Konstantinos Tsilionis and Yves Wautelet},
keywords = {Strategic agility, IT governance, Strategic value, Model-driven development, Agile, Agility, i* framework, Scaled agile framework, SAFe, MoDrIGo, StratAMoDrIGo},
abstract = {Context:
The Covid-19 pandemic has shown the entire world that the habits of work, freedom, and consumption can change quickly and significantly for an undetermined amount of time. A dynamic environment as such, prompts organizations to move fast in order to leverage changing circumstances as sources of opportunity rather than deadly threats. Drastic changes in work organization, consumption habits, compliance, etc., may require firms to quickly adopt new technology delivering all sorts of added value.
Objective:
The development and adoption of new technology – structurally impacting the way the organization conducts its activities – requires a considerable amount of effort in a short time frame, thus rendering it a governance decision where the alignment of the technology’s adoption and use to the long term strategy needs to be evaluated. The short time frame requiring fast response implies that agility should not remain a development or management/operational concept but should also be adopted onto the strategic layer.
Method:
Design Science Research (DSR) has been applied to build-up a framework supporting strategic agility in a model-driven fashion called Strategic Agile Model Driven IT Governance (StratAMoDrIGo). The relevance, rigor and design cycles of DSR have been applied and presented.
Results:
StratAMoDrIGo is based on the identification of sources of value for the organization’s strategy, its stakeholders and the users of the implemented/adopted technology. Relevant concepts are consolidated in an ontology of which the application uses the NFR Model at strategic-level and the i* Strategic Rationale Model at management-level. The proposal is applied on the case of an hospital facing the Covid-19 pandemic.
Conclusion:
The value brought by strategic opportunities’ adoption to the organization, stakeholders and users can be evaluated ex ante through conceptual models.}
}
@article{SADOVYKH2024103047,
title = {An iterative approach for model-based requirements engineering in large collaborative projects: A detailed experience report},
journal = {Science of Computer Programming},
volume = {232},
pages = {103047},
year = {2024},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2023.103047},
url = {https://www.sciencedirect.com/science/article/pii/S0167642323001296},
author1 = {Andrey Sadovykh and Bilal Said and Dragos Truscan and Hugo Bruneliere},
keywords = {Requirements engineering, Model-based engineering, Collaborative projects, Experience report},
abstract = {In this paper, we report on our 7 years of practical experience designing, developing, deploying, using, and evolving an iterative Model-based Requirements Engineering (MBRE) approach and language in the context of five large European collaborative projects providing complex software-intensive solutions. Based on significant data sets collected both during project execution and via surveys realized afterward, we demonstrate that such a model-based approach can bring interesting benefits in terms of scalability (e.g., a large number of handled requirements), heterogeneity (e.g., partners with different types of RE background), adaptability and extensibility (e.g., to various project's needs), traceability (e.g., from the requirements to the software components), automation (e.g., documentation generation), consistency and quality (e.g., central model), and usefulness or usability (e.g., actual deployment and practical use). Along the way, we illustrate the application of our MBRE approach and language with concrete elements from these several European collaborative projects. More broadly, we discuss the general benefits and current limitations of using such a model-based approach and corresponding language, as well as the related lessons we learned during these past years.}
}
@article{VILLAR2020103244,
title = {Mega-modeling of complex, distributed, heterogeneous CPS systems},
journal = {Microprocessors and Microsystems},
volume = {78},
pages = {103244},
year = {2020},
issn = {0141-9331},
doi = {https://doi.org/10.1016/j.micpro.2020.103244},
url = {https://www.sciencedirect.com/science/article/pii/S0141933120304051},
author1 = {Eugenio Villar and Javier Merino and Hector Posadas and Rafik Henia and Laurent Rioux},
abstract = {Model-Driven Design (MDD) has proven to be a powerful technology to address the development of increasingly complex embedded systems. Beyond complexity itself, challenges come from the need to deal with parallelism and heterogeneity. System design must target different execution platforms with different OSs and HW resources, even bare-metal, support local and distributed systems, and integrate on top of these heterogeneous platforms multiple functional component coming from different sources (developed from scratch, legacy code and third-party code), with different behaviors operating under different models of computation and communication. Additionally, system optimization to improve performance, power consumption, cost, etc. requires analyzing huge lists of possible design solutions. Addressing these challenges require flexible design technologies able to support from a single-source model its architectural mapping to different computing resources, of different kind and in different platforms. Traditional MDD methods and tools typically rely on fixed elements, which makes difficult their integration under this variability. For example, it is unlikely to integrate in the same system legacy code with a third-party component. Usually some re-coding is required to enable such interconnection. This paper proposes a UML/MARTE system modeling methodology able to address the challenges mentioned above by improving flexibility and scalability. This approach is illustrated and demonstrated on a flight management system. The model is flexible enough to be adapted to different architectural solutions with a minimal effort by changing its underlying Model of Computation and Communication (MoCC). Being completely platform independent, from the same model it is possible to explore various solutions on different execution platforms.}
}
@article{ERAMO2024111833,
title = {Architectural support for software performance in continuous software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {207},
pages = {111833},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111833},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223002285},
author1 = {Romina Eramo and Michele Tucci and Daniele {Di Pompeo} and Vittorio Cortellessa and Antinisca {Di Marco} and Davide Taibi},
keywords = {Software architecture, Software performance, Continuous software engineering, DevOps},
abstract = {The continuous software engineering paradigm is gaining popularity in modern development practices, where the interleaving of design and runtime activities is induced by the continuous evolution of software systems. In this context, performance assessment is not easy, but recent studies have shown that architectural models evolving with the software can support this goal. In this paper, we present a mapping study aimed at classifying existing scientific contributions that deal with the architectural support for performance-targeted continuous software engineering. We have applied the systematic mapping methodology to an initial set of 215 potentially relevant papers and selected 66 primary studies that we have analyzed to characterize and classify the current state of research. This classification helps to focus on the main aspects that are being considered in this domain and, mostly, on the emerging findings and implications for future research. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board. (see [https://www.sciencedirect.com/science/article/pii/S0164121221002168] for an example for where to place the statement and how to format it).}
}
@article{GIAMATTEI2024111906,
title = {Monitoring tools for DevOps and microservices: A systematic grey literature review},
journal = {Journal of Systems and Software},
volume = {208},
pages = {111906},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111906},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003011},
author1 = {L. Giamattei and A. Guerriero and R. Pietrantuono and S. Russo and I. Malavolta and T. Islam and M. Dînga and A. Koziolek and S. Singh and M. Armbruster and J.M. Gutierrez-Martinez and S. Caro-Alvaro and D. Rodriguez and S. Weber and J. Henss and E. Fernandez Vogelin and F. Simon Panojo},
keywords = {Monitoring, Microservice, DevOps, MSA, Tools},
abstract = {Microservice-based systems are usually developed according to agile practices like DevOps, which enables rapid and frequent releases to promptly react and adapt to changes. Monitoring is a key enabler for these systems, as they allow to continuously get feedback from the field and support timely and tailored decisions for a quality-driven evolution. In the realm of monitoring tools available for microservices in the DevOps-driven development practice, each with different features, assumptions, and performance, selecting a suitable tool is an as much difficult as impactful task. This article presents the results of a systematic study of the grey literature we performed to identify, classify and analyze the available monitoring tools for DevOps and microservices. We selected and examined a list of 71 monitoring tools, drawing a map of their characteristics, limitations, assumptions, and open challenges, meant to be useful to both researchers and practitioners working in this area. Results are publicly available and replicable. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{BIESIALSKA2021106448,
title = {Big Data analytics in Agile software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {132},
pages = {106448},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106448},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920301981},
author1 = {Katarzyna Biesialska and Xavier Franch and Victor Muntés-Mulero},
keywords = {Agile software development, Software analytics, Data analytics, Machine learning, Artificial intelligence, Literature review},
abstract = {Context:
Over the last decade, Agile methods have changed the software development process in an unparalleled way and with the increasing popularity of Big Data, optimizing development cycles through data analytics is becoming a commodity.
Objective:
Although a myriad of research exists on software analytics as well as on Agile software development (ASD) practice on itself, there exists no systematic overview of the research done on ASD from a data analytics perspective. Therefore, the objective of this work is to make progress by linking ASD with Big Data analytics (BDA).
Method:
As the primary method to find relevant literature on the topic, we performed manual search and snowballing on papers published between 2011 and 2019.
Results:
In total, 88 primary studies were selected and analyzed. Our results show that BDA is employed throughout the whole ASD lifecycle. The results reveal that data-driven software development is focused on the following areas: code repository analytics, defects/bug fixing, testing, project management analytics, and application usage analytics.
Conclusions:
As BDA and ASD are fast-developing areas, improving the productivity of software development teams is one of the most important objectives BDA is facing in the industry. This study provides scholars with information about the state of software analytics research and the current trends as well as applications in the business environment. Whereas, thanks to this literature review, practitioners should be able to understand better how to obtain actionable insights from their software artifacts and on which aspects of data analytics to focus when investing in such initiatives.}
}
@article{AHN2024112268,
title = {Reconstruction of an Execution Architecture View by Identifying Mapping Rules for Connectors},
journal = {Journal of Systems and Software},
pages = {112268},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112268},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224003121},
author1 = {Hwi Ahn and Sungwon Kang and Seonah Lee},
keywords = {Software engineering, Software architecture reconstruction, Execution architecture, Architecture connector},
abstract = {ABSTRACT
An execution architecture view plays a crucial role in depicting the structure of a software system at runtime and analyzing its execution aspects, such as concurrency and performance. However, such execution views are frequently missing in real-world practices. Therefore, researchers have endeavored to reconstruct execution architecture views from software systems. However, existing approaches either require domain experts’ knowledge or are applicable only to systems with particular architecture styles. In this paper, we propose a systematic approach to reconstructing an execution architecture view, without prior knowledge of the components and connectors in the target system. With the proposed approach, by defining a candidate set of execution view connectors and mapping rules from source code to execution view connectors, developers can reconstruct an execution view. To evaluate the proposed approach, we applied it to three real-world software systems. Our evaluation results show that the proposed approach reconstructs an execution architecture with a higher than 86% F1-score and less than 13.9 person-hours.}
}
@article{AHMAD2024102094,
title = {A reference architecture for quantum computing as a service},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {6},
pages = {102094},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102094},
url = {https://www.sciencedirect.com/science/article/pii/S1319157824001836},
author1 = {Aakash Ahmad and Ahmed B. Altamimi and Jamal Aqib},
keywords = {Quantum software, Software engineering, Software architecture, Service computing, Quantum service computing},
abstract = {Quantum computers (QCs) aim to disrupt the status-quo of computing – replacing traditional systems and platforms that are driven by digital circuits and modular software – with hardware and software that operate on the principle of quantum mechanics. QCs that rely on quantum mechanics can exploit quantum circuits (i.e., quantum bits for manipulating quantum gates) to achieve ‘quantum computational supremacy’ over traditional, i.e., digital computing systems. Currently, the issues that impede mass-scale adoption of quantum systems are rooted in the fact that building, maintaining, and/or programming QCs is a complex and radically distinct engineering paradigm when compared to the challenges of classical computing and software engineering. Quantum service orientation is seen as a solution that synergises the research on service computing and quantum software engineering (QSE) to allow developers and users to build and utilise quantum software services based on pay-per-shot utility computing model. The pay-per-shot model represents a single execution of instruction on quantum processing unit and it allows vendors (e.g., Amazon Braket) to offer their QC platforms, simulators, and software services to end-users. This research contributes by (i) developing a reference architecture for enabling Quantum Computing as a Service (QCaaS), (ii) implementing microservices with the quantum-classic split pattern as an architectural use-case, and (iii) evaluating the architecture based on practitioners’ feedback. The proposed reference architecture follows a layered software pattern to support the three phases of service lifecycle namely development, deployment, and split of quantum software services. In the QSE context, the research focuses on unifying architectural methods and service-orientation patterns to promote reuse knowledge and best practices to tackle emerging and futuristic challenges of architecting QCaaS.}
}
@article{FARSHCHI2018531,
title = {Metric selection and anomaly detection for cloud operations using log and metric correlation analysis},
journal = {Journal of Systems and Software},
volume = {137},
pages = {531-549},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.012},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300596},
author1 = {Mostafa Farshchi and Jean-Guy Schneider and Ingo Weber and John Grundy},
keywords = {Cloud application operations, Cloud monitoring, Metric selection, Anomaly detection, Error detection, Log analysis},
abstract = {Cloud computing systems provide the facilities to make application services resilient against failures of individual computing resources. However, resiliency is typically limited by a cloud consumer’s use and operation of cloud resources. In particular, system operations have been reported as one of the leading causes of system-wide outages. This applies specifically to DevOps operations, such as backup, redeployment, upgrade, customized scaling, and migration – which are executed at much higher frequencies now than a decade ago. We address this problem by proposing a novel approach to detect errors in the execution of these kinds of operations, in particular for rolling upgrade operations. Our regression-based approach leverages the correlation between operations’ activity logs and the effect of operation activities on cloud resources. First, we present a metric selection approach based on regression analysis. Second, the output of a regression model of selected metrics is used to derive assertion specifications, which can be used for runtime verification of running operations. We have conducted a set of experiments with different configurations of an upgrade operation on Amazon Web Services, with and without randomly injected faults to demonstrate the utility of our new approach.}
}
@article{DIFRANCESCO201977,
title = {Architecting with microservices: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {150},
pages = {77-97},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.01.001},
url = {https://www.sciencedirect.com/science/article/pii/S0164121219300019},
author1 = {Paolo {Di Francesco} and Patricia Lago and Ivano Malavolta},
keywords = {Microservices, Software architecture, Systematic mapping study},
abstract = {Context
A microservice architecture is composed of a set of small services, each running in its own process and communicating with lightweight mechanisms. Many aspects on architecting with microservices are still unexplored and existing research is still far from being crispy clear.
Objective
We aim at identifying, classifying, and evaluating the state of the art on architecting with microservices from the following perspectives: publication trends, focus of research, and potential for industrial adoption.
Method
We apply the systematic mapping methodology. We rigorously selected 103 primary studies and we defined and applied a classification framework to them for extracting key information for subsequent analysis. We synthesized the obtained data and produced a clear overview of the state of the art.
Results
This work contributes with (i) a classification framework for research studies on architecting with microservices, (ii) a systematic map of current research of the field, (iii) an evaluation of the potential for industrial adoption of research results, and (iv) a discussion of emerging findings and implications for future research.
Conclusion
This study provides a solid, rigorous, and replicable picture of the state of the art on architecting with microservices. Its results can benefit both researchers and practitioners of the field.}
}
@article{DAVIES201488,
title = {Model-driven engineering of information systems: 10 years and 1000 versions},
journal = {Science of Computer Programming},
volume = {89},
pages = {88-104},
year = {2014},
note = {Special issue on Success Stories in Model Driven Engineering},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2013.02.002},
url = {https://www.sciencedirect.com/science/article/pii/S0167642313000270},
author1 = {Jim Davies and Jeremy Gibbons and James Welch and Edward Crichton},
keywords = {Model-driven, Evolution, Data migration, Formal methods, Agile, Databases},
abstract = {This paper reports upon ten years of experience in the development and application of model-driven technology. The technology in question was inspired by work on formal methods: in particular, by the B toolkit. It was used in the development of a number of information systems, all of which were successfully deployed in real world situations. The paper reports upon three systems: one that informed the design of the technology, one that was used by an internal customer, and one that is currently in use outside the development organisation. It records a number of lessons regarding the application of model-driven techniques.}
}
@article{KOURTIS20247,
title = {Towards Continuous Development for Quantum Programming in Decentralized IoT environments},
journal = {Procedia Computer Science},
volume = {238},
pages = {7-14},
year = {2024},
note = {The 15th International Conference on Ambient Systems, Networks and Technologies Networks (ANT) / The 7th International Conference on Emerging Data and Industry 4.0 (EDI40), April 23-25, 2024, Hasselt University, Belgium},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2024.05.190},
url = {https://www.sciencedirect.com/science/article/pii/S1877050924012286},
author1 = {Michail Alexandros Kourtis and Nikolay Tcholtchev and Ilie-Daniel Gheorghe-Pop and Colin Kai-Uwe Becker and Georgios Xylouris and Evangelos Markakis and Matic Petric and Raphael Seidel and Sebastian Bock},
keywords = {Quantum Programming, Swarm Intelligence, Decentralized Computing, Edge, Blockchain},
abstract = {The progression in quantum computing and the rapid development of quantum computation hardware has raised expectations for its application to commercially relevant use cases in the future. However, the need for high-level quantum programming abstractions and targeted use cases paired with vertical applications, which can directly benefit from quantum computing, remains an open challenge. This paper presents our vision for a decentralized architecture for swarm based IoT systems that leverages a high-level continuous development and integration quantum programming suite to support edge processing capabilities for different use cases across the edge-fog-cloud continuum. The planned Quantum DevKit provides the necessary abstractions and low-level backend interfaces for quantum computing infrastructure, enabling edge computation using quantum processing, with extensions to efficient management of Service Level Agreements (SLAs). The paper focuses on the presentation of the development kit and its coupling swarm-based architecture that automates the orchestration of the cloud-to-edge continuum, showcasing the potential of quantum technology in edge processing.}
}
@article{PANAHANDEH2024111917,
title = {ServiceAnomaly: An anomaly detection approach in microservices using distributed traces and profiling metrics},
journal = {Journal of Systems and Software},
volume = {209},
pages = {111917},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111917},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223003126},
author1 = {Mahsa Panahandeh and Abdelwahab Hamou-Lhadj and Mohammad Hamdaqa and James Miller},
keywords = {System observability, Distributed traces, Anomaly detection, Microservice architectures, Software reliability, AIOps},
abstract = {Anomaly detection is an essential activity for identifying abnormal behaviours in microservice-based systems. A common approach is to model the system behaviour during normal operation using either distributed traces or profiling metrics. The model is then used to detect anomalies during system operation. In this paper, we present a new anomaly detection approach, called ServiceAnomaly, for anomaly detection in microservice systems that combines distributed traces and six profiling metrics to build an annotated directed acyclic graph that characterizes the normal behaviour of the system. Unlike existing techniques, our approach captures the context propagation provided by distributed traces as a graph that is annotated with functions characterizing both linear and non-linear relationships between profiling metrics. The final annotated graph is used to detect abnormal executions during system operation. The results of applying our approach to two open-source benchmarks show that our approach detects anomalies with an F1-score up to 86%. We also show how developers can use the annotated graph to reason about the causes of anomalies}
}
@article{SUN2022105034,
title = {A review of Earth Artificial Intelligence},
journal = {Computers & Geosciences},
volume = {159},
pages = {105034},
year = {2022},
issn = {0098-3004},
doi = {https://doi.org/10.1016/j.cageo.2022.105034},
url = {https://www.sciencedirect.com/science/article/pii/S0098300422000036},
author1 = {Ziheng Sun and Laura Sandoval and Robert Crystal-Ornelas and S. Mostafa Mousavi and Jinbo Wang and Cindy Lin and Nicoleta Cristea and Daniel Tong and Wendy Hawley Carande and Xiaogang Ma and Yuhan Rao and James A. Bednar and Amanda Tan and Jianwu Wang and Sanjay Purushotham and Thomas E. Gill and Julien Chastang and Daniel Howard and Benjamin Holt and Chandana Gangodagamage and Peisheng Zhao and Pablo Rivas and Zachary Chester and Javier Orduz and Aji John},
keywords = {Geosphere, Hydrology, Atmosphere, Artificial intelligence/machine learning, Big data, Cyberinfrastructure},
abstract = {In recent years, Earth system sciences are urgently calling for innovation on improving accuracy, enhancing model intelligence level, scaling up operation, and reducing costs in many subdomains amid the exponentially accumulated datasets and the promising artificial intelligence (AI) revolution in computer science. This paper presents work led by the NASA Earth Science Data Systems Working Groups and ESIP machine learning cluster to give a comprehensive overview of AI in Earth sciences. It holistically introduces the current status, technology, use cases, challenges, and opportunities, and provides all the levels of AI practitioners in geosciences with an overall big picture and to “blow away the fog to get a clearer vision” about the future development of Earth AI. The paper covers all the majorspheres in the Earth system and investigates representative AI research in each domain. Widely used AI algorithms and computing cyberinfrastructure are briefly introduced. The mandatory steps in a typical workflow of specializing AI to solve Earth scientific problems are decomposed and analyzed. Eventually, it concludes with the grand challenges and reveals the opportunities to give some guidance and pre-warnings on allocating resources wisely to achieve the ambitious Earth AI goals in the future.}
}
@article{LELANDAIS2019100919,
title = {Applying model-driven engineering to high-performance computing: Experience report, lessons learned, and remaining challenges},
journal = {Journal of Computer Languages},
volume = {55},
pages = {100919},
year = {2019},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2019.100919},
url = {https://www.sciencedirect.com/science/article/pii/S2590118419300425},
author1 = {Benoît Lelandais and Marie-Pierre Oudot and Benoît Combemale},
keywords = {Model-Driven Engineering, Modeling language, Domain-Specific Language, Language workbench, High-performance computing},
abstract = {Thanks to the increasing power of supercomputers, CEA develops ever more complex numerical simulators in the field of High Performance Computing (HPC) to cover a wide range of physical phenomena. As a consequence, simulation codes tend to become unmanageable and difficult to maintain and adapt to new hardware architectures. In this paper, we report on our experience in the use of Model-Driven Engineering (MDE) and Domain-Specific Languages (DSLs) to face these challenges through two projects, namely Modane and NabLab. From this experience, we discuss the main lessons learned to be considered for conducting future projects in the field of HPC, and the remaining challenges that are worth being included in the road-map of the MDE community.}
}
@article{KHAN2023111682,
title = {Software architecture for quantum computing systems — A systematic review},
journal = {Journal of Systems and Software},
volume = {201},
pages = {111682},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111682},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000778},
author1 = {Arif Ali Khan and Aakash Ahmad and Muhammad Waseem and Peng Liang and Mahdi Fahmideh and Tommi Mikkonen and Pekka Abrahamsson},
keywords = {Quantum computing, Quantum software engineering, Quantum software architecture, Systematic literature review},
abstract = {Quantum computing systems rely on the principles of quantum mechanics to perform a multitude of computationally challenging tasks more efficiently than their classical counterparts. The architecture of software-intensive systems can empower architects who can leverage architecture-centric processes, practices, description languages to model, develop, and evolve quantum computing software (quantum software for short) at higher abstraction levels. We conducted a Systematic Literature Review (SLR) to investigate (i) architectural process, (ii) modelling notations, (iii) architecture design patterns, (iv) tool support, and (iv) challenging factors for quantum software architecture. Results of the SLR indicate that quantum software represents a new genre of software-intensive systems; however, existing processes and notations can be tailored to derive the architecting activities and develop modelling languages for quantum software. Quantum bits (Qubits) mapped to Quantum gates (Qugates) can be represented as architectural components and connectors that implement quantum software. Tool-chains can incorporate reusable knowledge and human roles (e.g., quantum domain engineers, quantum code developers) to automate and customise the architectural process. Results of this SLR can facilitate researchers and practitioners to develop new hypotheses to be tested, derive reference architectures, and leverage architecture-centric principles and practices to engineer emerging and next generations of quantum software.}
}
@article{WASEEM2020110798,
title = {A Systematic Mapping Study on Microservices Architecture in DevOps},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110798},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110798},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220302053},
author1 = {Muhammad Waseem and Peng Liang and Mojtaba Shahin},
keywords = {Microservices Architecture, DevOps, Systematic Mapping Study},
abstract = {Context:
Applying Microservices Architecture (MSA) in DevOps has received significant attention in recent years. However, there exists no comprehensive review of the state of research on this topic.
Objective:
This work aims to systematically identify, analyze, and classify the literature on MSA in DevOps.
Methods:
A Systematic Mapping Study (SMS) has been conducted on the literature published between January 2009 and July 2018.
Results:
Forty-seven studies were finally selected and the key results are: (1) Three themes on the research on MSA in DevOps are “microservices development and operations in DevOps”, “approaches and tool support for MSA based systems in DevOps”, and “MSA migration experiences in DevOps”. (2) 24 problems with their solutions regarding implementing MSA in DevOps are identified. (3) MSA is mainly described by using boxes and lines. (4) Most of the quality attributes are positively affected when employing MSA in DevOps. (5) 50 tools that support building MSA based systems in DevOps are collected. (6) The combination of MSA and DevOps has been applied in a wide range of application domains.
Conclusion:
The results and findings will benefit researchers and practitioners to conduct further research and bring more dedicated solutions for the issues of MSA in DevOps.}
}
@article{VANDINTER2023109099,
title = {Reference architecture for digital twin-based predictive maintenance systems},
journal = {Computers & Industrial Engineering},
volume = {177},
pages = {109099},
year = {2023},
issn = {0360-8352},
doi = {https://doi.org/10.1016/j.cie.2023.109099},
url = {https://www.sciencedirect.com/science/article/pii/S0360835223001237},
author1 = {Raymon {van Dinter} and Bedir Tekinerdogan and Cagatay Catal},
keywords = {Reference architecture, Design, System architecture, Framework, Digital twin, Predictive maintenance},
abstract = {Context
Digital Twin-based predictive maintenance systems are frequently integrated into complex systems. The success of the integration depends on the design of the system. A Reference Architecture can be used as a blueprint to design Application Architectures rapidly and consistently for various application domains, resulting in a reduced time-to-market.
Objective
The main objective of this study is to develop and evaluate a Reference Architecture designed using renowned software architecture methods.
Method
A domain analysis was performed to gather and synthesize the literature on Digital Twin-based predictive maintenance systems, which we used to model the key features. We applied UML diagrams to design the reference architecture based on the feature model. We evaluated the reference architecture using three case studies.
Results
We derived three views for Digital Twin-based predictive maintenance systems. For the user's view, we developed a context diagram. We developed a package diagram for the structural view, and we designed a layered view to show the system's decomposition in layers. We designed an Application Architecture for each case study based on the study's features using each Reference Architecture view. Additionally, we designed a deployment view to describe the hardware and software and its environment.
Conclusion
We demonstrated that the methods of creating a Reference Architecture could be used in the Digital Twin-based predictive maintenance domain and showed how an Application Architecture could be designed in this context.}
}
@article{TUMA2018275,
title = {Threat analysis of software systems: A systematic literature review},
journal = {Journal of Systems and Software},
volume = {144},
pages = {275-294},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.06.073},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301304},
author1 = {K. Tuma and G. Calikli and R. Scandariato},
keywords = {Threat analysis (modeling), Risk assessment, Security-by-design, Software systems, Systematic literature review (SLR)},
abstract = {Architectural threat analysis has become an important cornerstone for organizations concerned with developing secure software. Due to the large number of existing techniques it is becoming more challenging for practitioners to select an appropriate threat analysis technique. Therefore, we conducted a systematic literature review (SLR) of the existing techniques for threat analysis. In our study we compare 26 methodologies for what concerns their applicability, characteristics of the required input for analysis, characteristics of analysis procedure, characteristics of analysis outcomes and ease of adoption. We also provide insight into the obstacles for adopting the existing approaches and discuss the current state of their adoption in software engineering trends (e.g. Agile, DevOps, etc.). As a summary of our findings we have observed that: the analysis procedure is not precisely defined, there is a lack of quality assurance of analysis outcomes and tool support and validation are limited.}
}
@article{DOMASCHKA2015151,
title = {Beyond Mere Application Structure Thoughts on the Future of Cloud Orchestration Tools},
journal = {Procedia Computer Science},
volume = {68},
pages = {151-162},
year = {2015},
note = {1st International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2015.09.231},
url = {https://www.sciencedirect.com/science/article/pii/S1877050915030768},
author1 = {Jörg Domaschka and Frank Griesinger and Daniel Baur and Alessandro Rossini},
keywords = {cloud computing, life-cycle management, cross-cloud application},
abstract = {Managing cloud applications running on IaaS is complicated and error prone. This is why DevOps tools and application description languages have been emerging. While these tools and languages enable the user to define the application and communication structure based on application components, they lack the possibility to define sophisticated communication patterns including the wiring on instance level. This paper details these shortcomings and presents approaches to overcome them. In particular, they we propose (i) adding boundaries to wiring specifications and (ii) introducing a higher-level abstraction—called facet—on top of the application. The combination of both concepts allows specifying wiring on basis of logical units and their relations. Hence, the concepts overcome general wiring problems that currently exist in cloud orchestration tools. In addition to that, the introduction of facets improves the re-use of components across different applications.}
}
@article{BASSILIADES2017203,
title = {A semantic recommendation algorithm for the PaaSport platform-as-a-service marketplace},
journal = {Expert Systems with Applications},
volume = {67},
pages = {203-227},
year = {2017},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2016.09.032},
url = {https://www.sciencedirect.com/science/article/pii/S0957417416305164},
author1 = {Nick Bassiliades and Moisis Symeonidis and Georgios Meditskos and Efstratios Kontopoulos and Panagiotis Gouvas and Ioannis Vlahavas},
keywords = {Cloud computing, Platform-as-a-service, Semantic interoperability, Platform offering, Cloud application, Recommendation, Semantic matchmaking, Ranking},
abstract = {Platform as a service (PaaS) is one of the Cloud computing services that provide a computing platform in the Cloud, allowing customers to develop, run, and manage web applications without the complexity of building and maintaining the infrastructure. The primary disadvantage for an SME to enter the emerging PaaS market is the possibility of being locked into a certain platform, mostly provided by the market's giants. The PaaSport project focuses on facilitating SMEs to deploy business applications on the best-matching Cloud PaaS offering and to seamlessly migrate these applications on demand, via a thin, non-intrusive Cloud-broker, in the form of a Cloud PaaS Marketplace. PaaSport enables PaaS provider SMEs to roll out semantically interoperable PaaS offerings, by annotating them using a unified PaaS semantic model that has been defined as an OWL ontology. In this paper we focus on the recommendation algorithm that has been developed on top of the ontology, for providing the application developer with recommendations about the best-matching Cloud PaaS offering. The algorithm consists of: a) a matchmaking part, where the functional parameters of the application are taken into account to rule out inconsistent offerings, and b) a ranking part, where the non-functional parameters of the application are considered to score and rank offerings. Τhe algorithm is extensively evaluated showing linear scalability to the number of offerings and application requirements. Furthermore, it is extensible upon future semantic model extensions, because it is agnostic to domain specific concepts and parameters, using SPARQL template queries.}
}
@article{IHIRWE2024101254,
title = {CHESSIoT: A model-driven approach for engineering multi-layered IoT systems},
journal = {Journal of Computer Languages},
volume = {78},
pages = {101254},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101254},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000643},
author1 = {Felicien Ihirwe and Davide {Di Ruscio} and Simone Gianfranceschi and Alfonso Pierantonio},
keywords = {Model-driven engineering, System design, Safety analysis, Code generation, System deployment, Internet of Things},
abstract = {Context:
The current technology revolution, which places the highest value on people’s welfare, is frequently seen as being mainly supported by Internet of Things (IoT) technologies. IoT is regarded as a powerful multi-layered network of systems that integrates several heterogeneous, independently networked (sub-)systems working together to achieve a shared purpose.
Objective:
In this article, we present CHESSIoT, a model-driven engineering environment that integrates high-level visual design languages, software development, safety analysis, and deployment approaches for engineering multi-layered IoT systems. With CHESSIoT, users may conduct different engineering tasks on system and software models under development to enable earlier decision-making and take prospective measures, all supported by a unique environment.
Methodology:
This is achieved through multi-staged designs, most notably the physical, functional, and deployment architectures. The physical model specification is used to perform both qualitative and quantitative safety analysis by employing logical Fault-Trees models (FTs). The functional model specifies the system’s functional behavior and is later used to generate platform-specific code that can be deployed on low-level IoT device nodes. Additionally, the framework supports modeling the system’s deployment plan and run-time service provisioning, which would ultimately be transformed into deployment configuration artifacts ready for execution on remote servers.
Results:
To showcase the effectiveness of our proposed approach, as well as the capability of the supporting tool, a multi-layered Home Automation system (HAS) scenario has been developed covering all its design, development, analysis, and deployment aspects. Furthermore, we present the results from different evaluation mechanisms which include a comparative analysis and a qualitative assessment. The evaluation mechanisms target mainly completeness of CHESSIoT by addressing specific research questions.}
}
@article{KIRCHHOF2022111087,
title = {MontiThings: Model-Driven Development and Deployment of Reliable IoT Applications},
journal = {Journal of Systems and Software},
volume = {183},
pages = {111087},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111087},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001849},
author1 = {Jörg Christian Kirchhof and Bernhard Rumpe and David Schmalzing and Andreas Wortmann},
keywords = {Internet of Things, Model-driven engineering, Architecture modeling, Code generation, Deployment},
abstract = {Internet of Things (IoT) applications are exposed to harsh conditions due to factors such as device failure, network problems, or implausible sensor values. We investigate how the inherent encapsulation of component and connector (C&C) architectures can be used to develop and deploy reliable IoT applications. Existing C&C languages for the development of IoT applications mainly focus on the description of architectures and the distribution of components to IoT devices. Furthermore, related approaches often pollute the models with low-level implementation details, tying the models to a particular platform and making them harder to understand. In this paper, we introduce MontiThings, a C&C language offering automatic error handling capabilities and a clear separation between business logic and implementation details. The error-handling methods presented in this paper can make C&C-based IoT applications more reliable without cluttering the business logic with error-handling code that is time-consuming to develop and makes the models hard to understand, especially for non-experts.}
}
@article{LIMAJ2022101721,
title = {A taxonomy of scaling agility},
journal = {The Journal of Strategic Information Systems},
volume = {31},
number = {3},
pages = {101721},
year = {2022},
issn = {0963-8687},
doi = {https://doi.org/10.1016/j.jsis.2022.101721},
url = {https://www.sciencedirect.com/science/article/pii/S0963868722000178},
author1 = {Everist Limaj and Edward W.N. Bernroider},
keywords = {Scaling agility, Taxonomy development, Qualitative research},
abstract = {Driven by environmental uncertainty, many organizations today attempt to achieve agility at scale. However, given the variety and complexity of these efforts, there is currently a limited understanding in terms of their most relevant configurations, especially over the process of change. Based on an iterative taxonomy development approach grounded in design science, we present a taxonomy to structure, configure, and update scaling agility. The resulting multi-dimensional classification system offers an integrative view on elements only selectively considered in prior research. While offering a conceptual anchor for further research, we also provide specific configurations covering three different stages of scaling agility.}
}
@article{RAJAPAKSE2022106700,
title = {Challenges and solutions when adopting DevSecOps: A systematic review},
journal = {Information and Software Technology},
volume = {141},
pages = {106700},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2021.106700},
url = {https://www.sciencedirect.com/science/article/pii/S0950584921001543},
author1 = {Roshan N. Rajapakse and Mansooreh Zahedi and M. Ali Babar and Haifeng Shen},
keywords = {DevOps, Security, DevSecOps, Continuous Software Engineering, Systematic Literature Review},
abstract = {Context:
DevOps (Development and Operations) has become one of the fastest-growing software development paradigms in the industry. However, this trend has presented the challenge of ensuring secure software delivery while maintaining the agility of DevOps. The efforts to integrate security in DevOps have resulted in the DevSecOps paradigm, which is gaining significant interest from both industry and academia. However, the adoption of DevSecOps in practice is proving to be a challenge.
Objective:
This study aims to systemize the knowledge about the challenges faced by practitioners when adopting DevSecOps and the proposed solutions reported in the literature. We also aim to identify the areas that need further research in the future.
Method:
We conducted a Systematic Literature Review of 54 peer-reviewed studies. The thematic analysis method was applied to analyze the extracted data.
Results:
We identified 21 challenges related to adopting DevSecOps, 31 specific solutions, and the mapping between these findings. We also determined key gap areas in this domain by holistically evaluating the available solutions against the challenges. The results of the study were classified into four themes: People, Practices, Tools, and Infrastructure. Our findings demonstrate that tool-related challenges and solutions were the most frequently reported, driven by the need for automation in this paradigm. Shift-left security and continuous security assessment were two key practices recommended for DevSecOps. People-related factors were considered critical for successful DevSecOps adoption but less studied.
Conclusions:
We highlight the need for developer-centered application security testing tools that target the continuous practices in DevSecOps. More research is needed on how the traditionally manual security practices can be automated to suit rapid software deployment cycles. Finally, achieving a suitable balance between the speed of delivery and security is a significant issue practitioners face in the DevSecOps paradigm.}
}
@article{LEA2024281,
title = {Automatic Generation of a Microservice Data Structure from ReLEL},
journal = {Procedia Computer Science},
volume = {231},
pages = {281-286},
year = {2024},
note = {14th International Conference on Emerging Ubiquitous Systems and Pervasive Networks / 13th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (EUSPN/ICTH 2023)},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2023.12.204},
url = {https://www.sciencedirect.com/science/article/pii/S1877050923022160},
author1 = {Fanomezana Mihajasoa Léa and Rapatsalahy Miary Andrianjaka and Razafindrakoto Nicolas Raft and Costin Bădică},
keywords = {MSA, SOA, ReLEL, Praxeme, ATL, MDA},
abstract = {The popularity of microservice architecture abbreviated MSA has attracted researchers and practitioners in recent years. MSA aims to divide the complex monolithic system from the service to the database into several autonomous blocks called microservices. In this way, updates and deployment of each service can be managed independently. The dissociation of data storage for each microservice is one of the particularities of this architecture. However, the manual implementation of certain tasks in the design and development of the MSA is time-consuming particularly with regard to data management. The objective of this article is therefore to automatically generate the data structure of a microservice from ReLEL in order to systematically derive the MSA databases. Our approach is based on the Praxeme methodology. It is realized in two stages. First, the semantic aspect is derived. Then, we develop ATL derivation rules to design the data structure.}
}
@article{SCHINTKE202482,
title = {Validity constraints for data analysis workflows},
journal = {Future Generation Computer Systems},
volume = {157},
pages = {82-97},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.037},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001079},
author1 = {Florian Schintke and Khalid Belhajjame and Ninon {De Mecquenem} and David Frantz and Vanessa Emanuela Guarino and Marcus Hilbrich and Fabian Lehmann and Paolo Missier and Rebecca Sattler and Jan Arne Sparka and Daniel T. Speckhard and Hermann Stolte and Anh Duc Vu and Ulf Leser},
keywords = {Scientific workflow systems, Workflow specification languages, Validity constraints, Dependability, Integrity and conformance checking},
abstract = {Porting a scientific data analysis workflow (DAW) to a cluster infrastructure, a new software stack, or even only a new dataset with some notably different properties is often challenging. Despite the structured definition of the steps (tasks) and their interdependencies during a complex data analysis in the DAW specification, relevant assumptions may remain unspecified and implicit. Such hidden assumptions often lead to crashing tasks without a reasonable error message, poor performance in general, non-terminating executions, or silent wrong results of the DAW, to name only a few possible consequences. Searching for the causes of such errors and drawbacks in a distributed compute cluster managed by a complex infrastructure stack, where DAWs for large datasets typically are executed, can be tedious and time-consuming. We propose validity constraints (VCs) as a new concept for DAW languages to alleviate this situation. A VC is a constraint specifying logical conditions that must be fulfilled at certain times for DAW executions to be valid. When defined together with a DAW, VCs help to improve the portability, adaptability, and reusability of DAWs by making implicit assumptions explicit. Once specified, VCs can be controlled automatically by the DAW infrastructure, and violations can lead to meaningful error messages and graceful behavior (e.g., termination or invocation of repair mechanisms). We provide a broad list of possible VCs, classify them along multiple dimensions, and compare them to similar concepts one can find in related fields. We also provide a proof-of-concept implementation for the workflow system Nextflow.}
}
@incollection{DAVIS2017417,
title = {Smart Manufacturing},
editor = {Martin A. Abraham},
booktitle = {Encyclopedia of Sustainable Technologies},
publisher = {Elsevier},
address = {Oxford},
pages = {417-427},
year = {2017},
isbn = {978-0-12-804792-7},
doi = {https://doi.org/10.1016/B978-0-12-409548-9.10212-X},
url = {https://www.sciencedirect.com/science/article/pii/B978012409548910212X},
author1 = {Jim Davis},
keywords = {Advance manufacturing, Digital manufacturing, Industry 4.0, Intelligent manufacturing, Manufacturing 4.0, Smart manufacturing},
abstract = {Smart Manufacturing (SM) uses the integration of next generation Operations Technology (OT) and Information Technology (IT) to realize significant untapped market opportunities. Opportunities accrue from a broader, more customized product space and accelerated dynamic, and precision manufacturing, drawing upon the cumulative impacts of every step across entire enterprise value and supply chains. Significantly increased energy, material and workforce productivity and improved environment sustainability are pathways to opportunities as well as outcomes of enterprise optimization. This paper returns smart manufacturing (SM) as a term of “practice” to its roots, revisiting it as a forward looking term about next-generation operations technology (OT) and information technology (IT) integration based on the manufacturing objectives that originally defined it. Building meaning through a lens of practices that orchestrate capabilities to achieve business and operational objectives makes it possible to describe SM itself as set of practices that distinguish it from today’s 40-year history of applying IT to manufacturing. Key practices at SM’s core are centered on: business and technology; seams and an OT/IT journey; OT/IT enterprise workflow modeling; cyberinfrastructure and reference architecture; and workforce, innovation, and marketplace.}
}
@article{WETTINGER2016317,
title = {Streamlining DevOps automation for Cloud applications using TOSCA as standardized metamodel},
journal = {Future Generation Computer Systems},
volume = {56},
pages = {317-332},
year = {2016},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2015.07.017},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X15002496},
author1 = {Johannes Wettinger and Uwe Breitenbücher and Oliver Kopp and Frank Leymann},
keywords = {DevOps, Deployment automation, Transformation, TOSCA, Cloud standards, Cloud computing},
abstract = {DevOps as an emerging paradigm aims to tightly integrate developers with operations personnel. This enables fast and frequent releases in the sense of continuously delivering new iterations of a particular application. Users and customers of today’s Web applications and mobile apps running in the Cloud expect fast feedback to problems and feature requests. Thus, it is a critical competitive advantage to be able to respond quickly. Besides cultural and organizational changes that are necessary to apply DevOps in practice, tooling is required to implement end-to-end automation of deployment processes. Automation is the key to efficient collaboration and tight integration between development and operations. The DevOps community is constantly pushing new approaches, tools, and open-source artifacts to implement such automated processes. However, as all these proprietary and heterogeneous DevOps automation approaches differ from each other, it is hard to integrate and combine them to deploy applications in the Cloud using an automated deployment process. In this paper we present a systematic classification of DevOps artifacts and show how different kinds of artifacts can be discovered and transformed toward TOSCA, which is an emerging standard. We present an integrated modeling and runtime framework to enable the seamless and interoperable integration of different approaches to model and deploy application topologies. The framework is implemented by an open-source, end-to-end toolchain. Moreover, we validate and evaluate the presented approach to show its practical feasibility based on a detailed case study, in particular considering the performance of the transformation toward TOSCA.}
}
@article{PEREZSANCHEZ2025103907,
title = {A theory on human factors in DevOps adoption},
journal = {Computer Standards & Interfaces},
volume = {92},
pages = {103907},
year = {2025},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2024.103907},
url = {https://www.sciencedirect.com/science/article/pii/S092054892400076X},
author1 = {Juanjo Pérez-Sánchez and Saima Rafi and Juan Manuel {Carrillo de Gea} and Joaquín {Nicolás Ros} and José Luis {Fernández Alemán}},
keywords = {DevOps, DevOps adoption, Human factor, Literature review, Clustering, Theory proposal},
abstract = {Context:
DevOps is a software engineering paradigm that enables faster deliveries and higher quality products. However, DevOps adoption is a complex process that is still insufficiently supported by research. In addition, human factors are the main difficulty for a successful DevOps adoption, although very few studies address this topic.
Objective:
This paper addresses two research gaps identified in literature, namely: (1) the characterization of DevOps from the perspective of human factors, i.e. the description of DevOps’ human characteristics to better define it, and (2) the identification and analysis of human factors’ effect in the adoption of DevOps.
Method:
We employed a hybrid methodology that included a Systematic Mapping Study followed by the application of a clustering technique. A questionnaire for DevOps practitioners (n=15) was employed as an evaluation method.
Results:
A total of 59 human factors related to DevOps were identified, described, and synthesized. The results were used to build a theory on DevOps human factors.
Conclusion:
The main contribution of this paper is a theory proposal regarding human factors in DevOps adoption. The evaluation results show that almost every human factor identified in the mapping study was found relevant in DevOps adoption. The results of the study represent an extension of DevOps characterization and a first approximation to human factors in DevOps adoption.}
}
@article{TURIN2023111750,
title = {Predicting resource consumption of Kubernetes container systems using resource models},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111750},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111750},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001450},
author1 = {Gianluca Turin and Andrea Borgarelli and Simone Donetti and Ferruccio Damiani and Einar Broch Johnsen and S. Lizeth {Tapia Tarifa}},
keywords = {Kubernetes, Microservices, Cloud computing, Resource models, Resource prediction},
abstract = {Cloud computing has radically changed the way organizations operate their Software by allowing them to achieve high availability of services at affordable cost. Containerized microservices is an enabling technology for this change, and advanced container orchestration platforms such as Kubernetes are used for service management. Despite the flourishing ecosystem of monitoring tools for such orchestration platforms, service management is still mainly a manual effort. The modeling of cloud computing systems is an essential step towards automatic management, but the modeling of cloud systems of such complexity remains challenging and, as yet, unaddressed. In fact modeling resource consumption will be a key to comparing the outcome of possible deployment scenarios. This paper considers how to derive resource models for cloud systems empirically. We do so based on models of deployed services in a formal modeling language with explicit CPU and memory resources; once the adherence to the real system is good enough, formal properties can be verified in the model. Targeting a likely microservices application, we present a model of Kubernetes developed in Real-Time ABS. We report on leveraging data collected empirically from small deployments to simulate the execution of higher intensity scenarios on larger deployments. We discuss the challenges and limitations that arise from this approach, and identify constraints under which we obtain satisfactory accuracy.}
}
@article{BARTUSEVICS201781,
title = {Automation of Continuous Services: What Companies of Latvia Says about It?},
journal = {Procedia Computer Science},
volume = {104},
pages = {81-88},
year = {2017},
note = {ICTE 2016, Riga Technical University, Latvia},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.01.075},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917300765},
author1 = {Arturs Bartusevics},
keywords = {Continuous services, Automation, DevOps, IT operations, Automation scripts},
abstract = {Automation is a one of mandatory success factor of continuous services in the software development area. There are many challenges related to automation of different IT operations to support continuous services up time. The story of current paper has been started a couple years ago, when a novel EAF approach has been designed. The main scope of this approach is decreasing of implementation time of automation of IT operation using model-driven paradigm and reusable functions. Based on first results of approbation of EAF, a survey has been developed to get opinions about challenges of automation and possible EAF improvement topics. Current paper provides results of mentioned survey where more than 40 IT companies provide the opinion about challenges in continuous services automation. Based on these results, EAF improvement topics are defined. All these topics are devoted to make EAF approach more useful and friendly for IT companies of Latvia, taking in account needs and challenges of mentioned companies.}
}
@article{PULPARAMBIL2021106487,
title = {A methodical framework for service oriented architecture adoption: Guidelines, building blocks, and method fragments},
journal = {Information and Software Technology},
volume = {132},
pages = {106487},
year = {2021},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2020.106487},
url = {https://www.sciencedirect.com/science/article/pii/S0950584920302299},
author1 = {Supriya Pulparambil and Youcef Baghdadi and Camille Salinesi},
keywords = {Method engineering, SOA maturity model, Methodical framework, SOA method, Method fragment, Methodical building block},
abstract = {Context
Rapidly-changing business requirements expect high business process flexibility that can be achieved using service oriented architecture (SOA). This requires enterprises to adopt SOA and assess their SOA adoption maturity to achieve continuous improvement. SOA realization demands service development with varying levels of granularity.
Objectives
The research aims to develop a methodical framework for SOA realization based on Welke's SOA maturity model, a model that assumes a methodology dimension. The framework is concerned with formalizing knowledge on how to identify and shape the main building blocks of a method at each maturity level.
Methods
The research applies the principles of design science research and method engineering to develop a methodical framework for SOA realization.
Results
The research identifies the gaps in SOA realization methods and illustrates how a methodical framework based on a maturity model facilitates the SOA adoption process. The evaluation results revealed that the framework would help enterprises to select method fragments required at each maturity level to accomplish business excellence.
Conclusion
The implications of this research are twofold: from a theoretical perspective, the researchers or practitioners can use the results for further study. From a practical standpoint, enterprises can use the methodical guidelines to assess their current maturity level and select and implement the required method fragments from the method base provided in the proposed framework.}
}
@article{ERAMO2024112180,
title = {An architecture for model-based and intelligent automation in DevOps},
journal = {Journal of Systems and Software},
volume = {217},
pages = {112180},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112180},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002255},
author1 = {Romina Eramo and Bilal Said and Marc Oriol and Hugo Bruneliere and Sergio Morales},
keywords = {Software architecture, DevOps, Continuous software engineering, Artificial Intelligence, Mode-driven engineering},
abstract = {The increasing complexity of modern systems poses numerous challenges at all stages of system development and operation. Continuous software and system engineering processes, e.g., DevOps, are increasingly adopted and spread across organizations. In parallel, many leading companies have begun to apply artificial intelligence (AI) principles and techniques, including Machine Learning (ML), to improve their products. However, there is no holistic approach that can support and enhance the growing challenges of DevOps. In this paper, we propose a software architecture that provides the foundations of a model-based framework for the development of AI-augmented solutions incorporating methods and tools for continuous software and system engineering and validation. The key characteristic of the proposed architecture is that it allows leveraging the advantages of both AI/ML and Model Driven Engineering (MDE) approaches and techniques in a DevOps context. This architecture has been designed, developed and applied in the context of the European large collaborative project named AIDOaRt. In this paper, we also report on the practical evaluation of this architecture. This evaluation is based on a significant set of technical solutions implemented and applied in the context of different real industrial case studies coming from the AIDOaRt project. Moreover, we analyze the collected results and discuss them according to both architectural and technical challenges we intend to tackle with the proposed architecture.}
}
@article{MARTINEZSAUCEDO2025107590,
title = {Migration of monolithic systems to microservices: A systematic mapping study},
journal = {Information and Software Technology},
volume = {177},
pages = {107590},
year = {2025},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2024.107590},
url = {https://www.sciencedirect.com/science/article/pii/S0950584924001952},
author1 = {Ana {Martínez Saucedo} and Guillermo Rodríguez and Fabio {Gomes Rocha} and Rodrigo Pereira dos Santos},
keywords = {Microservices, Monolith, Migration, Architecture, Systematic Mapping Study},
abstract = {Context:
The popularity of microservices architecture has grown due to its ability to address monolithic architecture issues, such as limited scalability, hard maintenance, and technological dependence. Nonetheless, the migration of monolith systems to microservices is complex. Therefore, methodologies and techniques are needed to facilitate migration and support practitioners and software architects.
Objective:
The objective of this study is to investigate cases of application migration, microservices identification techniques, tools used during migration, factors that promote migration, as well as issues and benefits of the migration.
Method:
We have conducted this SMS following the guidelines established by Kitchenham and Petersen. The research objective was defined using part of the Goal-Question-Metric model and the Population, Intervention, and Outcome criteria. From 1546 studies that were retrieved from the search execution, 114 were selected and analyzed to answer the research questions.
Results:
This SMS contributes with (i) a migration process proposal based on migration cases, (ii) a characterization of migration techniques based on different criteria, (iii) an analysis of tools to support migration, (iv) the identification of migration drivers, and (v) an exploration of migration issues as well as benefits.
Conclusion:
This SMS sheds light on the complexity and variability of migrating monolithic systems to microservices, as well as the limited number of migration tools. While scalability and maintenance drive migration, few studies assess them. Key challenges include microservices communication and database migration, with most research focusing primarily on monolith decomposition. Despite these difficulties, migration offers benefits, particularly in scalability and maintainability.}
}
@incollection{HEINRICH201769,
title = {Chapter 5 - An Architectural Model-Based Approach to Quality-Aware DevOps in Cloud Applicationsc**This work was partially supported by the DFG (German Research Foundation) under the Priority Programme SPP1593: Design For Future – Managed Software Evolution and the MWK (Ministry of Science, Research and the Arts Baden-Württemberg) in the funding line Research Seed Capital (RiSC).},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {69-89},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00005-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000053},
author1 = {Robert Heinrich and Reiner Jung and Christian Zirkelbach and Wilhelm Hasselbring and Ralf Reussner},
keywords = {Software architecture, Cloud application, Runtime model, Adaptation, Evolution, DevOps},
abstract = {Cloud-based software applications are designed to change often and rapidly during operations to provide constant quality of service. As a result the boundary between development and operations is becoming increasingly blurred. DevOps provides a set of practices for the integrated consideration of developing and operating software. Software architecture is a central artifact in DevOps practices. Existing architectural models used in the development phase differ from those used in the operation phase in terms of purpose, abstraction, and content. In this chapter, we present the iObserve approach to address these differences and allow for phase-spanning usage of architectural models.}
}
@article{MAYRDORN2024112064,
title = {Actionable light-weight process guidance},
journal = {Journal of Systems and Software},
volume = {214},
pages = {112064},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112064},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001092},
author1 = {Christoph Mayr-Dorn and Cosmina-Cristina Ratiu and Luciano {Marchezan de Paula} and Felix Keplinger and Alexander Egyed and Gala Walden},
keywords = {Process, Constraints, Quality assurance, Traceability, Engineering guidance, Repairs},
abstract = {Software engineering organizations in safety-critical domains require rigorous processes that include explicit software quality assurance measures (QA) to achieve high-quality and safe engineering artifacts. One major challenge for engineers is adhering to the correct process that is applicable in their specific working context, to understand which steps are ready to start, what actions are missing to complete their step, and when rework has happened. In this paper, we propose and evaluate ProGuide, a framework that provides actionable, light-weight process guidance by continuously assessing pre-conditions, post-conditions, and QA constraints. In case of a violation, it provides concrete repair actions. Evaluation on a safety-critical open source system and engineers from our industry partner Bosch showed that repairs are complete and small in number, and resulted in less frustration and fewer mistakes compared to being provided with no process guidance. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{TEKINERDOGAN2019531,
title = {Special issue on architecting for hyper connectivity and hyper virtualization},
journal = {Journal of Systems and Software},
volume = {149},
pages = {531-532},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302802},
author1 = {Bedir Tekinerdogan and Uwe Zdun and M. Ali Babar}
}
@article{MUNOZ2024103072,
title = {Preface to the special issue on success stories in model driven engineering},
journal = {Science of Computer Programming},
volume = {233},
pages = {103072},
year = {2024},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2023.103072},
url = {https://www.sciencedirect.com/science/article/pii/S0167642323001545},
author1 = {Paula Muñoz and Steffen Zschaler and Richard F. Paige}
}
@article{CUADRA2024360,
title = {Enabling DevOps for Fog Applications in the Smart Manufacturing domain: A Model-Driven based Platform Engineering approach},
journal = {Future Generation Computer Systems},
volume = {157},
pages = {360-375},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.03.053},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24001286},
author1 = {Julen Cuadra and Ekaitz Hurtado and Isabel Sarachaga and Elisabet Estévez and Oskar Casquero and Aintzane Armentia},
keywords = {Fog Computing, Model Driven Engineering, Node-RED, Smart Manufacturing, DevOps, Platform Engineering},
abstract = {Cloud Computing is revolutionizing smart manufacturing by offering on-demand and scalable computer systems that facilitate plant data analysis and operational efficiency optimization. DevOps is a methodology, widely used for developing Cloud Computing systems, that streamlines software development by improving its integration, delivery, and deployment. Although cloud application designers within a DevOps team are assumed to have development and operational knowledge, this does not fall within the skills of experts that design analytics applications of plant data. The deployment environment is also relevant since, as such applications are often hosted in the Fog, the proliferation of application components may hinder their composition and validation. This work is aimed at embracing the Platform Engineering approach to provide a tailored toolkit that guides the design and development of OpenFog compliant applications for the experts in the Smart Manufacturing domain. The platform uses Model Driven Engineering techniques and a flow-based visual editor to allow application designers to graphically compose applications from components previously delivered by component developers, abstracting them from the underlying technologies. As a result, containerized applications, ready to be deployed and run by a container orchestrator, are obtained. The feasibility of the proposal is proved through an industrial case study.}
}
@article{GIRAY2021111031,
title = {A software engineering perspective on engineering machine learning systems: State of the art and challenges},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111031},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111031},
url = {https://www.sciencedirect.com/science/article/pii/S016412122100128X},
author1 = {Görkem Giray},
keywords = {Software engineering, Software development, Software process, Machine learning, Deep learning, Systematic literature review},
abstract = {Context:
Advancements in machine learning (ML) lead to a shift from the traditional view of software development, where algorithms are hard-coded by humans, to ML systems materialized through learning from data. Therefore, we need to revisit our ways of developing software systems and consider the particularities required by these new types of systems.
Objective:
The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of software engineering (SE) research for engineering ML systems.
Method:
I performed a systematic literature review (SLR). I systematically selected a pool of 141 studies from SE venues and then conducted a quantitative and qualitative analysis using the data extracted from these studies.
Results:
The non-deterministic nature of ML systems complicates all SE aspects of engineering ML systems. Despite increasing interest from 2018 onwards, the results reveal that none of the SE aspects have a mature set of tools and techniques. Testing is by far the most popular area among researchers. Even for testing ML systems, engineers have only some tool prototypes and solution proposals with weak experimental proof. Many of the challenges of ML systems engineering were identified through surveys and interviews. Researchers should conduct experiments and case studies, ideally in industrial environments, to further understand these challenges and propose solutions.
Conclusion:
The results may benefit (1) practitioners in foreseeing the challenges of ML systems engineering; (2) researchers and academicians in identifying potential research questions; and (3) educators in designing or updating SE courses to cover ML systems engineering.}
}
@article{PETROVIC2020102033,
title = {SMADA-Fog: Semantic model driven approach to deployment and adaptivity in fog computing},
journal = {Simulation Modelling Practice and Theory},
volume = {101},
pages = {102033},
year = {2020},
note = {Modeling and Simulation of Fog Computing},
issn = {1569-190X},
doi = {https://doi.org/10.1016/j.simpat.2019.102033},
url = {https://www.sciencedirect.com/science/article/pii/S1569190X19301649},
author1 = {Nenad Petrovic and Milorad Tosic},
keywords = {DevOps, Fog Computing, Infrastructure as code, Linear optimization, Model-driven engineering, Semantic technology},
abstract = {The deployment, monitoring and configuration of applications in Fog Computing are becoming quite challenging, due to heterogeneity of mobile and IoT devices involved, data movement constraints imposed by legal regulations as well as frequent changes in the execution environment that may affect quality of service. As a consequence, the system administration procedures are becoming more complex and time-consuming, especially if done manually. In this paper, a Semantic Model driven Approach to Deployment and Adaptivity of container-based applications in Fog Computing (SMADA-Fog) is proposed. Modeling tools, semantic framework, linear optimization model, simulation environment and infrastructure management code generator leveraging the semantic annotations are implemented and presented. According to results of the two experimentally tested scenarios, the proposed approach improves the application performance, while the time required for deployment as well as service adaptation is reduced for at least an order of magnitude.}
}
@article{ROMERO2022100409,
title = {A hybrid deep learning and ontology-driven approach to perform business process capability assessment},
journal = {Journal of Industrial Information Integration},
volume = {30},
pages = {100409},
year = {2022},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2022.100409},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X22000760},
author1 = {Marcelo Romero and Wided Guédria and Hervé Panetto and Béatrix Barafort},
keywords = {Deep learning, Long Short-Term Memory Network, Ontology, Process capability assessment},
abstract = {Enterprises are constantly transforming to adapt to an ever-changing and competitive environment. In this context, assessments allow to understand the state of different organisational aspects before performing transformation activities. One of these aspects is the capability of business processes. Evaluating the quality of business processes is relevant to guide improvement initiatives, considering that the way that processes are designed and executed in organisations has direct impact on the quality of products and services. However, assessments are expensive in terms of resources if they are performed by humans. In this sense, recent trends in Artificial Intelligence provide means to improve process capability assessment through the automation of some of its tasks. Following this line, this work presents a method to perform process capability assessment using raw text as input data with the aid of a smart system, able to reduce the need of human intervention to provide reliable assessment results. For this purpose, we introduce a hybrid approach to perform assessments in enterprises using text data as assessment evidence. The method combines the Long Short-Term Memory Network (LSTM) approach and the use of an Ontology named Process Capability Assessment Ontology (PCAO), which also contains a set of rules to calculate process attribute ratings, capability levels, among other aspects. The approach is grounded on the Smart Assessment Framework, a conceptual model devised to guide the development of intelligent assessments in enterprises. We introduce a demonstration of the assessment of a process based on the management of chemical samples from a research institute.}
}
@article{ALIDRA2023104,
title = {A feature-based survey of Fog modeling languages},
journal = {Future Generation Computer Systems},
volume = {138},
pages = {104-119},
year = {2023},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2022.08.010},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X22002710},
author1 = {Abdelghani Alidra and Hugo Bruneliere and Thomas Ledoux},
keywords = {Fog Computing, Cloud Computing, Internet of Things, Modeling language, Survey},
abstract = {Fog Computing is a new paradigm aiming at decentralizing the Cloud by geographically distributing away computation, storage and network resources as well as related services. In order to design, develop, deploy, maintain and evolve Fog systems, languages are required for properly modeling both their entities (e.g., infrastructures, topologies, resources configurations) and their specific features such as the locality concept, QoS constraints applied on resources (e.g., energy, data privacy, latency) and their dependencies, the dynamicity of considered workloads, the heterogeneity of both applications and devices, etc. This paper provides a detailed overview of the current state-of-the-art in terms of Fog modeling languages. We relied on our long-term experience in Cloud Computing and Cloud Modeling to contribute a feature model describing what we believe to be the most important characteristics of Fog modeling languages. We also performed a systematic scientific literature search and selection process to obtain a list of already existing Fog modeling languages. Then, we evaluated and compared these Fog modeling languages according to the characteristics expressed in our feature model. As a result, we discuss in this paper the main capabilities of these Fog modeling languages and propose a corresponding set of open research challenges in this area. We expect the presented work to be helpful to both current and future researchers or engineers working on/with Fog systems, as well as to anybody genuinely interested in Fog Computing or more generally in distributed systems.}
}
@article{BINAMUNGU2023111749,
title = {Behaviour driven development: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {203},
pages = {111749},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111749},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223001449},
author1 = {Leonard Peter Binamungu and Salome Maro},
keywords = {Behaviour Driven Development, Systematic mapping study, Systematic mapping studies in software engineering},
abstract = {Context:
Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps.
Objective:
To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021.
Method:
By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research.
Results:
The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects.
Conclusion:
The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research.}
}
@article{CHONDAMRONGKUL2023102978,
title = {Software evolutionary architecture: Automated planning for functional changes},
journal = {Science of Computer Programming},
volume = {230},
pages = {102978},
year = {2023},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2023.102978},
url = {https://www.sciencedirect.com/science/article/pii/S0167642323000606},
author1 = {Nacha Chondamrongkul and Jing Sun},
keywords = {Evolution planning, Formal method, Model checking, Functional property, Evolutionary architecture},
abstract = {Software systems often evolve over time due to frequent changes in user requirements. The refactoring of the architectural design due to numerous functional changes significantly impacts the software system. Evolutionary architecture is a design principle that supports the implementation of frequent changes. One key aspect of an evolutionary architecture is the definition of fitness functions to ensure that the changes align with the intended goals. However, planning the incremental evolution of the architectural design remains a challenge. This paper presents an approach to automatically generate evolution plans for refactoring the architectural design in support of new functionalities. Formal modelling has been applied to allow for the verification of functional properties against the design. By utilising the generated evolution plan, we can determine a safe path for evolving the software system with minimal risk of failure. We evaluated the rigour and effectiveness of the evolution plan generated by our approach for six software systems. Our experimental results demonstrate the effectiveness of the proposed approach in generating evolution plans. Additionally, we were able to identify the most suitable planning strategy that minimises system interruptions in the generated evolution plan.}
}
@article{CASOLA2024103639,
title = {Secure software development and testing: A model-based methodology},
journal = {Computers & Security},
volume = {137},
pages = {103639},
year = {2024},
issn = {0167-4048},
doi = {https://doi.org/10.1016/j.cose.2023.103639},
url = {https://www.sciencedirect.com/science/article/pii/S0167404823005497},
author1 = {Valentina Casola and Alessandra {De Benedictis} and Carlo Mazzocca and Vittorio Orbinato},
keywords = {Secure software development, SecDevOps methodology, DevSecOps methodology, Model-based testing, Automated security testing plan generation, Secure containerized microservice applications},
abstract = {Modern industries widely rely upon software and IT services, in a context where cybercrime is rapidly spreading in more and more sectors. Unfortunately, despite greater general awareness of security risks and the availability of security tools that can help to cope with those risks, many organizations (especially medium/small-size ones) still lag when it comes to building security into their services. This is mainly due to the limited security skills of common developers/IT project managers and to the typically high costs of security procedures. In fact, while automated tools exist to perform code analysis, vulnerability scanning, or security testing, the manual intervention of security experts is still required not only for security analysis and design, but also to configure and elaborate the output of the security testing tools. In this paper, we propose a novel secure software development methodology aimed at supporting developers from security design to security testing, suitable for integration within modern DevOps pipelines according to a DevSecOps (or SecDevOps) approach. The proposed methodology leverages a model-based process that enables identifying existing threats, selecting appropriate countermeasures to enforce, and verify their mitigation effectiveness through both static assessment procedures and targeted security tests. To demonstrate our approach's feasibility and concretely illustrate the devised activities, we provide a step-by-step description of the whole process concerning a containerized microservice-based application case study. In addition, we discuss the application of the proposed methodology, in its threat modeling and security testing phases, to a well-known vulnerable web application widely used for security training purposes, to illustrate that we can identify most of the existing vulnerabilities and determine appropriate test plans to assess and mitigate such vulnerabilities.}
}
@incollection{MISTRIK2017xxxi,
title = {Preface},
editor = {Ivan Mistrik and Rami Bahsoon and Nour Ali and Maritta Heisel and Bruce Maxim},
booktitle = {Software Architecture for Big Data and the Cloud},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {xxxi-xxxviii},
year = {2017},
isbn = {978-0-12-805467-3},
doi = {https://doi.org/10.1016/B978-0-12-805467-3.00028-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128054673000284},
author1 = {Ivan Mistrik and Nour Ali and Rami Bahsoon and Maritta Heisel and Bruce R. Maxim}
}
@article{CHIARI2024102422,
title = {DOML: A new modeling approach to Infrastructure-as-Code},
journal = {Information Systems},
volume = {125},
pages = {102422},
year = {2024},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2024.102422},
url = {https://www.sciencedirect.com/science/article/pii/S0306437924000802},
author1 = {Michele Chiari and Bin Xiang and Sergio Canzoneri and Galia Novakova Nedeltcheva and Elisabetta {Di Nitto} and Lorenzo Blasi and Debora Benedetto and Laurentiu Niculut and Igor Škof},
keywords = {DOML, Infrastructure-as-Code, DevOps, IaC Modeling languages, Multi-layer modeling approach, Evaluation},
abstract = {One of the main DevOps practices is the automation of resource provisioning and deployment of complex software. This automation is enabled by the explicit definition of Infrastructure-as-Code (IaC), i.e., a set of scripts, often written in different modeling languages, which defines the infrastructure to be provisioned and applications to be deployed. We introduce the DevOps Modeling Language (DOML), a new Cloud modeling language for infrastructure deployments. DOML is a modeling approach that can be mapped into multiple IaC languages, addressing infrastructure provisioning, application deployment and configuration. The idea behind DOML is to use a single modeling paradigm which can help to reduce the need of deep technical expertise in using different specialized IaC languages. We present the DOML’s principles and discuss the related work on IaC languages. Furthermore, the advantages of the DOML for the end-user are demonstrated in comparison with some state-of-the-art IaC languages such as Ansible, Terraform, and Cloudify, and an evaluation of its effectiveness through several examples and a case study is provided.}
}
@article{WASEEM2021111061,
title = {Design, monitoring, and testing of microservices systems: The practitioners’ perspective},
journal = {Journal of Systems and Software},
volume = {182},
pages = {111061},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111061},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001588},
author1 = {Muhammad Waseem and Peng Liang and Mojtaba Shahin and Amleto {Di Salle} and Gastón Márquez},
keywords = {Microservices architecture, Design, Monitoring, Testing, Industrial survey},
abstract = {Context:
Microservices Architecture (MSA) has received significant attention in the software industry. However, little empirical evidence exists on design, monitoring, and testing of microservices systems.
Objective:
This research aims to gain a deep understanding of how microservices systems are designed, monitored, and tested in the industry.
Methods:
A mixed-methods study was conducted with 106 survey responses and 6 interviews from microservices practitioners.
Results:
The main findings are: (1) a combination of domain-driven design and business capability is the most used strategy to decompose an application into microservices, (2) over half of the participants used architecture evaluation and architecture implementation when designing microservices systems, (3) API gateway and Backend for frontend patterns are the most used MSA patterns, (4) resource usage and load balancing as monitoring metrics, log management and exception tracking as monitoring practices are widely used, (5) unit and end-to-end testing are the most used testing strategies, and (6) the complexity of microservices systems poses challenges for their design, monitoring, and testing, for which there are no dedicated solutions.
Conclusions:
Our findings reveal that more research is needed to (1) deal with microservices complexity at the design level, (2) handle security in microservices systems, and (3) address the monitoring and testing challenges through dedicated solutions.}
}
@article{GONZALEZMOYANO2022107028,
title = {Uses of business process modeling in agile software development projects},
journal = {Information and Software Technology},
volume = {152},
pages = {107028},
year = {2022},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107028},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922001483},
author1 = {Cielo {González Moyano} and Luise Pufahl and Ingo Weber and Jan Mendling},
keywords = {Process models, Agile methodologies, Multi-method, Literature review, Thematic synthesis, Focus group},
abstract = {Context:
Agile methodologies and frameworks are widely used in software development projects because of their support for continuous change and delivery. Agile software development advocates de-prioritizing aspects such as processes and documentation. In traditional software engineering methodologies, however, business process models have been extensively used to support these aspects. Up until now, it is unclear to what extent recommendations to focus on code imply that conceptual modeling should be discontinued.
Objective:
The objective of this study is to investigate this hypothesis. More specifically, we develop a theoretical argument of how business process models are and can be used to support agile software development projects.
Method:
To this end, we use a multi-method study design. First, we conduct a systematic literature review, in which we identify studies on the usage of business process models in agile software development. Second, we apply procedures from thematic synthesis to analyze the connection between these uses and the phases of the development cycle. Third, we use a focus group design with practitioners to systematically reflect upon how these uses can help regarding four categories of challenges in agile software development: management, team, technology, and process.
Results:
From 37 relevant studies, we distill 15 different uses. The results highlight the benefits of process modeling as an instrument to support agile software development projects from different angles and in all project phases. Process modeling appears to be particularly relevant for the first phases of the development cycle, and for management and process issues in agile projects.
Conclusion:
We conclude that business process models indeed provide benefits for agile software development projects. Our findings have practical implications and emphasize the need for future research on modeling and agile development.}
}
@article{PINHO2023101185,
title = {What about the usability in low-code platforms? A systematic literature review},
journal = {Journal of Computer Languages},
volume = {74},
pages = {101185},
year = {2023},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2022.101185},
url = {https://www.sciencedirect.com/science/article/pii/S259011842200082X},
author1 = {Daniel Pinho and Ademar Aguiar and Vasco Amaral},
keywords = {Low-code development, Usability, Model-driven engineering, User experience},
abstract = {Context:
Low-code development is a concept whose presence has grown both in academia and the software industry and is discussed alongside others, such as model-driven engineering and domain-specific languages. Usability is an important concept in low-code contexts since users of these tools often lack a background in programming. Grey literature articles have also stated that low-code tools have high usability.
Objective:
This paper examines the current literature about low-code and no-code to discover more about them and their relationship with usability, particularly its quality, which factors are the most relevant, and how users view these tools. This focus on usability aims to provide a different point of view from other works on low-code.
Method:
We performed a systematic literature review based on a formal protocol for this study. The search protocol returned a total of 207 peer-review articles across five databases, which was supplemented with a snowballing process. These were filtered using inclusion and exclusion criteria, resulting in 38 relevant articles that were analysed, synthesised and reported.
Conclusion:
Despite growing interest and a strong enterprise presence in academia, we did not find a formal definition of low-code, although common characteristics have been specified. We found that users have a heightened awareness of usability regarding low-code tools, with some author1s performing feasibility studies on their implementations or listing factors that influence the user experience in a given tool. Researchers are considering usability factors unconsciously, and the low-code field would grow if research on usability increased. This paper also suggests a definition for low-code development.}
}
@article{OTTA2022112,
title = {Towards a health software supporting platform for wearable devices},
journal = {Procedia Computer Science},
volume = {210},
pages = {112-115},
year = {2022},
note = {The 13th International Conference on Emerging Ubiquitous Systems and Pervasive Networks (EUSPN) / The 12th International Conference on Current and Future Trends of Information and Communication Technologies in Healthcare (ICTH-2022) / Affiliated Workshops},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.10.126},
url = {https://www.sciencedirect.com/science/article/pii/S1877050922015836},
author1 = {Maxmilian Otta},
keywords = {healthcare IoT, wearable devices, SmartCGMS, FreeRTOS},
abstract = {The number of broadly available wearable devices like smart watches or fitness bands keeps growing, as well as their performance and number of provided features related to user's health. This was the reason for our decision to bring the SmartCGMS (Smart Continuous Glucose Monitoring and Controlling System) to a wearable device, in order to foster its way to practical deployment in healthcare. Currently, the SmartCGMS system is able to run on Windows, macOS, Linux, RaspberryPi or Android phones and tablets. What is currently hindering us to run SmartCGMS on a wearable device is the heterogeneity of devices and primarily lack of real-time OS features available to developers. This is natural, because device vendors aim for best user experience and so foreground tasks get the highest priority in order to maximize device responsiveness and suppress background tasks for minimum CPU load and battery drain. But running medical software on a wearable device requires almost the opposite – tasks reading sensor data and especially tasks controlling drug dosage, that are running on the wearable, require high priority and minimum interference with other running tasks. In this article, we present an initial design of a software framework in development, that will provide us the features we are currently missing in available wearable devices operating systems: a common application image that is able to run on various wearable devices and support for high priority tasks.}
}
@article{LY2025112243,
title = {The Power of Words in Agile vs. Waterfall Development: Written Communication in Hybrid Software Teams},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112243},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112243},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002875},
author1 = {Delina Ly and Michiel Overeem and Sjaak Brinkkemper and Fabiano Dalpiaz},
keywords = {Collaborative software development, Development paradigms, Waterfall development, Agile development, Communication channel formality, Hybrid software teams},
abstract = {Software development is constantly evolving, adapting to emerging technologies and development paradigms while leveraging advancements in communication technologies and work modes. We conduct an exploratory case study in a large software organization to investigate how the development paradigm and the formality of communication channels affect written communication within hybrid teams. We perform statistical and content analysis of written conversations from 20 projects involving two software products that use industry adaptations of the Waterfall model and of Scrum, respectively. We found that in agile-developed projects, communication related to the execution-monitoring-control phase of the Project Management Life Cycle is more prevalent, and communication related to the initiation phase occurs more frequently in informal channels. For both project types, communication primarily pertains to the software construction phase of the Software Development Life Cycle. After annotating communication contents using speech acts, representatives are found to be prevalent in informal channels for agile-developed projects, directives are more prevalent in informal channels for waterfall-developed projects, and expressives are more frequent in informal channels for both project types. We provide empirical evidence that development paradigms and communication channel formality impact written communication, with agile-developed projects showing more collaborative interactions in informal channels compared to waterfall-developed projects. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.}
}
@article{DISIPIO2024101256,
title = {LEV4REC: A feature-based approach to engineering RSSEs},
journal = {Journal of Computer Languages},
volume = {78},
pages = {101256},
year = {2024},
issn = {2590-1184},
doi = {https://doi.org/10.1016/j.cola.2023.101256},
url = {https://www.sciencedirect.com/science/article/pii/S2590118423000667},
author1 = {Claudio {Di Sipio} and Juri {Di Rocco} and Davide {Di Ruscio} and Phuong T. Nguyen},
keywords = {Recommender systems, Model-driven engineering, Feature modeling},
abstract = {To facilitate the development of recommender systems for software engineering (RSSEs), this paper introduces LEV4REC, a model-driven approach supporting all RSSE development stages, from design to deployment. It enables parameter fine-tuning, enhancing the developer and user experience by using a dedicated feature model for early configuration. We evaluated LEV4REC by applying it to two existing RSSEs based on different algorithms. Results demonstrate its ability to recreate suitable recommendations and outperform a state-of-the-art approach. Qualitative findings from a focus group study further validate LEV4REC’s effectiveness, while indicating the need for extension points to support additional systems.}
}
@article{BRABRA2020101450,
title = {Toward higher-level abstractions based on state machine for cloud resources elasticity},
journal = {Information Systems},
volume = {90},
pages = {101450},
year = {2020},
note = {Advances in Information Systems Engineering Best Papers of CAiSE 2018},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2019.101450},
url = {https://www.sciencedirect.com/science/article/pii/S0306437919305022},
author1 = {Hayet Brabra and Achraf Mtibaa and Walid Gaaloul and Boualem Benatallah},
keywords = {Abstractions, Elasticity, Cloud resources, State machine, Orchestration},
abstract = {With the dynamic nature of cloud applications and rapid change of their resource requirements, elasticity over cloud resources has to be effectively supported. It represents the ability to dynamically adjust cloud resources that applications use in order to adapt to their varying workloads, while maintaining the desired quality of service. However, implementing elasticity is still challenging task for cloud users as heterogeneous and low-level interfaces are provided to manage cloud resources. To alleviate this, we believe that elasticity features should be provided at resource description level. In this paper, we propose a new Cloud Resource Description Model called cRDM, which is based on State Machine formalism. Using this model, we aim at representing cloud resources while considering their elasticity behavior over the time without referring to any low level interfaces or cloud provider technical constraints. We also propose a software system based on this new specification to support the elasticity-aware orchestration of cloud resources by exploiting the underlying cloud orchestration tools and APIs. We rely on a real use case to demonstrate the applicability of the proposed system and conduct a set of experiments proving the productivity and expressiveness of the cRDM model in comparison to existing solutions. The resulted findings of our evaluation shows the efficiency of our proposal.}
}
@article{LEVA20233686,
title = {Node-level response time feedback loops to ease QoS control in “as a Service” architectures},
journal = {IFAC-PapersOnLine},
volume = {56},
number = {2},
pages = {3686-3691},
year = {2023},
note = {22nd IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2023.10.1534},
url = {https://www.sciencedirect.com/science/article/pii/S2405896323019420},
author1 = {Alberto Leva and Emilio Incerto},
keywords = {Quality of Service control, “as a Service” architectures, response time control, computing systems control, feedback linearisation, event-based control, PI control},
abstract = {Many ICT (Information and Communications Technology) functionalities are nowadays offered “as a Service” (aaS), i.e., by collecting user requests and routing them through a dynamically chosen set of network nodes. In such systems, enforcing the desired Quality of Service (QoS) is particularly complex. We argue that a prominent reason for this problem is that aaS frameworks invariantly miss a simple but crucial component, namely a layer of node-level response time control loops. We propose a solution to fill this gap by exploiting periodic event-based feedback linearisation and PI control. We demonstrate the potential of the said solution on a real application and briefly discuss the numerous implications to address in subsequent works.}
}
@article{CERNY2023111829,
title = {Catalog and detection techniques of microservice anti-patterns and bad smells: A tertiary study},
journal = {Journal of Systems and Software},
volume = {206},
pages = {111829},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111829},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223002248},
author1 = {Tomas Cerny and Amr S. Abdelfattah and Abdullah Al Maruf and Andrea Janes and Davide Taibi},
keywords = {Microservices, Anti-patterns, Antipatterns, Anti patterns, Bad smells, Software maintenance},
abstract = {Background:
Various works investigated microservice anti-patterns and bad smells in the past few years. We identified seven secondary publications that summarize these, but they have little overlap in purpose and often use different terms to describe the identified anti-patterns and smells.
Objective:
This work catalogs recurring bad design practices known as anti-patterns and bad smells for microservice architectures, and provides a classification into categories as well as methods for detecting these practices.
Method:
We conducted a systematic literature review in the form of a tertiary study targeting secondary studies identifying poor design practices for microservices.
Results:
We provide a comprehensive catalog of 58 disjoint anti-patterns, grouped into five categories, which we derived from 203 originally identified anti-patterns for microservices.
Conclusion:
The results provide a reference to microservice developers to design better-quality systems and researchers who aim to detect system quality based on anti-patterns. It also serves as an anti-pattern catalog for development-aiding tools, which are not currently available for microservice system development but could mitigate quality degradation throughout system evolution.}
}