@article{HEINRICH2020110722,
title = {Architectural runtime models for integrating runtime observations and component-based models},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110722},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110722},
url = {https://www.sciencedirect.com/science/article/pii/S016412122030159X},
author1 = {Robert Heinrich},
keywords = {Software architecture, Runtime model, Performance model, Workload, Palladio Component Model},
abstract = {Keeping track of modern software applications while dynamically changing requires strong interaction of evolution activities on development level and adaptation activities on operation level. Knowledge about software architecture is key for both, developers while evolving the system and operators while adapting the system. Existing architectural models used in development differ from those used in operation in terms of purpose, abstraction and content. Consequences are limited reuse of development models during operation, lost architectural knowledge and limited phase-spanning consideration of software architecture. In this paper, we propose modeling concepts of the iObserve approach to align architectural models used in development and operation. We present a correspondence model to bridge the divergent levels of abstraction between implementation artifacts and component-based architectural models. A transformation pipeline uses the information stored in the correspondence model to update architectural models based on changes during operation. Moreover, we discuss the modeling of complex workload based on observations during operation. In a case study-based evaluation, we examine the accuracy of our models to reflect observations during operation and the scalability of the transformation pipeline. Evaluation results show the accuracy of iObserve. Furthermore, evaluation results indicate iObserve adequately scales for some cases but shows scalability limits for others.}
}
@article{PASTORRICOS2023111645,
title = {Distributed state model inference for scriptless GUI testing},
journal = {Journal of Systems and Software},
volume = {200},
pages = {111645},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2023.111645},
url = {https://www.sciencedirect.com/science/article/pii/S0164121223000407},
author1 = {Fernando {Pastor Ricós} and Arend Slomp and Beatriz Marín and Pekka Aho and Tanja E.J. Vos},
keywords = {Distributed systems, Scriptless testing, State model inference, Empirical evaluation},
abstract = {State model inference of software applications through the Graphical User Interface (GUI) is a technique that identifies GUI states and transitions, and maps them into a model. Scriptless GUI testing tools can benefit substantially from the availability of these state models, for example, to improve the exploration, or have sophisticated test oracles. However, inferring models for large systems requires a long execution time. Our goal is to improve the speed of the state model inference process. To achieve this goal, this paper presents a distributed state model inference approach with an open source scriptless GUI testing tool. Moreover, in order to be able to infer a suitable model, we design a set of strategies to deal with abstraction challenges and to distinguish GUI states and transitions in the model. To validate it, we conduct an experiment with two open source web applications that have been tested with the distributed architecture using one to six Docker containers sharing the same state model. With the obtained results, we can conclude that it is feasible to infer a model with a distributed approach and that using the distributed approach reduces the time required for inferring a state model.}
}
@article{RIZVI201783,
title = {Three-Step Approach to QoS Maintenance in Cloud Computing Using a Third-Party Auditor},
journal = {Procedia Computer Science},
volume = {114},
pages = {83-92},
year = {2017},
note = {Complex Adaptive Systems Conference with Theme: Engineering Cyber Physical Systems, CAS October 30 – November 1, 2017, Chicago, Illinois, USA},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2017.09.014},
url = {https://www.sciencedirect.com/science/article/pii/S1877050917318082},
author1 = {Syed Rizvi and Hannah Roddy and Joseph Gualdoni and Ilva Myzyri},
keywords = {cloud computing, security evaluation, cloud service providers, cloud service agreement, quality of service, service level agreement},
abstract = {Cloud computing is a rapidly evolving service for potential clients who wish to outsource their data storage or subscribe to cloud services. In a data-driven society, every company needs to make cost-effective and responsible decisions in terms of how they handle their sensitive data. Should a company make the decision to use cloud services, they must complete the arduous task of not only choosing the right CSP for that particular firm, but also the constant monitoring of the data being stored and processed externally. In addition, under certain circumstances, there are laws that might prevent the firm from using certain CSPs. When using a cloud service, there is a need for ongoing agreements between cloud service users (CSUs) and cloud service providers (CSPs). Currently, this comes in the form of written agreements known as cloud service agreements (CSAs). CSAs contain information on the quality of service (QoS) that the CSP is promising to deliver to the CSU. However, CSAs can often be inflexible and unmaintained; therefore, there is a need for constant monitoring of the QoS being provided by the CSP. In this paper, we propose a model in which a third-party auditing body (e.g. cloud carriers, cloud brokers, cloud auditors) can assist a CSU in ensuring that they are receiving the promised services from their chosen CSP. To support this model, we propose a three-step approach for customers to evaluate the service-level agreement (SLA) of their CSAs. The three basic steps of our model are: an initial review of any valuable information useful to a CSA, an assessment of specific cloud metrics, and quarterly reevaluations of the CSA. Our proposed model will allow CSUs to be more confident that they are receiving the services they are subscribed to, as well as receiving the best QoS that a CSP can provide. With our model, any IT department, with the assistance of the third party, should be able to monitor and evaluate their cloud services and CSAs.}
}
@incollection{SALIMI2018279,
title = {Chapter 5 - Modeling and Simulation: The Essential Tools to Manage the Complexities},
editor = {Fabienne Salimi and Frederic Salimi},
booktitle = {A Systems Approach to Managing the Complexities of Process Industries},
publisher = {Elsevier},
pages = {279-407},
year = {2018},
isbn = {978-0-12-804213-7},
doi = {https://doi.org/10.1016/B978-0-12-804213-7.00005-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128042137000050},
author1 = {Fabienne Salimi and Frederic Salimi},
keywords = {Model Based System Engineering (MBSE), IIoT, Cybersecurity, Big Data management, Cloud computing, Virtual & Augmented reality, Industry 4.0, interoperability, Integration of PSM-OTS-RBI, ADEPP},
abstract = {This chapter discusses the modeling and simulations that are needed to understand and manage complex systems such as a process plant and its supply chain values. Recent progress in web technology, connectivity and interoperability of the automation provides the required tools to implement effectively process safety management system as a part of the operational excellence on an IIoT platform. A background discussion on industry 4.0, IIoT, Cybersecurity, cloud and fog computing, “Big Data Management,” virtual and augmented reality is given. The PLM and IIoT software are discussed and concept of Installation Lifecycle Management (ILM) for process industry is introduced. Chapter concluded the discussion by presentation of ADEPP approach to integrate Process Safety Management (PSM), Operator Training Simulator (OTS) and Risk Based Inspection (RBI) softwares on a user-friendly web based platform. Thanks to OPC connection facilities of Ignition software, the process simulations, Safety models and RBI interfaces are interoperated on visualised on a common User Graphic Interface.}
}
@incollection{2019431,
title = {Index},
editor = {Lin Zhang and Bernard P. Zeigler and Yuanjun laili},
booktitle = {Model Engineering for Simulation},
publisher = {Academic Press},
pages = {431-439},
year = {2019},
isbn = {978-0-12-813543-3},
doi = {https://doi.org/10.1016/B978-0-12-813543-3.09989-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780128135433099893}
}
@incollection{2017395,
title = {Index},
editor = {Paul Göransson and Chuck Black and Timothy Culver},
booktitle = {Software Defined Networks (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {395-409},
year = {2017},
isbn = {978-0-12-804555-8},
doi = {https://doi.org/10.1016/B978-0-12-804555-8.09984-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780128045558099841}
}
@incollection{TEKINERDOGAN20161,
title = {Chapter 1 - Quality concerns in large-scale and complex software-intensive systems},
editor = {Ivan Mistrik and Richard Soley and Nour Ali and John Grundy and Bedir Tekinerdogan},
booktitle = {Software Quality Assurance},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-17},
year = {2016},
isbn = {978-0-12-802301-3},
doi = {https://doi.org/10.1016/B978-0-12-802301-3.00001-6},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023013000016},
author1 = {Bedir Tekinerdogan and Nour Ali and John Grundy and Ivan Mistrik and Richard Soley},
keywords = {Software quality management, software quality assurance, software quality metrics},
abstract = {Software quality management (SQM) is the collection of all processes that ensure that software products, services, and life cycle process implementations meet organizational software quality objectives and achieve stakeholder satisfaction. SQM comprises three basic subcategories: software quality planning, software quality assurance (SQA), and software quality control and software process improvement. This chapter provides a general overview of the SQA domain and discuss the related concept. A conceptual model for software quality framework is provided together with the current approaches for SQA. The chapter concludes with some of the identified challenges and future challenges regarding SQA.}
}
@article{MEDING2021111006,
title = {MeTeaM—A method for characterizing mature software metrics teams},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111006},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111006},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001035},
author1 = {Wilhelm Meding and Miroslaw Staron and Ola Söder},
keywords = {Metrics, Measurements, Software, Team maturity, Software engineering, MeTeaM},
abstract = {Background:
Metrics teams play an increasingly important role in handling data and information in modern software development organizations; they manage their companies’ measurement programs, collect and process data, and develop and distribute information products. Metrics teams can comprise several roles, and their set-up can differ between companies, as can the metrics maturity of host organizations. These differences impact the effectiveness and quality of a team’s measurement program.
Objective:
Our objective was to design and evaluate a model to describe the characteristics of a mature metrics team, which efficiently designs, develops, maintains, and evolves its organization’s measurement program.
Method:
We conducted an action research study on four metrics teams of four distinct companies. We designed and evaluated a domain-specific model for assessing the maturity of metrics teams – MeTeaM – and also assessed the four metrics teams per se.
Results:
Our results were two-fold: the creation of the metrics team maturity model MeTeaM and a template to assess metrics teams. Our evaluation showed that the model captures the characteristics of successful metrics teams and quantifies the maturity status of both the metrics teams and their host organizations.
Conclusions:
More mature metrics teams score higher in the MeTeaM model than less mature teams. The assessment provides less mature metrics teams with valuable insights on what factors to improve. Such insights can be shared with and acted upon successfully with their organizations.}
}
@incollection{BETZ20111,
title = {Chapter 1 - IT in a World of Continuous Improvement},
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {1-31},
year = {2011},
isbn = {978-0-12-385017-1},
doi = {https://doi.org/10.1016/B978-0-12-385017-1.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850171000018},
author1 = {Charles T. Betz},
abstract = {Publisher Summary
This chapter introduces a book that deals with enabling IT value and increasing its flow while reducing IT waste through presenting reference architecture for an automated and integrated IT management system. In applying architectural methods to a given domain, that domain must be well understood. Users must go to the real, operational heart of the problem trying to be solved. This introductory chapter starts with the underpinnings for an architectural exploration of large-scale IT management focusing on what is “Information Technology,” what is a “Service” and an “IT service,” a discussion on “Lean,” what an “IT Value” is, how to improve the delivery of “IT Value” through concepts such as Lean, and the relationship of Lean IT to other industry trends in IT. There are many discussions in the field of information technology, and many of them stem from basic questions of definition. To discuss IT value and the specific practices and tools needed to enable it, one needs to pay attention to fundamental principles and definitions. Information technology is assumed to encompass well-known historic variants such as “Information Systems,” “Data Processing,” and “Management Information Systems.” IT is about computing practices and assets applied to enterprise needs.}
}
@article{RESTREPO2021111010,
title = {A sustainable-development approach for self-adaptive cyber–physical system’s life cycle: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {180},
pages = {111010},
year = {2021},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2021.111010},
url = {https://www.sciencedirect.com/science/article/pii/S0164121221001072},
author1 = {Luisa Restrepo and Jose Aguilar and Mauricio Toro and Elizabeth Suescún},
keywords = {Self-adaptive systems, Sustainability, Cyber–physical systems, Systems-development life-cycle},
abstract = {Cyber–Physical Systems (CPS) refer to a new generation of systems where the cyber and physical layers are –strongly– interconnected. The development of these systems requires two fundamental parts. First, the design of sustainable architectures –centered on adaptation, throughout a System-Development Life-Cycle (SDLC)– to develop robust and economically profitable products. Second, the use of self-adaptive techniques to adjust CPSs to the evolving circumstances of their operation context. This work presents a systematic mapping study (SMS) that discusses different approaches used to develop self-adaptive CPSs (SA-CPSs) at each stage of the SDLC, focused on sustainability. The results show trends such as (i) Designs are not limited to particular application domains, (ii) Performance was the most commonly used attribute, and (iii) Monitor–Analyze–Plan–Execute over a shared Knowledge (MAPE-K) is the predominant feedback loop applied in the cyber layer. The results also raise challenges such as (i) How to design and evaluate sustainable SA-CPSs, (ii) How to apply unit and integration testing in the development of SA-CPSs, and (iii) How to develop feedback loops on SA-CPSs with the integration of machine-learning techniques.}
}
@incollection{2016363,
title = {Subject Index},
editor = {Ivan Mistrik and Richard Soley and Nour Ali and John Grundy and Bedir Tekinerdogan},
booktitle = {Software Quality Assurance},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {363-373},
year = {2016},
isbn = {978-0-12-802301-3},
doi = {https://doi.org/10.1016/B978-0-12-802301-3.00026-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128023013000260}
}
@article{FAYOUMI2021100221,
title = {An integrated socio-technical enterprise modelling: A scenario of healthcare system analysis and design},
journal = {Journal of Industrial Information Integration},
volume = {23},
pages = {100221},
year = {2021},
issn = {2452-414X},
doi = {https://doi.org/10.1016/j.jii.2021.100221},
url = {https://www.sciencedirect.com/science/article/pii/S2452414X21000212},
author1 = {Amjad Fayoumi and Richard Williams},
keywords = {Enterprise Modelling, Socio-technical Systems, Enterprise Integrated Model, Conceptual Modelling, Healthcare System},
abstract = {One of the crucial issues facing enterprise modelling (EM) practices is that EM is considered technical, and rarely or never has a social focus. Social aspects referred to here are the soft aspects of the organisation that lead to organic organisation development (communication, collaboration, culture, skills and personal goals). There are many EM approaches and enterprise architecture frameworks were proposed recently. These cover different enterprise aspects, perspectives, artefacts and models with different qualities and levels of details. Yet, the imperative determination has overlaid the declarative exploration in EM as a necessity of the design effort. Rethinking the assumptions underlying EM should bring a new and different understanding on how EM can be tackled within the enterprise, in particular the joint development and optimisation of socio-technical systems. This paper discusses EM from a socio-technical systems (STS) perspective, and towards forming a new model of EM that is driven from STS theory and combined with STS practices. Then proposes a conceptual integrated model that incorporates the new concepts of STS toward building an EM framework for balanced socio-technical joint development and optimisation. The approach is illustrated in a scenario from healthcare industry. A combination between modelling and STS practices proved powerful for holistic IT modernisation, future work discussed toward the end of the paper.}
}
@article{MOHAMAD2024112082,
title = {Managing security evidence in safety-critical organizations},
journal = {Journal of Systems and Software},
volume = {214},
pages = {112082},
year = {2024},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112082},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224001274},
author1 = {Mazen Mohamad and Jan-Philipp Steghöfer and Eric Knauss and Riccardo Scandariato},
keywords = {Security, Assurance, Evidence, Safety-critical},
abstract = {With the increasing prevalence of open and connected products, cybersecurity has become a serious issue in safety-critical domains such as the automotive industry. As a result, regulatory bodies have become more stringent in their requirements for cybersecurity, necessitating security assurance for products developed in these domains. In response, companies have implemented new or modified processes to incorporate security into their product development lifecycle, resulting in a large amount of evidence being created to support claims about the achievement of a certain level of security. However, managing evidence is not a trivial task, particularly for complex products and systems. This paper presents a qualitative interview study conducted in six companies on the maturity of managing security evidence in safety-critical organizations. We find that the current maturity of managing security evidence is insufficient for the increasing requirements set by certification author1ities and standardization bodies. Organizations currently fail to identify relevant artifacts as security evidence and manage this evidence on an organizational level. One part of the reason are educational gaps, the other a lack of processes. The impact of AI on the management of security evidence is still an open question.}
}
@article{RAIBULET2018409,
title = {Collaborative and teamwork software development in an undergraduate software engineering course},
journal = {Journal of Systems and Software},
volume = {144},
pages = {409-422},
year = {2018},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.07.010},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218301389},
author1 = {Claudia Raibulet and Francesca {Arcelli Fontana}},
keywords = {Collaborative software development, Teamwork, Software engineering course, GitHub, SonarQube, Microsoft project},
abstract = {Two key elements of modern software development are collaboration and teamwork. Current methodologies (e.g., agile) and platforms are based on these key elements. This paper describes our experience in stimulating collaboration and teamwork activities of students in the context of a software engineering course at the third year of an undergraduate program in computer science at the University of Milano-Bicocca in Italy. The students were asked to develop a software project in teams of 3 to 5 students for the final exam of the course. The students used GitHub as a collaborative software development platform. In addition, they analyzed the quality of the developed software through SonarQube. The students were also asked to perform project management tasks (e.g., the Gantt) using the Microsoft Project tool. At the end of the course, we gathered the student feedback through a questionnaire on their collaboration and teamwork experience (through GitHub and Microsoft Project tools) and on the use of a software analysis assessment tool, i.e., SonarQube. From their feedback, the students were enthusiastic about working in teams for their project development and about learning how to use tools which are exploited not only in the academic world but also in industry.}
}
@incollection{THOMASIAN202289,
title = {Chapter 2 - Storage technologies and their data},
editor = {Alexander Thomasian},
booktitle = {Storage Systems},
publisher = {Morgan Kaufmann},
pages = {89-196},
year = {2022},
isbn = {978-0-323-90796-5},
doi = {https://doi.org/10.1016/B978-0-32-390796-5.00011-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780323907965000115},
author1 = {Alexander Thomasian},
keywords = {Punched cards, core memories, dynamic RAM, static RAS, magnetic disks, magnetic tapes, magnetic bubble memories, charged couple devices - CCDs, phase change memory - PCM, micro-electro-mechanical systems, flash memory, optical storage, persistent memory forum, data compression, Huffman encoding, Lempel-Ziv coding, arithmetic coding, main memory compression, data deduplication, Pure Storage Purity, Intel Optane, Cleversafe, hyperconverged infrastructure, OceanStore, cloud storage, Inktomi, data encryption},
abstract = {Paper allowed writing and later printing. Punch cards were used to store census data, company records, and allowed simple data processing. Magnetic tapes and disks (also optical) held data, text, images, audio and video. Tapes were used for batch data processing and disks with random access capability also allowed online processing. High bandwidth communications have resulted in a paradigm shift by downloading and streaming to IPOD rather than Compact Disks - CDs and Digital Versatile Disks - DVDs. Tapes only accessible sequentially are used for batch processing, while disks allow random access via hashing or indexing (B+-trees) Linera and extensible hashing allow additions and deletions. For high performance flash memories and other Storage Class Memories - SCMs such as Phase Change Memory - PCM are expected to replace magnetic disks. Data compression and deduplication can be used in preserving storage space and network bandwidth utilization and encryption for data security and privacy.}
}
@article{GESVINDR2020110701,
title = {Architecture design evaluation of PaaS cloud applications using generated prototypes: PaaSArch Cloud Prototyper tool},
journal = {Journal of Systems and Software},
volume = {169},
pages = {110701},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110701},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301485},
author1 = {David Gesvindr and Ondrej Gasior and Barbora Buhnova},
keywords = {Cloud computing, Software architecture design, Prototype generation, Quality evaluation, Performance, Internet of things},
abstract = {Platform as a Service (PaaS) cloud domain brings great benefits of an elastic platform with many prefabricated services, but at the same time challenges software architects who need to navigate a rich set of services, variability of PaaS cloud environment and quality conflicts in existing design tactics, which makes it almost impossible to foresee the impact of architectural design decisions on the overall application quality without time-consuming implementation of application prototypes. To ease the architecture design of PaaS cloud applications, this paper proposes a design-time quality evaluation approach for PaaS cloud applications based on automatically generated prototypes, which are deployed to the cloud and repeatedly evaluated in the context of multiple quality attributes and environment configurations. In this paper, all steps of the approach are described and demonstrated on an example of a real-world complex IoT system for collection and processing of Smart Home sensor data. The approach has been implemented and the automated prototype generation and evaluation tool, referred to as PaaSArch Cloud Prototyper, is presented together with the approach.}
}
@article{CARRASCO2018167,
title = {Trans-cloud: CAMP/TOSCA-based bidimensional cross-cloud},
journal = {Computer Standards & Interfaces},
volume = {58},
pages = {167-179},
year = {2018},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2018.01.005},
url = {https://www.sciencedirect.com/science/article/pii/S0920548917303185},
author1 = {Jose Carrasco and Francisco Durán and Ernesto Pimentel},
keywords = {Cloud applications, Multi-deployment, Cross-deployment, Apache Brooklyn, TOSCA, CAMP},
abstract = {The diversity in the way in which different cloud providers offer their services, give their SLAs, present their QoS, or support different technologies complicates the portability and interoperability of cloud applications, and favors vendor lock-in. Trying to solve these issues, we have recently witnessed the proposal of unified APIs for IaaS services, unified APIs for PaaS services, and a variety of cross-cloud application management tools. We go one step further in the unification of cloud services, building on the TOSCA and CAMP standards, with a proposal in which the management of IaaS and PaaS services, possibly offered by different providers, are integrated into a unified interface. The TOSCA standard is used for the definition of portable models describing the topology of cloud applications and the required resources in an agnostic, providers-and-resources-independent way. Based on the CAMP standard, we abstract from the particularities of specific providers. Indeed, to change the service on which any of the modules of an application is to be deployed, whether it be IaaS or PaaS, we just need to change its target location by picking from the catalog of supported locations. We provide insights into our implementation on Apache Brooklyn, present a non-trivial case study that illustrates our approach, and show some experimental results.}
}
@article{DEMDELESPOSTE2019427,
title = {Design and evaluation of a scalable smart city software platform with large-scale simulations},
journal = {Future Generation Computer Systems},
volume = {93},
pages = {427-441},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2018.10.026},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18307301},
author1 = {Arthur {de M. Del Esposte} and Eduardo F.Z. Santana and Lucas Kanashiro and Fabio M. Costa and Kelly R. Braghetto and Nelson Lago and Fabio Kon},
keywords = {Smart cities, Smart urban spaces, Middleware, Simulation, Scalability, Open source, Microservices},
abstract = {Smart Cities combine advances in Internet of Things, Big Data, Social Networks, and Cloud Computing technologies with the demand for cyber–physical applications in areas of public interest, such as Health, Public Safety, and Mobility. The end goal is to leverage the use of city resources to improve the quality of life of its citizens. Achieving this goal, however, requires advanced support for the development and operation of applications in a complex and dynamic environment. Middleware platforms can provide an integrated infrastructure that enables solutions for smart cities by combining heterogeneous city devices and providing unified, high-level facilities for the development of applications and services. Although several smart city platforms have been proposed in the literature, there are still open research and development challenges related to their scalability, maintainability, interoperability, and reuse in the context of different cities, to name a few. Moreover, available platforms lack extensive scientific validation, which hinders a comparative analysis of their applicability. Aiming to close this gap, we propose InterSCity, a microservices-based, open-source, smart city platform that enables the collaborative development of large-scale systems, applications, and services for the cities of the future, contributing to turn them into truly smart cyber–physical environments. In this paper, we present the architecture of the InterSCity platform, followed by a comprehensive set of experiments that evaluate its scalability. The experiments were conducted using a smart city simulator to generate realistic workloads used to assess the platform in extreme conditions. The experimental results demonstrate that the platform can scale horizontally to handle the highly dynamic demands of a large smart city while maintaining low response times. The experiments also show the effectiveness of the technique used to generate synthetic workloads.}
}
@article{DEGOUW2019108,
title = {On the modeling of optimal and automatized cloud application deployment},
journal = {Journal of Logical and Algebraic Methods in Programming},
volume = {107},
pages = {108-135},
year = {2019},
issn = {2352-2208},
doi = {https://doi.org/10.1016/j.jlamp.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S235222081830138X},
author1 = {Stijn {de Gouw} and Jacopo Mauro and Gianluigi Zavattaro},
keywords = {Software system modeling, Declarative specification of deployment rules, Automatic cloud application deployment},
abstract = {We investigate the problem of modeling the optimal and automatic deployment of cloud applications. We follow an approach based on three main pillars: (i) the specification of the computing resources needed by software components and those provided by the executing environment (e.g. virtual machines or containers), (ii) the declarative description of deployment rules, (iii) and the computation of an optimal deployment that minimizes the total cost by using constraint solving techniques. We experiment with such an approach by applying it to the Abstract Behavioural Specification language ABS, and we validate it by modeling and simulating with ABS (and its tool-suite) the Fredhopper Cloud Services, a worldwide system offering e-Commerce services, currently deployed on Amazon EC2.}
}
@article{PEREZ2020110657,
title = {Systematic literature reviews in software engineering—enhancement of the study selection process using Cohen’s Kappa statistic},
journal = {Journal of Systems and Software},
volume = {168},
pages = {110657},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110657},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301217},
author1 = {Jorge Pérez and Jessica Díaz and Javier Garcia-Martin and Bernardo Tabuenca},
keywords = {Systematic review, Evidence-based practice, Cohen’s kappa},
abstract = {Context:
Systematic literature reviews (SLRs) rely on a rigorous and auditable methodology for minimizing biases and ensuring reliability. A common kind of bias arises when selecting studies using a set of inclusion/exclusion criteria. This bias can be decreased through dual revision, which makes the selection process more time-consuming and remains prone to generating bias depending on how each researcher interprets the inclusion/exclusion criteria.
Objective:
To reduce the bias and time spent in the study selection process, this paper presents a process for selecting studies based on the use of Cohen’s Kappa statistic. We have defined an iterative process based on the use of this statistic during which the criteria are refined until obtain almost perfect agreement (k>0.8). At this point, the two researchers interpret the selection criteria in the same way, and thus, the bias is reduced. Starting from this agreement, dual review can be eliminated; consequently, the time spent is drastically shortened.
Method:
The feasibility of this iterative process for selecting studies is demonstrated through a tertiary study in the area of software engineering on works that were published from 2005 to 2018.
Results:
The time saved in the study selection process was 28% (for 152 studies) and if the number of studies is sufficiently large, the time saved tend asymptotically to 50%.
Conclusions:
Researchers and students may take advantage of this iterative process for selecting studies when conducting SLRs to reduce bias in the interpretation of inclusion and exclusion criteria. It is especially useful for research with few resources.}
}
@article{DELIAS2021101812,
title = {Prototyping a business process improvement plan. An evidence-based approach},
journal = {Information Systems},
volume = {101},
pages = {101812},
year = {2021},
issn = {0306-4379},
doi = {https://doi.org/10.1016/j.is.2021.101812},
url = {https://www.sciencedirect.com/science/article/pii/S0306437921000594},
author1 = {Pavlos Delias and Gia-Thi Nguyen},
keywords = {Process mining, Business process improvement, Decision support, Bipartite network analysis},
abstract = {Knowledge absorption, information, and management tactics play a vital role for organizations to pursue process improvement. In this work, we aim to support organizations in building a prototype business process improvement plan through an evidence-based approach to make the most of the available information and knowledge. We exploit existing data that describe the business process’s execution to capture the governing process behaviors. By comparing the process’s execution across multiple business units, we assess the versatility of organizations and the pervasiveness of the process behaviors. Through a bipartite network, we suggest the concept of Operations Sophistication and deliver insights for the improvement prospects. Our approach contributes by identifying the behaviors with the greatest potential in a rapid and evidence-based way, by indicating a prioritization customized per organization, and by highlighting the most secure change paths. To illustrate our approach, we applied it to a real-world case from Siemens AG, and we present the methodology and the results through that case study.}
}
@incollection{WALKER2019155,
title = {Chapter Six - Testing at scale of IoT blockchain applications},
editor = {Shiho Kim and Ganesh Chandra Deka and Peng Zhang},
series = {Advances in Computers},
publisher = {Elsevier},
volume = {115},
pages = {155-179},
year = {2019},
booktitle = {Role of Blockchain Technology in IoT Applications},
issn = {0065-2458},
doi = {https://doi.org/10.1016/bs.adcom.2019.07.008},
url = {https://www.sciencedirect.com/science/article/pii/S0065245819300348},
author1 = {Michael A. Walker and Douglas C. Schmidt and Abhishek Dubey},
keywords = {Automation, Blockchain, Distributed systems, IoT, Scalability, Testing, Testing at scale},
abstract = {Due to the ever-increasing adaptation of Blockchain technologies in the private, public, and business domains, both the use of Distributed Systems and the increased demand for their reliability has exploded recently, especially with their desired integration with Internet-of-Things devices. This has resulted in a lot of work being done in the fields of distributed system analysis and design, specifically in the areas of blockchain smart contract design and formal verification. However, the focus on formal verification methodologies has meant that less attention has been given toward more traditional testing methodologies, such as unit testing and integration testing. This includes a lack of full support by most, if not all, the major blockchain implementations for testing at scale, except on fully public test networks. This has several drawbacks, such as: (1) The inability to do repeatable testing under identical scenarios, (2) reliance upon public mining of blocks, which introduces unreasonable amounts of delay for a test driven development scenario that a private network could reduce or eliminate, and (3) the inability to design scenarios where parts of the network go down. In this chapter we discuss design, testing methodologies, and tools to allow Testing at Scale of IoT Blockchain Applications.}
}
@incollection{GRAY20161,
title = {Chapter 1 - Network Function Virtualization},
editor = {Ken Gray and Thomas D. Nadeau},
booktitle = {Network Function Virtualization},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {1-18},
year = {2016},
isbn = {978-0-12-802119-4},
doi = {https://doi.org/10.1016/B978-0-12-802119-4.00001-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021194000018},
author1 = {Ken Gray and Thomas D. Nadeau},
keywords = {Network function virtualization, NFV, COTS, service function chaining, SFC},
abstract = {In this chapter, we define what Network Function Virtualization is in our view. We also take the time to discuss what its key components are and more importantly, are not.}
}
@article{CAMILLI2022111225,
title = {Automated test-based learning and verification of performance models for microservices systems},
journal = {Journal of Systems and Software},
volume = {187},
pages = {111225},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111225},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000061},
author1 = {Matteo Camilli and Andrea Janes and Barbara Russo},
keywords = {Performance testing, Test-based model learning, Markov models, Automated verification, Microservices},
abstract = {Effective and automated verification techniques able to provide assurances of performance and scalability are highly demanded in the context of microservices systems. In this paper, we introduce a methodology that applies specification-driven load testing to learn the behavior of the target microservices system under multiple deployment configurations. Testing is driven by realistic workload conditions sampled in production. The sampling produces a formal description of the users’ behavior through a Discrete Time Markov Chain. This model drives multiple load testing sessions that query the system under test and feed a Bayesian inference process which incrementally refines the initial model to obtain a complete specification from run-time evidence as a Continuous Time Markov Chain. The complete specification is then used to conduct automated verification by using probabilistic model checking and to compute a configuration score that evaluates alternative deployment options. This paper introduces the methodology, its theoretical foundation, and the toolchain we developed to automate it. Our empirical evaluation shows its applicability, benefits, and costs on a representative microservices system benchmark. We show that the methodology detects performance issues, traces them back to system-level requirements, and, thanks to the configuration score, provides engineers with insights on deployment options. The comparison between our approach and a selected state-of-the-art baseline shows that we are able to reduce the cost up to 73% in terms of number of tests. The verification stage requires negligible execution time and memory consumption. We observed that the verification of 360 system-level requirements took ∼1 minute by consuming at most 34 KB. The computation of the score involved the verification of ∼7k (automatically generated) properties verified in ∼72 seconds using at most ∼50 KB.}
}
@article{LU2019564,
title = {uBaaS: A unified blockchain as a service platform},
journal = {Future Generation Computer Systems},
volume = {101},
pages = {564-575},
year = {2019},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2019.05.051},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X18319873},
author1 = {Qinghua Lu and Xiwei Xu and Yue Liu and Ingo Weber and Liming Zhu and Weishan Zhang},
keywords = {Blockchain, Architecture, Design patterns, Deployment, Blockchain as a service},
abstract = {Blockchain is an innovative distributed ledger technology which has attracted a wide range of interests for building the next generation of applications to address lack-of-trust issues in business. Blockchain as a service (BaaS) is a promising solution to improve the productivity of blockchain application development. However, existing BaaS deployment solutions are mostly vendor-locked: they are either bound to a cloud provider or a blockchain platform. In addition to deployment, design and implementation of blockchain-based applications is a hard task requiring deep expertise. Therefore, this paper presents a unified blockchain as a service platform (uBaaS) to support both design and deployment of blockchain-based applications. The services in uBaaS include deployment as a service, design pattern as a service and auxiliary services. In uBaaS, deployment as a service is platform agnostic, which can avoid lock-in to specific cloud platforms, while design pattern as a service applies design patterns for data management and smart contract design to address the scalability and security issues of blockchain. The proposed solutions are evaluated using a real-world quality tracing use case in terms of feasibility and scalability.}
}
@article{GARCIAVALLS201883,
title = {Introducing the new paradigm of Social Dispersed Computing: Applications, Technologies and Challenges},
journal = {Journal of Systems Architecture},
volume = {91},
pages = {83-102},
year = {2018},
issn = {1383-7621},
doi = {https://doi.org/10.1016/j.sysarc.2018.05.007},
url = {https://www.sciencedirect.com/science/article/pii/S1383762118301036},
author1 = {Marisol García-Valls and Abhishek Dubey and Vicent Botti},
keywords = {Social dispersed computing, IoT, Fog Computing, Cloud Computing, Dispersed Computing, Social Computing, Edge Computing, Distributed computing, Cyber physical systems, Real time, Middleware, Virtualization, Containers, Microservices, Distributed transactions, Blockchain, Multi agent systems, Distributed coordination, Complex event processing, Networking},
abstract = {If last decade viewed computational services as a utilitythen surely this decade has transformed computation into a commodity. Computation is now progressively integrated into the physical networks in a seamless way that enables cyber-physical systems (CPS) and the Internet of Things (IoT) meet their latency requirements. Similar to the concept of “platform as a service” or “software as a service”, both cloudlets and fog computing have found their own use cases. Edge devices (that we call end or user devices for disambiguation) play the role of personal computers, dedicated to a user and to a set of correlated applications. In this new scenario, the boundaries between the network node, the sensor, and the actuator are blurring, driven primarily by the computation power of IoT nodes like single board computers and the smartphones. The bigger data generated in this type of networks needs clever, scalable, and possibly decentralized computing solutions that can scale independently as required. Any node can be seen as part of a graph, with the capacity to serve as a computing or network router node, or both. Complex applications can possibly be distributed over this graph or network of nodes to improve the overall performance like the amount of data processed over time. In this paper, we identify this new computing paradigm that we call Social Dispersed Computing, analyzing key themes in it that includes a new outlook on its relation to agent based applications. We architect this new paradigm by providing supportive application examples that include next generation electrical energy distribution networks, next generation mobility services for transportation, and applications for distributed analysis and identification of non-recurring traffic congestion in cities. The paper analyzes the existing computing paradigms (e.g., cloud, fog, edge, mobile edge, social, etc.), solving the ambiguity of their definitions; and analyzes and discusses the relevant foundational software technologies, the remaining challenges, and research opportunities.}
}
@article{KAUR2022109281,
title = {A review on Virtualized Infrastructure Managers with management and orchestration features in NFV architecture},
journal = {Computer Networks},
volume = {217},
pages = {109281},
year = {2022},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2022.109281},
url = {https://www.sciencedirect.com/science/article/pii/S1389128622003395},
author1 = {Karamjeet Kaur and Veenu Mangat and Krishan Kumar},
keywords = {Virtualized Infrastructure Manager, Network Function Virtualization, Service Function Chain, VNF manager, Mininet, OpenStack, Kubernetes, Virtual machine, Container, Docker, Tacker, Mini-NFV},
abstract = {Nowadays, Network Function Virtualization (NFV) is a growing and powerful technology in the research community and IT world. Traditional computer networks consist of hardware appliances such as firewalls and load balancers, called middleboxes. The implementation of these hardware devices is a difficult task due to their proprietary nature. NFV proposes an alternative way to design and deploy network functions called Virtual Network Functions (VNFs) on top of the commercial hardware by leveraging virtualization technology. NFV offers many advantages such as flexibility, agility, reduced capital and operational expenditure over the traditional network architecture. With the emergence of VNF, NFV needs to add new features regarding life-cycle management and end-to-end orchestration of VNFs. To fulfill this demand, NFV introduced the NFV-MANO framework for the management and orchestration of VNFs and provide network services to users. The NFV-MANO consists of NFV Orchestrator (NFVO), VNF Manager (VNFM), and Virtualized Infrastructure Manager (VIM). This paper provides a comprehensive overview of Virtualized Infrastructure Managers with NFV orchestration and VNF Management for implementing Service Function Chain (SFC) in NFV architecture. Further, this study critically analyzes relevant research articles and proposes a taxonomy to select an appropriate VIM based on Emulation, Virtualization, Containerization, and Hybrid environment for reliable SFC provisioning. Finally, various use cases have been identified for selecting particular VIM according to the requirements of the application.}
}
@article{KHAN2019396,
title = {Landscaping systematic mapping studies in software engineering: A tertiary study},
journal = {Journal of Systems and Software},
volume = {149},
pages = {396-436},
year = {2019},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2018.12.018},
url = {https://www.sciencedirect.com/science/article/pii/S0164121218302784},
author1 = {Muhammad Uzair Khan and Salman Sherin and Muhammad Zohaib Iqbal and Rubab Zahid},
keywords = {Tertiary study, Systematic mapping study, Secondary study, Survey, Software engineering},
abstract = {Context
A number of Systematic Mapping Studies (SMSs) that cover Software Engineering (SE) are reported in literature. Tertiary studies synthesize the secondary studies to provide a holistic view of an area.
Objectives
We synthesize SMSs in SE to provide insights into existing SE areas and to investigate the trends and quality of SMSs.
Methodology
We use Systematic Literature Review protocol to analyze and map the SMSs in SE, till August 2017, to SE Body of Knowledge (SWEBOK).
Results
We analyze 210 SMSs and results show that: (1) Software design and construction are most active areas in SE; (2) Some areas lack SMSs, including mathematical foundations, software configuration management, and SE tools; (3) The quality of SMSs is improving with time; (4) SMSs in journals have higher quality than SMSs in conferences and are cited more often; (5) Low quality in SMSs can be attributed to a lack of quality assessment in SMSs and not reporting information about the primary studies.
Conclusion
There is a potential for more SMSs in some SE areas. A number of SMSs do not provide the required information for an SMS, which leads to a low quality score.}
}
@article{DEHURY20201,
title = {CCoDaMiC: A framework for Coherent Coordination of Data Migration and Computation platforms},
journal = {Future Generation Computer Systems},
volume = {109},
pages = {1-16},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.03.029},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19330924},
author1 = {Chinmaya Kumar Dehury and Satish Narayana Srirama and Tek Raj Chhetri},
keywords = {Data pipeline, Data flow management, Serverless computing, Data migration, TOSCA},
abstract = {The amount of data generated by millions of connected IoT sensors and devices is growing exponentially. The need to extract relevant information from this data in modern and future generation computing system, necessitates efficient data handling and processing platforms that can migrate such big data from one location to other locations seamlessly and securely, and can provide a way to preprocess and analyze that data before migrating to the final destination. Various data pipeline architectures have been proposed allowing the data administrator/user to handle the data migration operation efficiently. However, the modern data pipeline architectures do not offer built-in functionalities for ensuring data veracity, which includes data accuracy, trustworthiness and security. Furthermore, allowing the intermediate data to be processed, especially in the serverless computing environment, is becoming a cumbersome task. In order to fill this research gap, this paper introduces an efficient and novel data pipeline architecture, named as CCoDaMiC (Coherent Coordination of Data Migration and Computation), which brings both the data migration operation and its computation together into one place. This also ensures that the data delivered to the next destination/pipeline block is accurate and secure. The proposed framework is implemented in private OpenStack environment and Apache Nifi.}
}
@incollection{2011425,
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {425-439},
year = {2011},
isbn = {978-0-12-385017-1},
doi = {https://doi.org/10.1016/B978-0-12-385017-1.00011-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850171000110}
}
@article{SHAHIN2020110752,
title = {Architectural Design Space for Modelling and Simulation as a Service: A Review},
journal = {Journal of Systems and Software},
volume = {170},
pages = {110752},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2020.110752},
url = {https://www.sciencedirect.com/science/article/pii/S0164121220301746},
author1 = {Mojtaba Shahin and M. Ali Babar and Muhammad Aufeef Chauhan},
keywords = {Modelling and Simulation as a Service, MSaaS, Architecture, Systematic review},
abstract = {Modelling and Simulation as a Service (MSaaS) is a promising approach to deploy and execute Modelling and Simulation (M&S) applications quickly and on-demand. An appropriate software architecture is essential to deliver quality M&S applications following the MSaaS concept to a wide range of users. This study aims to characterize the state-of-the-art MSaaS architectures by conducting a systematic review of 31 papers published from 2010 to 2018. Our findings reveal that MSaaS applications are mainly designed using layered architecture style, followed by service-oriented architecture, component-based architecture, and pluggable component-based architecture. We also found that interoperability and deployability have the greatest importance in the architecture of MSaaS applications. In addition, our study indicates that the current MSaaS architectures do not meet the critical user requirements of modern M&S applications appropriately. Based on our results, we recommend that there is a need for more effort and research to (1) design the user interfaces that enable users to build and configure simulation models with minimum effort and limited domain knowledge, (2) provide mechanisms to improve the deployability of M&S applications, and (3) gain a deep insight into how M&S applications should be architected to respond to the emerging user requirements in the military domain.}
}
@incollection{BETZ2011151,
title = {Chapter 3 - Patterns for the IT Processes},
editor = {Charles T. Betz},
booktitle = {Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children (Second Edition)},
publisher = {Morgan Kaufmann},
edition = {Second Edition},
address = {Boston},
pages = {151-241},
year = {2011},
isbn = {978-0-12-385017-1},
doi = {https://doi.org/10.1016/B978-0-12-385017-1.00003-1},
url = {https://www.sciencedirect.com/science/article/pii/B9780123850171000031},
author1 = {Charles T. Betz},
abstract = {Publisher Summary
This chapter discusses the major IT processes and some nonobvious, interesting patterns for implementing them, especially across functional boundaries. Such integrations represent a clear maturation of any large IT organization, as these processes are larger grained and deliver higher-order value. But they are not the highest order of value, which is to be found in the longest lived value streams of technology product, asset, infrastructure service, and application service. The pervasive effect of these lifecycles on the IT capability as a whole indicates the value to be found in managing them. The objective of the pattern analysis is to tie the system's architecture, data, and processes together across the functional barriers, so that the value chain (and its governance) is enabled. The patterns focus especially on breaking down the functional boundaries between IT planning, solution development, and service management, and enabling the accuracy of the core information store at the heart of well-managed IT. The chapter focuses on patterns for the nine major IT processes: accept demand, execute project, deliver release, complete change, fulfill service request, deliver transactional service, restore service, improve service, and retire service. Because processes are known for crossing functions and sharing data, describing specifically how these coordinations may take place is the primary goal of this discussion. A “Lifecycle implications” section at the end of each process discussion examines how that process affects the longer lived IT portfolio lifecycles (application service, infrastructure service, asset, and technology product).}
}
@article{DVORAK2022102747,
title = {Tackling rapid technology changes by applying enterprise engineering theories},
journal = {Science of Computer Programming},
volume = {215},
pages = {102747},
year = {2022},
issn = {0167-6423},
doi = {https://doi.org/10.1016/j.scico.2021.102747},
url = {https://www.sciencedirect.com/science/article/pii/S0167642321001404},
author1 = {Ondřej Dvořák and Robert Pergl},
keywords = {ADA, Component-based systems, Enterprise engineering, Technology acceleration, Evolvability},
abstract = {Moore's law states that the number of transistors on a chip will double every two years. A similar force appears to drive the progress of information technology (IT). IT companies tend to struggle to keep up with the latest technological developments, and software solutions are becoming increasingly outdated. The ability for software to change easily is defined as evolvability. One of the major fields researching evolvability is enterprise engineering (EE). The EE research paradigm applies theories from other fields to the evolvability of organisations. We argue that such theories can be applied to software engineering (SE) as well, which can contribute to the construction of software with a clear separation of dynamically changing technologies based on a relatively stable description of functions required for a specific user. EE theories introduce notions of function, construction, and affordance. We reify these concepts in terms of SE. Based on this reification, we propose affordance-driven assembling (ADA) as a software design approach that can aid in the construction of more evolvable software solutions. We exemplify the implementation of ADA in a case study on a commercial system and measure its effectiveness in terms of the impact of changes, as defined by the normalised systems theory.}
}
@article{BARAKABITZE2020106984,
title = {5G network slicing using SDN and NFV: A survey of taxonomy, architectures and future challenges},
journal = {Computer Networks},
volume = {167},
pages = {106984},
year = {2020},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2019.106984},
url = {https://www.sciencedirect.com/science/article/pii/S1389128619304773},
author1 = {Alcardo Alex Barakabitze and Arslan Ahmad and Rashid Mijumbi and Andrew Hines},
keywords = {5G, SDN, NFV, Network slicing, Cloud/edge computing, Network softwarization},
abstract = {The increasing consumption of multimedia services and the demand of high-quality services from customers has triggered a fundamental change in how we administer networks in terms of abstraction, separation, and mapping of forwarding, control and management aspects of services. The industry and the academia are embracing 5G as the future network capable to support next generation vertical applications with different service requirements. To realize this vision in 5G network, the physical network has to be sliced into multiple isolated logical networks of varying sizes and structures which are dedicated to different types of services based on their requirements with different characteristics and requirements (e.g., a slice for massive IoT devices, smartphones or autonomous cars, etc.). Softwarization using Software-Defined Networking (SDN) and Network Function Virtualization (NFV)in 5G networks are expected to fill the void of programmable control and management of network resources. In this paper, we provide a comprehensive review and updated solutions related to 5G network slicing using SDN and NFV. Firstly, we present 5G service quality and business requirements followed by a description of 5G network softwarization and slicing paradigms including essential concepts, history and different use cases. Secondly, we provide a tutorial of 5G network slicing technology enablers including SDN, NFV, MEC, cloud/Fog computing, network hypervisors, virtual machines & containers. Thidly, we comprehensively survey different industrial initiatives and projects that are pushing forward the adoption of SDN and NFV in accelerating 5G network slicing. A comparison of various 5G architectural approaches in terms of practical implementations, technology adoptions and deployment strategies is presented. Moreover, we provide a discussion on various open source orchestrators and proof of concepts representing industrial contribution. The work also investigates the standardization efforts in 5G networks regarding network slicing and softwarization. Additionally, the article presents the management and orchestration of network slices in a single domain followed by a comprehensive survey of management and orchestration approaches in 5G network slicing across multiple domains while supporting multiple tenants. Furthermore, we highlight the future challenges and research directions regarding network softwarization and slicing using SDN and NFV in 5G networks.}
}
@article{TURNER2018782,
title = {Enterprise Thinking for Self-aware Systems},
journal = {IFAC-PapersOnLine},
volume = {51},
number = {11},
pages = {782-789},
year = {2018},
note = {16th IFAC Symposium on Information Control Problems in Manufacturing INCOM 2018},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2018.08.414},
url = {https://www.sciencedirect.com/science/article/pii/S2405896318315404},
author1 = {Pat Turner and Peter Bernus and Ovidiu Noran},
keywords = {Internet of Things, Systems of Systems, Self-aware systems, Service Oriented Enterprise Architectures, Enterprise System Engineering, Self-organization},
abstract = {The paper aims to provide high-level guidance for architects of cyber-physical enterprises. We propose that interactions within such systems should be largely self-determined, based on system self-awareness and dynamic re-configuration, with the architecture evolving based on a set of foundational principles, rather than being pre-defined by an external designer. We investigate the suitability of typical development life cycles and identify architectural challenges in the context of dynamic cyber-physical systems that utilize the power of the Internet of Things. Desired systemic attributes are defined, which are necessary for making suitable core architectural choices. The application of the findings is exemplified through a case study, a synthesis of issues, and implications for further research.}
}
@incollection{BARMPIS202087,
title = {Chapter 5 - Monitoring model analytics over large repositories with Hawk and MEASURE},
editor = {Bedir Tekinerdogan and Önder Babur and Loek Cleophas and Mark {van den Brand} and Mehmet Akşit},
booktitle = {Model Management and Analytics for Large Scale Systems},
publisher = {Academic Press},
pages = {87-123},
year = {2020},
isbn = {978-0-12-816649-9},
doi = {https://doi.org/10.1016/B978-0-12-816649-9.00014-4},
url = {https://www.sciencedirect.com/science/article/pii/B9780128166499000144},
author1 = {Konstantinos Barmpis and Antonio García-Domínguez and Alessandra Bagnato and Antonin Abherve},
keywords = {software metrics, model analytics, model-driven engineering, graph databases},
abstract = {Software analytics play an important role in developing systems today. They can provide continuous and valuable insights into areas of interest throughout the life cycle of a system, guiding the engineers to the areas that need to be worked on to create timely and robust software. An important requirement is to have a low-effort way of producing them, without largely affecting current work patterns. This work presents the integration of the Hawk model indexing framework into the MEASURE metrics platform, allowing for the efficient production of model metrics over large collections of models managed by teams of developers. We evaluate this integration by monitoring a real-world model with hundreds of changes over its lifetime and producing relevant metrics over this period of time. This evaluation has demonstrated the ability to produce complex metrics and evaluate them over the entire history of a model.}
}
@article{ALVAREZRODRIGUEZ2023103744,
title = {Towards a method to quantitatively measure toolchain interoperability in the engineering lifecycle: A case study of digital hardware design},
journal = {Computer Standards & Interfaces},
volume = {86},
pages = {103744},
year = {2023},
issn = {0920-5489},
doi = {https://doi.org/10.1016/j.csi.2023.103744},
url = {https://www.sciencedirect.com/science/article/pii/S0920548923000259},
author1 = {Jose María Alvarez-Rodríguez and Roy Mendieta and Eduardo Cibrián and Juan Llorens},
keywords = {Software tools, Software reusability, Web services, Software as a service, Internet},
abstract = {The engineering lifecycle of cyber-physical systems is becoming more challenging than ever. Multiple engineering disciplines must be orchestrated to produce both a virtual and physical version of the system. Each engineering discipline makes use of their own methods and tools generating different types of work products that must be consistently linked together and reused throughout the lifecycle. Requirements, logical/descriptive and physical/analytical models, 3D designs, test case descriptions, product lines, ontologies, evidence argumentations, and many other work products are continuously being produced and integrated to implement the technical engineering and technical management processes established in standards such as the ISO/IEC/IEEE 15288:2015 “Systems and software engineering-System life cycle processes”. Toolchains are then created as a set of collaborative tools to provide an executable version of the required technical processes. In this engineering environment, there is a need for technical interoperability enabling tools to easily exchange data and invoke operations among them under different protocols, formats, and schemas. However, this automation of tasks and lifecycle processes does not come free of charge. Although enterprise integration patterns, shared and standardized data schemas and business process management tools are being used to implement toolchains, the reality shows that in many cases, the integration of tools within a toolchain is implemented through point-to-point connectors or applying some architectural style such as a communication bus to ease data exchange and to invoke operations. In this context, the ability to measure the current and expected degree of interoperability becomes relevant: 1) to understand the implications of defining a toolchain (need of different protocols, formats, schemas and tool interconnections) and 2) to measure the effort to implement the desired toolchain. To improve the management of the engineering lifecycle, a method is defined: 1) to measure the degree of interoperability within a technical engineering process implemented with a toolchain and 2) to estimate the effort to transition from an existing toolchain to another. A case study in the field of digital hardware design comprising 6 different technical engineering processes and 7 domain engineering tools is conducted to demonstrate and validate the proposed method.}
}
@article{TRUONG201614,
title = {Towards the Realization of Multi-dimensional Elasticity for Distributed Cloud Systems},
journal = {Procedia Computer Science},
volume = {97},
pages = {14-23},
year = {2016},
note = {2nd International Conference on Cloud Forward: From Distributed to Complete Computing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2016.08.276},
url = {https://www.sciencedirect.com/science/article/pii/S1877050916320920},
author1 = {Hong-Linh Truong and Schahram Dustdar and Frank Leymann},
keywords = {distributed clouds, elasticity, cloud services, cloud optimization, monitoring ;},
abstract = {As multiple types of distributed, heterogeneous cloud computing environments have proliferated, cloud software can leverage diverse types of infrastructural, platform and data resources with different cost and quality models. This introduces a multidimensional elasticity perspective for cloud software that would greatly meet changing demands from the user. However, we argue that current techniques are not enough for dealing with multi-dimensional elasticity in distributed cloud environments. We present our approach to the realization of multi-dimensional elasticity by introducing novel concepts and a roadmap to achieve them.}
}
@article{SINGJAI2022111433,
title = {Conformance assessment of Architectural Design Decisions on API endpoint designs derived from domain models},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111433},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111433},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001352},
author1 = {Apitchaka Singjai and Uwe Zdun},
keywords = {Microservice architecture, API design, Domain driven design, Architecture conformance assessment},
abstract = {Context:
Domain-driven design (DDD) is commonly used to design microservices. A crucial aspect of microservice design is API design, which includes the design of API endpoints.
Objective:
Our objective is to automate the assessment of conformance to Architectural Design Decisions (ADDs) on the interrelation of DDD and APIs. In particular, we studied link mapping, API operation design, and resource segregation as API endpoint design issues that are linked to domain model design. We particularly aim to address conformance checking in the context of frequent release practices, as frequent manual conformance checking is difficult or infeasible.
Methods:
We suggest a new approach for the automated assessment of conformance to ADD options. The approach suggests automated detectors to detect ADD options selected in a given API endpoint design, as well as an assessment scoring scheme based on empirical results. For the evaluation of our approach, we first manually created a ground truth for 12 cases in a multi-case study, and then compared the results of our automated detectors to the ground truth for each of those cases.
Results:
With our approach, all ADD options in our multi-case study possibly can be automatically detected. Without further improvements, our approach identifies 83% of the decision points in the multi-case study correctly. A statistical analysis of our data shows only a negligible effect size for differences to the ground truth.
Conclusion:
Our new approach provides a pragmatic method for automated detection of conformance to ADDs on the interrelation of DDD and APIs. The approach can support the continuous analysis of API endpoint designs.}
}
@incollection{GRAY2016103,
title = {Chapter 5 - The NFV Infrastructure Management},
editor = {Ken Gray and Thomas D. Nadeau},
booktitle = {Network Function Virtualization},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {103-125},
year = {2016},
isbn = {978-0-12-802119-4},
doi = {https://doi.org/10.1016/B978-0-12-802119-4.00005-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128021194000055},
author1 = {Ken Gray and Thomas D. Nadeau},
keywords = {OpenStack, controllers, NFVI, VIM, network architecture, OpenDaylight},
abstract = {In this chapter we revisit the high-level architecture framework we described in Chapter 3, ETSI NFV ISG. We begin with a discussion of the NFV-I functional area as it relates to management and control of the virtualized compute, storage, and network components that together realize all virtualized functions on a given hardware platform—the VIM. We do this within the context of various open source projects such as OpenStack, and OpenDaylight. We introduce the concept of PaaS versus IaaS approaches to NFV and begin to ask questions about the ETSI architecture.}
}
@article{BEHUTIYE2020106225,
title = {Management of quality requirements in agile and rapid software development: A systematic mapping study},
journal = {Information and Software Technology},
volume = {123},
pages = {106225},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106225},
url = {https://www.sciencedirect.com/science/article/pii/S095058491930240X},
author1 = {Woubshet Behutiye and Pertti Karhapää and Lidia López and Xavier Burgués and Silverio Martínez-Fernández and Anna Maria Vollmer and Pilar Rodríguez and Xavier Franch and Markku Oivo},
keywords = {Quality requirements, Non-functional requirements, Agile software development, Rapid software development, Systematic mapping study, Systematic literature reviews},
abstract = {Context
Quality requirements (QRs) describe the desired quality of software, and they play an important role in the success of software projects. In agile software development (ASD), QRs are often ill-defined and not well addressed due to the focus on quickly delivering functionality. Rapid software development (RSD) approaches (e.g., continuous delivery and continuous deployment), which shorten delivery times, are more prone to neglect QRs. Despite the significance of QRs in both ASD and RSD, there is limited synthesized knowledge on their management in those approaches.
Objective
This study aims to synthesize state-of-the-art knowledge about QR management in ASD and RSD, focusing on three aspects: bibliometric, strategies, and challenges.
Research method
Using a systematic mapping study with a snowballing search strategy, we identified and structured the literature on QR management in ASD and RSD.
Results
We found 156 primary studies: 106 are empirical studies, 16 are experience reports, and 34 are theoretical studies. Security and performance were the most commonly reported QR types. We identified various QR management strategies: 74 practices, 43 methods, 13 models, 12 frameworks, 11 advices, 10 tools, and 7 guidelines. Additionally, we identified 18 categories and 4 non-recurring challenges of managing QRs. The limited ability of ASD to handle QRs, time constraints due to short iteration cycles, limitations regarding the testing of QRs and neglect of QRs were the top categories of challenges.
Conclusion
Management of QRs is significant in ASD and is becoming important in RSD. This study identified research gaps, such as the need for more tools and guidelines, lightweight QR management strategies that fit short iteration cycles, investigations of the link between QRs challenges and technical debt, and extension of empirical validation of existing strategies to a wider context. It also synthesizes QR management strategies and challenges, which may be useful for practitioners.}
}
@article{FEMMINELLA2021107948,
title = {5G experiment design through Blueprint},
journal = {Computer Networks},
volume = {190},
pages = {107948},
year = {2021},
issn = {1389-1286},
doi = {https://doi.org/10.1016/j.comnet.2021.107948},
url = {https://www.sciencedirect.com/science/article/pii/S1389128621000864},
author1 = {Mauro Femminella and Matteo Pergolesi and Gianluca Reali},
keywords = {5G, Intent, NFV, Data models, Blueprint},
abstract = {The 5G technology includes innovative network services that allow vertical players to boost their service portfolio in critical application areas currently not suitably addressed by 4G systems. However, these improvements in both flexibility and performance have some consequences in terms of complexity. Indeed, 5G networks are quite complex systems, and require specific skills on network virtualization and cloud services for taking full advantage of their potential. This may represent a hard entrance barrier for vertical stakeholders willing to deploy new services on 5G networks. Thus, it becomes necessary to introduce some simplified approaches for implementing and deploying services in 5G edge clouds. We propose a solution based on the usage of simple yet flexible service templates, named blueprints. Such templates can be easily filled by vertical players by specifying just the main entities of their service. Each template allows specifying not only the raw structure of a service running in a virtualized infrastructure, but also how to test it in a variety of realistic network conditions. Our proposed tool processes blueprints and translates them into detailed network configuration. This way, verticals are unaware of the underlying operations, saving a significant amount of programming effort. In order to show the effectiveness of our solution and to ease the understanding of technical details, we make use of a vertical service related to the area of pedestrian traffic monitoring in smart cities as a running example. We express the effort saving by means of the well-known COCOMO model, which shows a reduction of about 70% or even more of the programming effort from the vertical.}
}
@incollection{PARASCHIS2020673,
title = {Chapter 15 - Innovations in DCI transport networks},
editor = {Alan E. Willner},
booktitle = {Optical Fiber Telecommunications VII},
publisher = {Academic Press},
pages = {673-718},
year = {2020},
isbn = {978-0-12-816502-7},
doi = {https://doi.org/10.1016/B978-0-12-816502-7.00017-8},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165027000178},
author1 = {Loukas Paraschis and Kannan Raj},
keywords = {Data center interconnects, DCI, coherent WDM, SDN, open-line system, cloud infrastructure},
abstract = {The capacity of the transport networks interconnecting data-centers (DCI) has grown more than any other traffic type due to the proliferation of “cloud” services. Consequently, DCI has motivated the evolution of dedicated DCI networks and DCI-optimized transport systems. This chapter reviews important current and emerging innovations in technology, systems, and networks, their synergies, and the related research, development, and standards efforts, that collectively have facilitated the DCI evolution to its current multi-Tb/s global infrastructure that employs some of the most spectrally efficient deployed fiber networks. Purpose-built DCI transport systems have been optimized for the data center operational requirements, simpler routing, and state-of-the-art coherent wavelength-division multiplexing (WDM) transmission that has already exceeded 6b/s/Hz. Moreover, DCI has pioneered in WDM transport the extensive adoption of software-defined networking innovations in programmability, automation, management abstraction, and control-plane disaggregation to simplify operations and enable “open” transport architectures.}
}
@incollection{20201039,
title = {Index},
editor = {Alan E. Willner},
booktitle = {Optical Fiber Telecommunications VII},
publisher = {Academic Press},
pages = {1039-1083},
year = {2020},
isbn = {978-0-12-816502-7},
doi = {https://doi.org/10.1016/B978-0-12-816502-7.00027-0},
url = {https://www.sciencedirect.com/science/article/pii/B9780128165027000270}
}
@article{HAGHIGHATKHAH201725,
title = {Automotive software engineering: A systematic mapping study},
journal = {Journal of Systems and Software},
volume = {128},
pages = {25-55},
year = {2017},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2017.03.005},
url = {https://www.sciencedirect.com/science/article/pii/S0164121217300560},
author1 = {Alireza Haghighatkhah and Ahmad Banijamali and Olli-Pekka Pakanen and Markku Oivo and Pasi Kuvaja},
keywords = {Literature survey, Systematic mapping study, Automotive software engineering, Automotive systems, Embedded systems, Software-intensive systems},
abstract = {The automotive industry is going through a fundamental change by moving from a mechanical to a software-intensive industry in which most innovation and competition rely on software engineering competence. Over the last few decades, the importance of software engineering in the automotive industry has increased significantly and has attracted much attention from both scholars and practitioners. A large body-of-knowledge on automotive software engineering has accumulated in several scientific publications, yet there is no systematic analysis of that knowledge. This systematic mapping study aims to classify and analyze the literature related to automotive software engineering in order to provide a structured body-of-knowledge, identify well-established topics and potential research gaps. The review includes 679 articles from multiple research sub-area, published between 1990 and 2015. The primary studies were analyzed and classified with respect to five different dimensions. Furthermore, potential research gaps and recommendations for future research are presented. Three areas, namely system/software architecture and design, qualification testing, and reuse were the most frequently addressed topics in the literature. There were fewer comparative and validation studies, and the literature lacks practitioner-oriented guidelines. Overall, research activity on automotive software engineering seems to have high industrial relevance but is relatively lower in its scientific rigor.}
}
@article{MONGA2020181,
title = {Software-Defined Network for End-to-end Networked Science at the Exascale},
journal = {Future Generation Computer Systems},
volume = {110},
pages = {181-201},
year = {2020},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2020.04.018},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X19305618},
author1 = {Inder Monga and Chin Guok and John MacAuley and Alex Sim and Harvey Newman and Justas Balcas and Phil DeMar and Linda Winkler and Tom Lehman and Xi Yang},
keywords = {Intent based networking, End-to-end orchestration, Intelligent network services, Distributed infrastructure, Resource modeling, Software defined networking, Real-time, Interactive},
abstract = {Domain science applications and workflow processes are currently forced to view the network as an opaque infrastructure into which they inject data and hope that it emerges at the destination with an acceptable Quality of Experience. There is little ability for applications to interact with the network to exchange information, negotiate performance parameters, discover expected performance metrics, or receive status/troubleshooting information in real time. The work presented here is motivated by a vision for a new smart network and smart application ecosystem that will provide a more deterministic and interactive environment for domain science workflows. The Software-Defined Network for End-to-end Networked Science at Exascale (SENSE) system includes a model-based architecture, implementation, and deployment which enables automated end-to-end network service instantiation across administrative domains. An intent based interface allows applications to express their high-level service requirements, an intelligent orchestrator and resource control systems allow for custom tailoring of scalability and real-time responsiveness based on individual application and infrastructure operator requirements. This allows the science applications to manage the network as a first-class schedulable resource as is the current practice for instruments, compute, and storage systems. Deployment and experiments on production networks and testbeds have validated SENSE functions and performance. Emulation based testing verified the scalability needed to support research and education infrastructures. Key contributions of this work include an architecture definition, reference implementation, and deployment. This provides the basis for further innovation of smart network services to accelerate scientific discovery in the era of big data, cloud computing, machine learning and artificial intelligence.}
}
@article{RAY2021129,
title = {SDN/NFV architectures for edge-cloud oriented IoT: A systematic review},
journal = {Computer Communications},
volume = {169},
pages = {129-153},
year = {2021},
issn = {0140-3664},
doi = {https://doi.org/10.1016/j.comcom.2021.01.018},
url = {https://www.sciencedirect.com/science/article/pii/S0140366421000396},
author1 = {Partha Pratim Ray and Neeraj Kumar},
keywords = {SDN, NFV, IoT, Edge, Cloud, Architecture},
abstract = {Software-defined network (SDN) and network function virtualization (NFV) have entirely changed the way internetwork backhaul should be utilized and behaved for virtualized service provisioning. Several benefits have been observed in multiple domains of applications that has used SDN and NFV in integrated way. Thus, SDN/NFV paradigm has been investigated to seek whether network services could be efficiently delivered, managed, and disseminated to the end users. Internet of Things (IoT) is justifiably associated with the SDN/NFV augmentation to make this task enriched. However, factors related to edge-cloud communication and network services have not been effectively mitigated until now. In this paper, we present an in-depth, qualitative, and comprehensive systematic review to find the answers of following research questions, such as, (i) how does state-of-the-art SDN/NFV architecture look like, (ii) how to solve next generation cellular services via architecture involvement, (iii) what type of application/test-bed need to be studied, and (iv) security framework should be catered. We further, elaborate various key issues and challenges in the existing architecture mitigation for SDN/NFV integration to the IoT-based edge-cloud oriented network service provisioning. Future directions are also prescribed to support fellow researchers to improve existing virtualized service scenario. Lessons learned after performing comparative study with other survey articles dictates that our work presents timely contribution in terms of novel knowledge toward understanding of formulating SDN/NFV virtualization services under the aegis of IoT-centric edge-cloud scenario.}
}
@article{PALOPAK2023107131,
title = {Knowledge diffusion trajectories of agile software development research: A main path analysis},
journal = {Information and Software Technology},
volume = {156},
pages = {107131},
year = {2023},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2022.107131},
url = {https://www.sciencedirect.com/science/article/pii/S0950584922002403},
author1 = {Yulianus Palopak and Sun-Jen Huang and Wiwit Ratnasari},
keywords = {Agile software development, Agile methods, Main path analysis, Citation analysis, Key-route analysis, Knowledge diffusion},
abstract = {Context
The dramatic growth of agile software development (ASD) research has resulted in a large number of diverse theoretical and empirical publications. The citation relationships among these publications indicate knowledge dissemination across and within academia or scientists.
Objective
This study offers a comprehensive understanding of the ASD literature by exploring the knowledge diffusion path through the citation network of publications that have made significant contributions to its research development.
Method
We employ a quantitative citation-based methodology, main path analysis (MPA), to examine the citation relationship of 1431 scientific articles published in the Web of Science (WoS) between 2001 and 2021 and visualize the MPA results using Pajek software.
Results
Through citation analysis this study discovers knowledge diffusion trajectories of publications concerning ASD method. Our key results present 32 publications identified along the key-route main path as the most influential ones in the trajectories of ASD. There are three phases of ASD research development: introduction, evaluation, and deployment and expansion. Using the multiple-global main path, we further uncover the publication trends from a set of recent papers and reveal four sub-themes: tailoring of agile practices, large-scale agile context, challenges and success factors of large-scale organizations, and agile global software development.
Conclusions
Although there was little academic interest in the initial phase, ASD-related publication and citation trends have consistently increased over time. The historical development of ASD methods was established in three distinct phases of publications in the domain. Each phase presents a narrative of agile methods’ development with different focuses. The most recent trends of ASD publications tend to focus on the agile tailoring and scaling process in the global and distributed environment.}
}
@article{SOBHY2020110428,
title = {Run-time evaluation of architectures: A case study of diversification in IoT},
journal = {Journal of Systems and Software},
volume = {159},
pages = {110428},
year = {2020},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2019.110428},
url = {https://www.sciencedirect.com/science/article/pii/S016412121930202X},
author1 = {Dalia Sobhy and Leandro Minku and Rami Bahsoon and Tao Chen and Rick Kazman},
keywords = {Run-time architecture evaluation, Runtime architecture evaluation, Software architectures for dynamic environments, Internet of things, IoT, Design diversity},
abstract = {Run-time properties of modern software system environments, such as Internet of Things (IoT), are a challenge for existing software architecture evaluation methods. Such systems are largely data-driven, characterized by their dynamism, unpredictability in operation, hyper-connectivity, and scale. Properties, such as performance, delayed delivery, and scalability, are acknowledged to pose great risk and are difficult to evaluate at design-time. Run-time evaluation could potentially be used to complement design-time evaluation, enabling significant deviations from the expected performance values to be captured. However, there are no systematic software architecture evaluation methods that intertwine and interleave design-time and run-time evaluation. This paper addresses this gap by proposing a novel run-time architecture evaluation method suited for systems that exhibit uncertainty and dynamism in their operation. Our method uses machine learning and cost-benefit analysis at run-time to continuously profile the architecture decisions made, to assess their added value. We demonstrate the applicability and effectiveness of this approach in the context of an IoT system architecture, where some architecture design decisions were diversified to meet Quality of Service (QoS) requirements. Our approach provides run-time assessment for these decisions which can inform deployment, refinement, and/or phasing-out decisions.}
}
@article{KAIYA20201449,
title = {A Tool to Manage Traceability on Several Models and Its Use Case},
journal = {Procedia Computer Science},
volume = {176},
pages = {1449-1458},
year = {2020},
note = {Knowledge-Based and Intelligent Information & Engineering Systems: Proceedings of the 24th International Conference KES2020},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2020.09.155},
url = {https://www.sciencedirect.com/science/article/pii/S187705092032055X},
author1 = {Haruhiko Kaiya and Shogo Tatsui and Atsuo Hazeyama and Shinpei Ogata and Takao Okubo and Nobukazu Yoshioka and Hironori Washizaki},
keywords = {Software Traceability, Several Different Notations, Graphical Models, UML, CASE Tool, Use Case},
abstract = {To examine requirements and design of a system, using graphical models such as UML is one of the effective ways because it helps developers to understand the system and activities using the system. Usually, more than two types of notations are used to represent a system. At the age of digital transformation, relationships among several different systems should be also discussed and they are of course represented in several different notations. To improve the development and the analysis of several systems using such several notations, traceability among elements in the different notations should be managed, but most techniques focus on the traceability among a single project. In this paper, we present a tool to manage traceability on several different models. The tool is developed as a plugin of an existing graphical modeling tool called Astah. Astah enables us to describe UML models as well as mind maps, data flow diagrams, flow charts and so on. To evaluate our tool, we performed a method to elicit requirements of several different systems together by using the tool. We confirmed our tool was helpful to perform the method, but some additional functions would improve the performance more than now. The additional functions are as follows: tracing links transitively, annotating each link to clarify its type and recording an end of a link while the end is removed from a model.}
}
@incollection{DROGSETH2015257,
title = {Chapter 13 - Closing the Gap: Fine-Tuning Before Full Deployment},
editor = {Dennis Nils Drogseth and Rick Sturm and Dan Twing},
booktitle = {CMDB Systems},
publisher = {Morgan Kaufmann},
address = {Boston},
pages = {257-273},
year = {2015},
isbn = {978-0-12-801265-9},
doi = {https://doi.org/10.1016/B978-0-12-801265-9.00013-5},
url = {https://www.sciencedirect.com/science/article/pii/B9780128012659000135},
author1 = {Dennis Nils Drogseth and Rick Sturm and Dan Twing},
keywords = {PoC, Proof of concept, Gap, Modeling CIs, Service mapping, Requirements},
abstract = {This chapter draws from real voices and real experiences at “the gap” after proof of concept but before full production deployment targeting critical issues such as integration, scope creep, process issues, and service modeling. This “fine-tuning” is essential to help you address key issues as you begin to move toward actualizing your investments and delivering your first tangible benefits.}
}
@article{DIAZ2023111520,
title = {Applying Inter-Rater Reliability and Agreement in collaborative Grounded Theory studies in software engineering},
journal = {Journal of Systems and Software},
volume = {195},
pages = {111520},
year = {2023},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111520},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222001960},
author1 = {Jessica Díaz and Jorge Pérez and Carolina Gallardo and Ángel González-Prieto},
keywords = {Grounded Theory, Inter-Rater Reliability, Inter-Rater Agreement},
abstract = {Context:
The qualitative research on empirical software engineering that uses Grounded Theory is increasing (GT). The trustworthiness, rigor, and transparency of GT qualitative data analysis can benefit, among others, when multiple analysts juxtapose diverse perspectives and collaborate to develop a common code frame based on a consensual and consistent interpretation. Inter-Rater Reliability (IRR) and/or Inter-Rater Agreement (IRA) are commonly used techniques to measure consensus, and thus develop a shared interpretation. However, minimal guidance is available about how and when to measure IRR/IRA during the iterative process of GT, so researchers have been using ad hoc methods for years.
Objective:
This paper presents a process for systematically measuring IRR/IRA in GT studies, when appropriate, which is grounded in a previous systematic mapping study on collaborative GT in the field of software engineering.
Methods:
Meta-science guided us to analyze the issues and challenges of collaborative GT and formalize a process to measure IRR/IRA in GT.
Results:
This process guides researchers to incrementally generate a theory while ensuring consensus on the constructs that support it, improving trustworthiness, rigor, and transparency, and promoting the communicability, reflexivity, and replicability of the research.
Conclusion:
The application of this process to a GT study seems to support its feasibility. In the absence of further confirmation, this would represent the first step in a de facto standard to be applied to those GT studies that may benefit from IRR/IRA techniques.}
}
@article{DALIBOR2022111361,
title = {A Cross-Domain Systematic Mapping Study on Software Engineering for Digital Twins},
journal = {Journal of Systems and Software},
volume = {193},
pages = {111361},
year = {2022},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2022.111361},
url = {https://www.sciencedirect.com/science/article/pii/S0164121222000917},
author1 = {Manuela Dalibor and Nico Jansen and Bernhard Rumpe and David Schmalzing and Louis Wachtmeister and Manuel Wimmer and Andreas Wortmann},
keywords = {Software Engineering, Digital Twins, Manufacturing, Industry 4.0},
abstract = {Digital Twins are currently investigated as the technological backbone for providing an enhanced understanding and management of existing systems as well as for designing new systems in various domains, e.g., ranging from single manufacturing components such as sensors to large-scale systems such as smart cities. Given the diverse application domains of Digital Twins, it is not surprising that the characterization of the term Digital Twin, as well as the needs for developing and operating Digital Twins are multi-faceted. Providing a better understanding what the commonalities and differences of Digital Twins in different contexts are, may allow to build reusable support for developing, running, and managing Digital Twins by providing dedicated concepts, techniques, and tool support. In this paper, we aim to uncover the nature of Digital Twins based on a systematic mapping study which is not limited to a particular application domain or technological space. We systematically retrieved a set of 1471 unique publications of which 356 were selected for further investigation. In particular, we analyzed the types of research and contributions made for Digital Twins, the expected properties Digital Twins have to fulfill, how Digital Twins are realized and operated, as well as how Digital Twins are finally evaluated. Based on this analysis, we also contribute a novel feature model for Digital Twins from a software engineering perspective as well as several observations to further guide future software engineering research in this area.}
}
@article{PERKUSICH2020106241,
title = {Intelligent software engineering in the context of agile software development: A systematic literature review},
journal = {Information and Software Technology},
volume = {119},
pages = {106241},
year = {2020},
issn = {0950-5849},
doi = {https://doi.org/10.1016/j.infsof.2019.106241},
url = {https://www.sciencedirect.com/science/article/pii/S0950584919302587},
author1 = {Mirko Perkusich and Lenardo {Chaves e Silva} and Alexandre Costa and Felipe Ramos and Renata Saraiva and Arthur Freire and Ednaldo Dilorenzo and Emanuel Dantas and Danilo Santos and Kyller Gorgônio and Hyggo Almeida and Angelo Perkusich},
keywords = {Intelligent software engineering, Agile software development, Search-based software engineering, Machine learning, Bayesian networks, Artificial intelligence},
abstract = {CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers.}
}